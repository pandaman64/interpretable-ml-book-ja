<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>5.5 Permutation Feature Importance | Interpretable Machine Learning</title>
  <meta name="description" content="Machine learning algorithms usually operate as black boxes and it is unclear how they derived a certain decision. This book is a guide for practitioners to make machine learning decisions interpretable." />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="5.5 Permutation Feature Importance | Interpretable Machine Learning" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Machine learning algorithms usually operate as black boxes and it is unclear how they derived a certain decision. This book is a guide for practitioners to make machine learning decisions interpretable." />
  <meta name="github-repo" content="christophM/interpretable-ml-book" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="5.5 Permutation Feature Importance | Interpretable Machine Learning" />
  
  <meta name="twitter:description" content="Machine learning algorithms usually operate as black boxes and it is unclear how they derived a certain decision. This book is a guide for practitioners to make machine learning decisions interpretable." />
  

<meta name="author" content="Christoph Molnar" />


<meta name="date" content="2021-04-20" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="interaction.html"/>
<link rel="next" href="global.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<!-- Global site tag (gtag.js) - Google Analytics -->
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-110543840-1', 'https://christophm.github.io/interpretable-ml-book/', {
  'anonymizeIp': true
  , 'storage': 'none'
  , 'clientId': window.localStorage.getItem('ga_clientId')
});
ga(function(tracker) {
  window.localStorage.setItem('ga_clientId', tracker.get('clientId'));
});
ga('send', 'pageview');
</script>

<link rel="stylesheet" type="text/css" href="css/cookieconsent.min.css" />
<script src="javascript/cookieconsent.min.js"></script>
<script>
window.addEventListener("load", function(){
window.cookieconsent.initialise({
  "palette": {
    "popup": {
      "background": "#000"
    },
    "button": {
      "background": "#f1d600"
    }
  },
  "position": "bottom-right",
  "content": {
    "message": "This website uses cookies for Google Analytics so that I know how many people are reading the book and which chapters are the most popular. The book website doesn't collect any personal data."
  }
})});
</script>



<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Interpretable machine learning</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>要約</a></li>
<li class="chapter" data-level="" data-path="著者による序文.html"><a href="著者による序文.html"><i class="fa fa-check"></i>著者による序文</a></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> イントロダクション</a><ul>
<li class="chapter" data-level="1.1" data-path="storytime.html"><a href="storytime.html"><i class="fa fa-check"></i><b>1.1</b> 物語の時間</a><ul>
<li class="chapter" data-level="" data-path="storytime.html"><a href="storytime.html#稲妻は二度と打たない"><i class="fa fa-check"></i>稲妻は二度と打たない</a></li>
<li class="chapter" data-level="" data-path="storytime.html"><a href="storytime.html#信用失墜"><i class="fa fa-check"></i>信用失墜</a></li>
<li class="chapter" data-level="" data-path="storytime.html"><a href="storytime.html#フェルミのペーパークリップ"><i class="fa fa-check"></i>フェルミのペーパー・クリップ</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="機械学習とは何か.html"><a href="機械学習とは何か.html"><i class="fa fa-check"></i><b>1.2</b> 機械学習とは何か？</a></li>
<li class="chapter" data-level="1.3" data-path="terminology.html"><a href="terminology.html"><i class="fa fa-check"></i><b>1.3</b> 専門用語</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="interpretability.html"><a href="interpretability.html"><i class="fa fa-check"></i><b>2</b> 解釈可能性</a><ul>
<li class="chapter" data-level="2.1" data-path="interpretability-importance.html"><a href="interpretability-importance.html"><i class="fa fa-check"></i><b>2.1</b> 解釈可能性の重要性</a></li>
<li class="chapter" data-level="2.2" data-path="解釈可能な手法の分類.html"><a href="解釈可能な手法の分類.html"><i class="fa fa-check"></i><b>2.2</b> 解釈可能な手法の分類</a></li>
<li class="chapter" data-level="2.3" data-path="解釈可能性の範囲.html"><a href="解釈可能性の範囲.html"><i class="fa fa-check"></i><b>2.3</b> 解釈可能性の範囲</a><ul>
<li class="chapter" data-level="2.3.1" data-path="解釈可能性の範囲.html"><a href="解釈可能性の範囲.html#アルゴリズムの透明性"><i class="fa fa-check"></i><b>2.3.1</b> アルゴリズムの透明性</a></li>
<li class="chapter" data-level="2.3.2" data-path="解釈可能性の範囲.html"><a href="解釈可能性の範囲.html#全体的なモデルの解釈可能性"><i class="fa fa-check"></i><b>2.3.2</b> 全体的なモデルの解釈可能性</a></li>
<li class="chapter" data-level="2.3.3" data-path="解釈可能性の範囲.html"><a href="解釈可能性の範囲.html#モジュールレベルのモデルの全体的な解釈可能性"><i class="fa fa-check"></i><b>2.3.3</b> モジュールレベルのモデルの全体的な解釈可能性</a></li>
<li class="chapter" data-level="2.3.4" data-path="解釈可能性の範囲.html"><a href="解釈可能性の範囲.html#単一の予測に対する局所的な解釈"><i class="fa fa-check"></i><b>2.3.4</b> 単一の予測に対する局所的な解釈</a></li>
<li class="chapter" data-level="2.3.5" data-path="解釈可能性の範囲.html"><a href="解釈可能性の範囲.html#予測のグループに対する局所的な解釈"><i class="fa fa-check"></i><b>2.3.5</b> 予測のグループに対する局所的な解釈</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="解釈可能性の評価.html"><a href="解釈可能性の評価.html"><i class="fa fa-check"></i><b>2.4</b> 解釈可能性の評価</a></li>
<li class="chapter" data-level="2.5" data-path="properties.html"><a href="properties.html"><i class="fa fa-check"></i><b>2.5</b> 説明に関する性質</a></li>
<li class="chapter" data-level="2.6" data-path="explanation.html"><a href="explanation.html"><i class="fa fa-check"></i><b>2.6</b> 人間に優しい説明</a><ul>
<li class="chapter" data-level="2.6.1" data-path="explanation.html"><a href="explanation.html#説明とはなにか"><i class="fa fa-check"></i><b>2.6.1</b> 説明とはなにか</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="data.html"><a href="data.html"><i class="fa fa-check"></i><b>3</b> データセット</a><ul>
<li class="chapter" data-level="3.1" data-path="bike-data.html"><a href="bike-data.html"><i class="fa fa-check"></i><b>3.1</b> 自転車レンタル (回帰)</a></li>
<li class="chapter" data-level="3.2" data-path="spam-data.html"><a href="spam-data.html"><i class="fa fa-check"></i><b>3.2</b> YouTube スパムコメント (テキスト分類)</a></li>
<li class="chapter" data-level="3.3" data-path="cervical.html"><a href="cervical.html"><i class="fa fa-check"></i><b>3.3</b> 子宮頸がんのリスク要因(クラス分類)</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="simple.html"><a href="simple.html"><i class="fa fa-check"></i><b>4</b> 解釈可能なモデル</a><ul>
<li class="chapter" data-level="4.1" data-path="limo.html"><a href="limo.html"><i class="fa fa-check"></i><b>4.1</b> 線形回帰</a><ul>
<li class="chapter" data-level="4.1.1" data-path="limo.html"><a href="limo.html#解釈"><i class="fa fa-check"></i><b>4.1.1</b> 解釈</a></li>
<li class="chapter" data-level="4.1.2" data-path="limo.html"><a href="limo.html#例"><i class="fa fa-check"></i><b>4.1.2</b> 例</a></li>
<li class="chapter" data-level="4.1.3" data-path="limo.html"><a href="limo.html#可視化による解釈"><i class="fa fa-check"></i><b>4.1.3</b> 可視化による解釈</a></li>
<li class="chapter" data-level="4.1.4" data-path="limo.html"><a href="limo.html#個々の予測に対する説明"><i class="fa fa-check"></i><b>4.1.4</b> 個々の予測に対する説明</a></li>
<li class="chapter" data-level="4.1.5" data-path="limo.html"><a href="limo.html#カテゴリカル特徴量のエンコーディング"><i class="fa fa-check"></i><b>4.1.5</b> カテゴリカル特徴量のエンコーディング</a></li>
<li class="chapter" data-level="4.1.6" data-path="limo.html"><a href="limo.html#線形モデルは良い説明を与えるか"><i class="fa fa-check"></i><b>4.1.6</b> 線形モデルは良い説明を与えるか?</a></li>
<li class="chapter" data-level="4.1.7" data-path="limo.html"><a href="limo.html#sparse-linear"><i class="fa fa-check"></i><b>4.1.7</b> スパースな線形モデル</a></li>
<li class="chapter" data-level="4.1.8" data-path="limo.html"><a href="limo.html#長所"><i class="fa fa-check"></i><b>4.1.8</b> 長所</a></li>
<li class="chapter" data-level="4.1.9" data-path="limo.html"><a href="limo.html#短所"><i class="fa fa-check"></i><b>4.1.9</b> 短所</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="logistic.html"><a href="logistic.html"><i class="fa fa-check"></i><b>4.2</b> ロジスティック回帰</a><ul>
<li class="chapter" data-level="4.2.1" data-path="logistic.html"><a href="logistic.html#線形回帰モデルを分類のために使うと何がいけないか"><i class="fa fa-check"></i><b>4.2.1</b> 線形回帰モデルを分類のために使うと何がいけないか。</a></li>
<li class="chapter" data-level="4.2.2" data-path="logistic.html"><a href="logistic.html#理論"><i class="fa fa-check"></i><b>4.2.2</b> 理論</a></li>
<li class="chapter" data-level="4.2.3" data-path="logistic.html"><a href="logistic.html#解釈性"><i class="fa fa-check"></i><b>4.2.3</b> 解釈性</a></li>
<li class="chapter" data-level="4.2.4" data-path="logistic.html"><a href="logistic.html#例-1"><i class="fa fa-check"></i><b>4.2.4</b> 例</a></li>
<li class="chapter" data-level="4.2.5" data-path="logistic.html"><a href="logistic.html#長所と短所"><i class="fa fa-check"></i><b>4.2.5</b> 長所と短所</a></li>
<li class="chapter" data-level="4.2.6" data-path="logistic.html"><a href="logistic.html#ソフトウェア"><i class="fa fa-check"></i><b>4.2.6</b> ソフトウェア</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="extend-lm.html"><a href="extend-lm.html"><i class="fa fa-check"></i><b>4.3</b> GLM、GAM、その他</a><ul>
<li class="chapter" data-level="4.3.1" data-path="extend-lm.html"><a href="extend-lm.html#glm"><i class="fa fa-check"></i><b>4.3.1</b> 結果が正規分布に従わない場合 - GLMs</a></li>
<li class="chapter" data-level="4.3.2" data-path="extend-lm.html"><a href="extend-lm.html#lm-interact"><i class="fa fa-check"></i><b>4.3.2</b> 相互作用</a></li>
<li class="chapter" data-level="4.3.3" data-path="extend-lm.html"><a href="extend-lm.html#gam"><i class="fa fa-check"></i><b>4.3.3</b> 非線形効果 - GAM</a></li>
<li class="chapter" data-level="4.3.4" data-path="extend-lm.html"><a href="extend-lm.html#長所-1"><i class="fa fa-check"></i><b>4.3.4</b> 長所</a></li>
<li class="chapter" data-level="4.3.5" data-path="extend-lm.html"><a href="extend-lm.html#短所-1"><i class="fa fa-check"></i><b>4.3.5</b> 短所</a></li>
<li class="chapter" data-level="4.3.6" data-path="extend-lm.html"><a href="extend-lm.html#ソフトウェア-1"><i class="fa fa-check"></i><b>4.3.6</b> ソフトウェア</a></li>
<li class="chapter" data-level="4.3.7" data-path="extend-lm.html"><a href="extend-lm.html#more-lm-extension"><i class="fa fa-check"></i><b>4.3.7</b> さらなる拡張</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="tree.html"><a href="tree.html"><i class="fa fa-check"></i><b>4.4</b> 決定木</a><ul>
<li class="chapter" data-level="4.4.1" data-path="tree.html"><a href="tree.html#決定木の解釈"><i class="fa fa-check"></i><b>4.4.1</b> 決定木の解釈</a></li>
<li class="chapter" data-level="4.4.2" data-path="tree.html"><a href="tree.html#例-2"><i class="fa fa-check"></i><b>4.4.2</b> 例</a></li>
<li class="chapter" data-level="4.4.3" data-path="tree.html"><a href="tree.html#長所-2"><i class="fa fa-check"></i><b>4.4.3</b> 長所</a></li>
<li class="chapter" data-level="4.4.4" data-path="tree.html"><a href="tree.html#短所-2"><i class="fa fa-check"></i><b>4.4.4</b> 短所</a></li>
<li class="chapter" data-level="4.4.5" data-path="tree.html"><a href="tree.html#ソフトウェア-2"><i class="fa fa-check"></i><b>4.4.5</b> ソフトウェア</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="rules.html"><a href="rules.html"><i class="fa fa-check"></i><b>4.5</b> 決定規則</a><ul>
<li class="chapter" data-level="4.5.1" data-path="rules.html"><a href="rules.html#単一の特徴量による規則学習-oner"><i class="fa fa-check"></i><b>4.5.1</b> 単一の特徴量による規則学習 (OneR)</a></li>
<li class="chapter" data-level="4.5.2" data-path="rules.html"><a href="rules.html#sequential-covering"><i class="fa fa-check"></i><b>4.5.2</b> Sequential Covering</a></li>
<li class="chapter" data-level="4.5.3" data-path="rules.html"><a href="rules.html#bayesian-rule-lists"><i class="fa fa-check"></i><b>4.5.3</b> Bayesian Rule Lists</a></li>
<li class="chapter" data-level="4.5.4" data-path="rules.html"><a href="rules.html#長所-3"><i class="fa fa-check"></i><b>4.5.4</b> 長所</a></li>
<li class="chapter" data-level="4.5.5" data-path="rules.html"><a href="rules.html#短所-3"><i class="fa fa-check"></i><b>4.5.5</b> 短所</a></li>
<li class="chapter" data-level="4.5.6" data-path="rules.html"><a href="rules.html#ソフトウェアと代替手法"><i class="fa fa-check"></i><b>4.5.6</b> ソフトウェアと代替手法</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="rulefit.html"><a href="rulefit.html"><i class="fa fa-check"></i><b>4.6</b> RuleFit</a><ul>
<li class="chapter" data-level="4.6.1" data-path="rulefit.html"><a href="rulefit.html#解釈と例"><i class="fa fa-check"></i><b>4.6.1</b> 解釈と例</a></li>
<li class="chapter" data-level="4.6.2" data-path="rulefit.html"><a href="rulefit.html#理論-1"><i class="fa fa-check"></i><b>4.6.2</b> 理論</a></li>
<li class="chapter" data-level="4.6.3" data-path="rulefit.html"><a href="rulefit.html#長所-4"><i class="fa fa-check"></i><b>4.6.3</b> 長所</a></li>
<li class="chapter" data-level="4.6.4" data-path="rulefit.html"><a href="rulefit.html#短所-4"><i class="fa fa-check"></i><b>4.6.4</b> 短所</a></li>
<li class="chapter" data-level="4.6.5" data-path="rulefit.html"><a href="rulefit.html#ソフトウェアと代替手法-1"><i class="fa fa-check"></i><b>4.6.5</b> ソフトウェアと代替手法</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="other-interpretable.html"><a href="other-interpretable.html"><i class="fa fa-check"></i><b>4.7</b> その他の解釈可能なモデル</a><ul>
<li class="chapter" data-level="4.7.1" data-path="other-interpretable.html"><a href="other-interpretable.html#単純ベイズ分類器-naive-bayes-classifier"><i class="fa fa-check"></i><b>4.7.1</b> 単純ベイズ分類器 (Naive Bayes Classifier)</a></li>
<li class="chapter" data-level="4.7.2" data-path="other-interpretable.html"><a href="other-interpretable.html#k近傍法"><i class="fa fa-check"></i><b>4.7.2</b> k近傍法</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="agnostic.html"><a href="agnostic.html"><i class="fa fa-check"></i><b>5</b> モデル非依存(Model-Agnostic)な手法</a><ul>
<li class="chapter" data-level="5.1" data-path="pdp.html"><a href="pdp.html"><i class="fa fa-check"></i><b>5.1</b> Partial Dependence Plot (PDP)</a><ul>
<li class="chapter" data-level="5.1.1" data-path="pdp.html"><a href="pdp.html#例-3"><i class="fa fa-check"></i><b>5.1.1</b> 例</a></li>
<li class="chapter" data-level="5.1.2" data-path="pdp.html"><a href="pdp.html#長所-5"><i class="fa fa-check"></i><b>5.1.2</b> 長所</a></li>
<li class="chapter" data-level="5.1.3" data-path="pdp.html"><a href="pdp.html#短所-5"><i class="fa fa-check"></i><b>5.1.3</b> 短所</a></li>
<li class="chapter" data-level="5.1.4" data-path="pdp.html"><a href="pdp.html#ソフトウェアと代替手法-2"><i class="fa fa-check"></i><b>5.1.4</b> ソフトウェアと代替手法</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="ice.html"><a href="ice.html"><i class="fa fa-check"></i><b>5.2</b> Individual Conditional Expectation (ICE)</a><ul>
<li class="chapter" data-level="5.2.1" data-path="ice.html"><a href="ice.html#例-4"><i class="fa fa-check"></i><b>5.2.1</b> 例</a></li>
<li class="chapter" data-level="5.2.2" data-path="ice.html"><a href="ice.html#長所-6"><i class="fa fa-check"></i><b>5.2.2</b> 長所</a></li>
<li class="chapter" data-level="5.2.3" data-path="ice.html"><a href="ice.html#短所-6"><i class="fa fa-check"></i><b>5.2.3</b> 短所</a></li>
<li class="chapter" data-level="5.2.4" data-path="ice.html"><a href="ice.html#ソフトウェアと代替手法-3"><i class="fa fa-check"></i><b>5.2.4</b> ソフトウェアと代替手法</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="ale.html"><a href="ale.html"><i class="fa fa-check"></i><b>5.3</b> Accumulated Local Effects (ALE) Plot</a><ul>
<li class="chapter" data-level="5.3.1" data-path="ale.html"><a href="ale.html#モチベーションと直感"><i class="fa fa-check"></i><b>5.3.1</b> モチベーションと直感</a></li>
<li class="chapter" data-level="5.3.2" data-path="ale.html"><a href="ale.html#理論-2"><i class="fa fa-check"></i><b>5.3.2</b> 理論</a></li>
<li class="chapter" data-level="5.3.3" data-path="ale.html"><a href="ale.html#予測"><i class="fa fa-check"></i><b>5.3.3</b> 予測</a></li>
<li class="chapter" data-level="5.3.4" data-path="ale.html"><a href="ale.html#例-6"><i class="fa fa-check"></i><b>5.3.4</b> 例</a></li>
<li class="chapter" data-level="5.3.5" data-path="ale.html"><a href="ale.html#利点"><i class="fa fa-check"></i><b>5.3.5</b> 利点</a></li>
<li class="chapter" data-level="5.3.6" data-path="ale.html"><a href="ale.html#欠点"><i class="fa fa-check"></i><b>5.3.6</b> 欠点</a></li>
<li class="chapter" data-level="5.3.7" data-path="ale.html"><a href="ale.html#実装と代替手法"><i class="fa fa-check"></i><b>5.3.7</b> 実装と代替手法</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="interaction.html"><a href="interaction.html"><i class="fa fa-check"></i><b>5.4</b> 特徴量の相互作用</a><ul>
<li class="chapter" data-level="5.4.1" data-path="interaction.html"><a href="interaction.html#特徴量の相互作用とは"><i class="fa fa-check"></i><b>5.4.1</b> 特徴量の相互作用とは</a></li>
<li class="chapter" data-level="5.4.2" data-path="interaction.html"><a href="interaction.html#friedman-の-h統計量の理論"><i class="fa fa-check"></i><b>5.4.2</b> Friedman の H統計量の理論</a></li>
<li class="chapter" data-level="5.4.3" data-path="interaction.html"><a href="interaction.html#例-7"><i class="fa fa-check"></i><b>5.4.3</b> 例</a></li>
<li class="chapter" data-level="5.4.4" data-path="interaction.html"><a href="interaction.html#利点-1"><i class="fa fa-check"></i><b>5.4.4</b> 利点</a></li>
<li class="chapter" data-level="5.4.5" data-path="interaction.html"><a href="interaction.html#欠点-1"><i class="fa fa-check"></i><b>5.4.5</b> 欠点</a></li>
<li class="chapter" data-level="5.4.6" data-path="interaction.html"><a href="interaction.html#実装"><i class="fa fa-check"></i><b>5.4.6</b> 実装</a></li>
<li class="chapter" data-level="5.4.7" data-path="interaction.html"><a href="interaction.html#代替手法"><i class="fa fa-check"></i><b>5.4.7</b> 代替手法</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="feature-importance.html"><a href="feature-importance.html"><i class="fa fa-check"></i><b>5.5</b> Permutation Feature Importance</a><ul>
<li class="chapter" data-level="5.5.1" data-path="feature-importance.html"><a href="feature-importance.html#理論-3"><i class="fa fa-check"></i><b>5.5.1</b> 理論</a></li>
<li class="chapter" data-level="5.5.2" data-path="feature-importance.html"><a href="feature-importance.html#feature-importance-data"><i class="fa fa-check"></i><b>5.5.2</b> 特徴量の重要度は、学習データとテストデータのどちらで計算するべきか</a></li>
<li class="chapter" data-level="5.5.3" data-path="feature-importance.html"><a href="feature-importance.html#例と解釈"><i class="fa fa-check"></i><b>5.5.3</b> 例と解釈</a></li>
<li class="chapter" data-level="5.5.4" data-path="feature-importance.html"><a href="feature-importance.html#利点-2"><i class="fa fa-check"></i><b>5.5.4</b> 利点</a></li>
<li class="chapter" data-level="5.5.5" data-path="feature-importance.html"><a href="feature-importance.html#欠点-2"><i class="fa fa-check"></i><b>5.5.5</b> 欠点</a></li>
<li class="chapter" data-level="5.5.6" data-path="feature-importance.html"><a href="feature-importance.html#ソフトウェアと代替手法-4"><i class="fa fa-check"></i><b>5.5.6</b> ソフトウェアと代替手法</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="global.html"><a href="global.html"><i class="fa fa-check"></i><b>5.6</b> グローバルサロゲート (Global Surrogate)</a><ul>
<li class="chapter" data-level="5.6.1" data-path="global.html"><a href="global.html#理論-4"><i class="fa fa-check"></i><b>5.6.1</b> 理論</a></li>
<li class="chapter" data-level="5.6.2" data-path="global.html"><a href="global.html#例-8"><i class="fa fa-check"></i><b>5.6.2</b> 例</a></li>
<li class="chapter" data-level="5.6.3" data-path="global.html"><a href="global.html#長所-7"><i class="fa fa-check"></i><b>5.6.3</b> 長所</a></li>
<li class="chapter" data-level="5.6.4" data-path="global.html"><a href="global.html#短所-7"><i class="fa fa-check"></i><b>5.6.4</b> 短所</a></li>
<li class="chapter" data-level="5.6.5" data-path="global.html"><a href="global.html#ソフトウェア-3"><i class="fa fa-check"></i><b>5.6.5</b> ソフトウェア</a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="lime.html"><a href="lime.html"><i class="fa fa-check"></i><b>5.7</b> Local Surrogate (LIME)</a><ul>
<li class="chapter" data-level="5.7.1" data-path="lime.html"><a href="lime.html#表形式データにおける-lime"><i class="fa fa-check"></i><b>5.7.1</b> 表形式データにおける LIME</a></li>
<li class="chapter" data-level="5.7.2" data-path="lime.html"><a href="lime.html#テキストデータに対するlime"><i class="fa fa-check"></i><b>5.7.2</b> テキストデータに対するLIME</a></li>
<li class="chapter" data-level="5.7.3" data-path="lime.html"><a href="lime.html#images-lime"><i class="fa fa-check"></i><b>5.7.3</b> 画像データに対するLIME</a></li>
<li class="chapter" data-level="5.7.4" data-path="lime.html"><a href="lime.html#長所-8"><i class="fa fa-check"></i><b>5.7.4</b> 長所</a></li>
<li class="chapter" data-level="5.7.5" data-path="lime.html"><a href="lime.html#短所-8"><i class="fa fa-check"></i><b>5.7.5</b> 短所</a></li>
</ul></li>
<li class="chapter" data-level="5.8" data-path="anchors.html"><a href="anchors.html"><i class="fa fa-check"></i><b>5.8</b> Scoped Rules (Anchors)</a><ul>
<li class="chapter" data-level="5.8.1" data-path="anchors.html"><a href="anchors.html#anchor-の発見"><i class="fa fa-check"></i><b>5.8.1</b> Anchor の発見</a></li>
<li class="chapter" data-level="5.8.2" data-path="anchors.html"><a href="anchors.html#複雑性と実行時間"><i class="fa fa-check"></i><b>5.8.2</b> 複雑性と実行時間</a></li>
<li class="chapter" data-level="5.8.3" data-path="anchors.html"><a href="anchors.html#表形式データの例"><i class="fa fa-check"></i><b>5.8.3</b> 表形式データの例</a></li>
<li class="chapter" data-level="5.8.4" data-path="anchors.html"><a href="anchors.html#長所-9"><i class="fa fa-check"></i><b>5.8.4</b> 長所</a></li>
<li class="chapter" data-level="5.8.5" data-path="anchors.html"><a href="anchors.html#短所-9"><i class="fa fa-check"></i><b>5.8.5</b> 短所</a></li>
<li class="chapter" data-level="5.8.6" data-path="anchors.html"><a href="anchors.html#ソフトウェアと代替手法-5"><i class="fa fa-check"></i><b>5.8.6</b> ソフトウェアと代替手法</a></li>
</ul></li>
<li class="chapter" data-level="5.9" data-path="shapley.html"><a href="shapley.html"><i class="fa fa-check"></i><b>5.9</b> シャープレイ値 (Shapley Values)</a><ul>
<li class="chapter" data-level="5.9.1" data-path="shapley.html"><a href="shapley.html#一般的なアイデア"><i class="fa fa-check"></i><b>5.9.1</b> 一般的なアイデア</a></li>
<li class="chapter" data-level="5.9.2" data-path="shapley.html"><a href="shapley.html#例と解釈-1"><i class="fa fa-check"></i><b>5.9.2</b> 例と解釈</a></li>
<li class="chapter" data-level="5.9.3" data-path="shapley.html"><a href="shapley.html#シャープレイ値の詳細"><i class="fa fa-check"></i><b>5.9.3</b> シャープレイ値の詳細</a></li>
<li class="chapter" data-level="5.9.4" data-path="shapley.html"><a href="shapley.html#長所-10"><i class="fa fa-check"></i><b>5.9.4</b> 長所</a></li>
<li class="chapter" data-level="5.9.5" data-path="shapley.html"><a href="shapley.html#短所-10"><i class="fa fa-check"></i><b>5.9.5</b> 短所</a></li>
<li class="chapter" data-level="5.9.6" data-path="shapley.html"><a href="shapley.html#ソフトウェアと代替手法-6"><i class="fa fa-check"></i><b>5.9.6</b> ソフトウェアと代替手法</a></li>
</ul></li>
<li class="chapter" data-level="5.10" data-path="shap.html"><a href="shap.html"><i class="fa fa-check"></i><b>5.10</b> SHAP (SHapley Additive exPlanations)</a><ul>
<li class="chapter" data-level="5.10.1" data-path="shap.html"><a href="shap.html#定義"><i class="fa fa-check"></i><b>5.10.1</b> 定義</a></li>
<li class="chapter" data-level="5.10.2" data-path="shap.html"><a href="shap.html#kernelshap"><i class="fa fa-check"></i><b>5.10.2</b> KernelSHAP</a></li>
<li class="chapter" data-level="5.10.3" data-path="shap.html"><a href="shap.html#treeshap"><i class="fa fa-check"></i><b>5.10.3</b> TreeSHAP</a></li>
<li class="chapter" data-level="5.10.4" data-path="shap.html"><a href="shap.html#例-12"><i class="fa fa-check"></i><b>5.10.4</b> 例</a></li>
<li class="chapter" data-level="5.10.5" data-path="shap.html"><a href="shap.html#shap-特徴量重要度-shap-feature-importance"><i class="fa fa-check"></i><b>5.10.5</b> SHAP 特徴量重要度 (SHAP Feature Importance)</a></li>
<li class="chapter" data-level="5.10.6" data-path="shap.html"><a href="shap.html#shap-summary-plot"><i class="fa fa-check"></i><b>5.10.6</b> SHAP Summary Plot</a></li>
<li class="chapter" data-level="5.10.7" data-path="shap.html"><a href="shap.html#shap-dependence-plot"><i class="fa fa-check"></i><b>5.10.7</b> SHAP Dependence Plot</a></li>
<li class="chapter" data-level="5.10.8" data-path="shap.html"><a href="shap.html#shap-相互作用値-shap-interaction-values"><i class="fa fa-check"></i><b>5.10.8</b> SHAP 相互作用値 (SHAP Interaction Values)</a></li>
<li class="chapter" data-level="5.10.9" data-path="shap.html"><a href="shap.html#clustering-shap-values"><i class="fa fa-check"></i><b>5.10.9</b> Clustering SHAP values</a></li>
<li class="chapter" data-level="5.10.10" data-path="shap.html"><a href="shap.html#長所-11"><i class="fa fa-check"></i><b>5.10.10</b> 長所</a></li>
<li class="chapter" data-level="5.10.11" data-path="shap.html"><a href="shap.html#短所-11"><i class="fa fa-check"></i><b>5.10.11</b> 短所</a></li>
<li class="chapter" data-level="5.10.12" data-path="shap.html"><a href="shap.html#ソフトウェア-4"><i class="fa fa-check"></i><b>5.10.12</b> ソフトウェア</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="example-based.html"><a href="example-based.html"><i class="fa fa-check"></i><b>6</b> 例示に基づいた説明手法</a><ul>
<li class="chapter" data-level="6.1" data-path="反事実的.html"><a href="反事実的.html"><i class="fa fa-check"></i><b>6.1</b> 反事実的説明 (Counterfactual Explanations)</a><ul>
<li class="chapter" data-level="6.1.1" data-path="反事実的.html"><a href="反事実的.html#反事実的説明の生成"><i class="fa fa-check"></i><b>6.1.1</b> 反事実的説明の生成</a></li>
<li class="chapter" data-level="6.1.2" data-path="反事実的.html"><a href="反事実的.html#例-13"><i class="fa fa-check"></i><b>6.1.2</b> 例</a></li>
<li class="chapter" data-level="6.1.3" data-path="反事実的.html"><a href="反事実的.html#長所-12"><i class="fa fa-check"></i><b>6.1.3</b> 長所</a></li>
<li class="chapter" data-level="6.1.4" data-path="反事実的.html"><a href="反事実的.html#短所-12"><i class="fa fa-check"></i><b>6.1.4</b> 短所</a></li>
<li class="chapter" data-level="6.1.5" data-path="反事実的.html"><a href="反事実的.html#ソフトウェアと代替手法-7"><i class="fa fa-check"></i><b>6.1.5</b> ソフトウェアと代替手法</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="adversarial.html"><a href="adversarial.html"><i class="fa fa-check"></i><b>6.2</b> 敵対的サンプル (Adversarial Examples)</a><ul>
<li class="chapter" data-level="6.2.1" data-path="adversarial.html"><a href="adversarial.html#手法及び例"><i class="fa fa-check"></i><b>6.2.1</b> 手法及び例</a></li>
<li class="chapter" data-level="6.2.2" data-path="adversarial.html"><a href="adversarial.html#サイバーセキュリティーの観点"><i class="fa fa-check"></i><b>6.2.2</b> サイバーセキュリティーの観点</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="proto.html"><a href="proto.html"><i class="fa fa-check"></i><b>6.3</b> prototype と criticism</a><ul>
<li class="chapter" data-level="6.3.1" data-path="proto.html"><a href="proto.html#理論-5"><i class="fa fa-check"></i><b>6.3.1</b> 理論</a></li>
<li class="chapter" data-level="6.3.2" data-path="proto.html"><a href="proto.html#例-14"><i class="fa fa-check"></i><b>6.3.2</b> 例</a></li>
<li class="chapter" data-level="6.3.3" data-path="proto.html"><a href="proto.html#長所-13"><i class="fa fa-check"></i><b>6.3.3</b> 長所</a></li>
<li class="chapter" data-level="6.3.4" data-path="proto.html"><a href="proto.html#短所-13"><i class="fa fa-check"></i><b>6.3.4</b> 短所</a></li>
<li class="chapter" data-level="6.3.5" data-path="proto.html"><a href="proto.html#コードと代替手法"><i class="fa fa-check"></i><b>6.3.5</b> コードと代替手法</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="influential.html"><a href="influential.html"><i class="fa fa-check"></i><b>6.4</b> Influential Instances</a><ul>
<li class="chapter" data-level="6.4.1" data-path="influential.html"><a href="influential.html#deletion-diagnostics"><i class="fa fa-check"></i><b>6.4.1</b> Deletion Diagnostics</a></li>
<li class="chapter" data-level="6.4.2" data-path="influential.html"><a href="influential.html#影響関数-influence-functions"><i class="fa fa-check"></i><b>6.4.2</b> 影響関数 (Influence Functions)</a></li>
<li class="chapter" data-level="6.4.3" data-path="influential.html"><a href="influential.html#長所-14"><i class="fa fa-check"></i><b>6.4.3</b> 長所</a></li>
<li class="chapter" data-level="6.4.4" data-path="influential.html"><a href="influential.html#短所-14"><i class="fa fa-check"></i><b>6.4.4</b> 短所</a></li>
<li class="chapter" data-level="6.4.5" data-path="influential.html"><a href="influential.html#ソフトウェアと代替手法-8"><i class="fa fa-check"></i><b>6.4.5</b> ソフトウェアと代替手法</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="neural-networks.html"><a href="neural-networks.html"><i class="fa fa-check"></i><b>7</b> ニューラルネットワークの解釈</a><ul>
<li class="chapter" data-level="7.1" data-path="cnn-features.html"><a href="cnn-features.html"><i class="fa fa-check"></i><b>7.1</b> 学習された特徴量</a><ul>
<li class="chapter" data-level="7.1.1" data-path="cnn-features.html"><a href="cnn-features.html#特徴量の可視化"><i class="fa fa-check"></i><b>7.1.1</b> 特徴量の可視化</a></li>
<li class="chapter" data-level="7.1.2" data-path="cnn-features.html"><a href="cnn-features.html#ネットワークの解剖"><i class="fa fa-check"></i><b>7.1.2</b> ネットワークの解剖</a></li>
<li class="chapter" data-level="7.1.3" data-path="cnn-features.html"><a href="cnn-features.html#利点-3"><i class="fa fa-check"></i><b>7.1.3</b> 利点</a></li>
<li class="chapter" data-level="7.1.4" data-path="cnn-features.html"><a href="cnn-features.html#欠点-3"><i class="fa fa-check"></i><b>7.1.4</b> 欠点</a></li>
<li class="chapter" data-level="7.1.5" data-path="cnn-features.html"><a href="cnn-features.html#ソフトウェアとその他の資料"><i class="fa fa-check"></i><b>7.1.5</b> ソフトウェアとその他の資料</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="future.html"><a href="future.html"><i class="fa fa-check"></i><b>8</b> 解釈可能な機械学習の未来</a><ul>
<li class="chapter" data-level="8.1" data-path="機械学習の未来.html"><a href="機械学習の未来.html"><i class="fa fa-check"></i><b>8.1</b> 機械学習の未来</a></li>
<li class="chapter" data-level="8.2" data-path="解釈性の未来.html"><a href="解釈性の未来.html"><i class="fa fa-check"></i><b>8.2</b> 解釈性の未来</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="contribute.html"><a href="contribute.html"><i class="fa fa-check"></i><b>9</b> 著者貢献</a></li>
<li class="chapter" data-level="10" data-path="cite.html"><a href="cite.html"><i class="fa fa-check"></i><b>10</b> この本の引用</a></li>
<li class="chapter" data-level="11" data-path="translations.html"><a href="translations.html"><i class="fa fa-check"></i><b>11</b> 翻訳</a></li>
<li class="chapter" data-level="12" data-path="謝辞.html"><a href="謝辞.html"><i class="fa fa-check"></i><b>12</b> 謝辞</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a><ul>
<li class="chapter" data-level="" data-path="r-packages-used-for-examples.html"><a href="r-packages-used-for-examples.html"><i class="fa fa-check"></i>R Packages Used for Examples</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Interpretable Machine Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="feature-importance" class="section level2">
<h2><span class="header-section-number">5.5</span> Permutation Feature Importance</h2>
<!--## Permutation Feature Importance {#feature-importance}-->
<!--
Permutation feature importance measures the increase in the prediction error of the model after we permuted the feature's values, which breaks the relationship between the feature and the true outcome. 
-->
<p>Permutation feature importance は、特徴量の値を並び替えることで、特徴量と真の結果との関係性を壊し、これによる予測誤差の増加を測定します。</p>
<!--### Theory-->
<div id="理論-3" class="section level3">
<h3><span class="header-section-number">5.5.1</span> 理論</h3>
<!--
The concept is really straightforward: 
We measure the importance of a feature by calculating the increase in the model's prediction error after permuting the feature.
A feature is "important" if shuffling its values increases the model error, because in this case the model relied on the feature for the prediction.
A feature is "unimportant" if shuffling its values leaves the model error unchanged, because in this case the model ignored the feature for the prediction.
The permutation feature importance measurement was introduced by Breiman (2001)[^Breiman2001] for random forests.
Based on this idea, Fisher, Rudin, and Dominici (2018)[^Fisher2018] proposed a model-agnostic version of the feature importance and called it model reliance. 
They also introduced more advanced ideas about feature importance, for example a (model-specific) version that takes into account that many prediction models may predict the data well.
Their paper is worth reading.
-->
<p>概念はとても単純です。 特徴量を並び替えたあとのモデルの予測誤差の影響を計算することで、特徴量の重要度を計算します。 特徴量の値を入れ替えるとモデル誤差が増加する場合、モデルは特徴量に依存した予測をしているので、その特徴量は「重要」です。 特徴量の値を入れ替えてもモデル誤差が変わらない場合、特徴量は「重要ではない」と言えます。 permutation feature importance は、Breiman (2001)<a href="#fn34" class="footnoteRef" id="fnref34"><sup>34</sup></a>によってランダムフォレストのために導入されました。 この考えに基づいて、Fisher, Rudin, Dominici (2018)<a href="#fn35" class="footnoteRef" id="fnref35"><sup>35</sup></a>は、モデルに依存しない特徴量重要度を提案し、これをモデル信頼度と呼んでいます。 彼らは、それに加えて特徴量の重要度に関する、より高度な考え方、例えば、多くの予測モデルがデータをうまく予測する可能性があることを考慮した（モデル固有の）バージョンも紹介しています。 彼らの論文 <strong>The permutation feature importance algorithm based on Fisher, Rudin, and Dominici (2018)</strong> は一読の価値があります。</p>
<!--Input: Trained model f, feature matrix X, target vector y, error measure L(y,f).-->
<p>入力: 学習モデル f, 特徴量行列 X, 目標ベクトル y, 誤差関数 L(y,f)</p>
<!--
1. Estimate the original model error e^orig^ = L(y, f(X))  (e.g. mean squared error)
2. For each feature j = 1,...,p do:
    - Generate feature matrix X^perm^ by permuting feature j in the data X. This breaks the association between feature j and true outcome y.
    - Estimate error e^perm^ = L(Y,f(X^perm^)) based on the predictions of the permuted data.
    - Calculate permutation feature importance FI^j^= e^perm^/e^orig^. Alternatively, the difference can be used: FI^j^ = e^perm^ - e^orig^
3. Sort features by descending FI.
-->
<ol style="list-style-type: decimal">
<li>元のモデル誤差 e<sup>orig</sup> = L(y, f(X)) を推定します。（例: 平均二乗誤差）</li>
<li>各特徴量 j = 1,....,p について
<ul>
<li>データ X の特徴量 j を並べ替えて特徴量行列 X<sup>perm</sup> を生成します。これにより、特徴量 j と真の結果 y との間の関連付けが解除されます。</li>
<li>並べ替えられたデータの予測値に基づいて、誤差 e<sup>perm</sup> = L(Y,f(X<sup>perm</sup>)) を推定します。</li>
<li>並べ替えた特徴量の重要度 FI<sup>j</sup>= e<sup>perm</sup>/e<sup>orig</sup> を計算します。あるいは、差分 FI<sup>j</sup> = e<sup>perm</sup> - e<sup>orig</sup> も使用できます。</li>
</ul></li>
<li>FIが高い順に特徴量をソートします。</li>
</ol>
<!--
Fisher, Rudin, and Dominici (2018) suggest in their paper to split the dataset in half and swap the values of feature j of the two halves instead of permuting feature j. 
This is exactly the same as permuting feature j, if you think about it. 
If you want a more accurate estimate, you can estimate the error of permuting feature j by pairing each instance with the value of feature j of each other instance (except with itself). 
This gives you a dataset of size `n(n-1)` to estimate the permutation error, and it takes a large amount of computation time. 
I can only recommend using the `n(n-1)` -method if you are serious about getting extremely accurate estimates.
-->
<p>Fisher, Rudin, Dominici (2018)の論文では、特徴量 j を並べ替える代わりに、データセットを半分に分割し、それらの間で特徴量 j の値を入れ替えることを提案しています。 これは考えてみれば、特徴量 j を並べ替えるのと全く同じです。 より正確な推定をしたい場合は、各インスタンスを他のインスタンスの特徴量 j の値とペアにすることで、特徴量 j の並べ替えの誤差を推定できます（自分自身とのペアを除く）。 これにより、並べ替え誤差を推定するための<code>n(n-1)</code>のサイズのデータセットが得られますが、膨大な計算時間がかかります。 厳密な推定値を得ることを真剣に考えている場合にのみ、この方法を使うことをお勧めします。</p>
<!--### Should I Compute Importance on Training or Test Data? {#feature-importance-data}-->
</div>
<div id="feature-importance-data" class="section level3">
<h3><span class="header-section-number">5.5.2</span> 特徴量の重要度は、学習データとテストデータのどちらで計算するべきか</h3>
<!--*tl;dr: I do not have a definite answer.*-->
<p><em>tl;dr: 明確な答えはありません。</em></p>
<!--
Answering the question about training or test data touches the fundamental question of what feature importance is.
The best way to understand the difference between feature importance based on training vs. based on test data is an "extreme" example.
I trained a support vector machine to predict a continuous, random target outcome given 50 random features (200 instances).
By "random" I mean that the target outcome is independent of the 50 features.
This is like predicting tomorrow's temperature given the latest lottery numbers.
If the model "learns" any relationships, then it overfits.
And in fact, the SVM did overfit on the training data.
The mean absolute error (short: mae) for the training data is 0.29 and for the test data 0.82, which is also the error of the best possible model that always predicts the mean outcome of 0 (mae of  0.78).
In other words, the SVM model is garbage.
What values for the feature importance would you expect for the 50 features of this overfitted SVM?
Zero because none of the features contribute to improved performance on unseen test data?
Or should the importances reflect how much the model depends on each of the features, regardless whether the learned relationships generalize to unseen data?
Let us take a look at how the distributions of feature importances for training and test data differ.
-->
<p>学習データかテストデータかの疑問に答えることは、特徴量重要度とは何かという根本的な問題に首を突っ込むことになります。 学習データとテストデータに基づいた特徴量重要度の違いを理解するためには、「極端な」例が一番適しています。 50個のランダムな特徴量（200のインスタンス）に基づいて、連続でランダムなターゲットを予測するサポートベクタマシンを学習しました。 「ランダム」というのは、ターゲットが 50 個の特徴量とは独立であることを意味しています。 これは最新の宝くじの番号から明日の気温を予測するようなものです。 もしモデルが何かの関係性を「学習した」のならば、それは過学習だといえます。 そして実際、SVMは学習データに過学習してしまいました。 平均絶対誤差 (MAE) は学習データにおいて 0.29 、テストデータにおいて 0.82 であり、これは常に平均値が0 (MAEは 0.78 ) となる結果を予測するような最良のモデルの誤差でもあります。 言い換えれば、このSVMモデルは何の役にも立たないということです。 この過学習したSVMの50個の特徴量の重要度にどんな値が期待できるでしょうか。 まだ見ぬテストデータのパフォーマンスを向上させるような特徴量がないことから 0 となるのでしょうか。 もしくは、学習された関係性がまだ見ぬデータに一般化されているかどうかにかかわらず、重要度はモデルがどれだけそれぞれの特徴量に依存しているかを反映するのでしょうか。 学習データとテストデータの特徴量重要度の分布がどのように異なるのか見ていきましょう。</p>
<!--
fig.cap="Distributions of feature importance values by data type. An SVM was trained on a regression dataset with 50 random features and 200 instances. The SVM overfits the data: Feature importance based on the training data shows many important features. Computed on unseen test data, the feature importances are close to a ratio of one (=unimportant)."
-->
<div class="figure"><span id="fig:feature-imp-sim"></span>
<img src="images/feature-imp-sim-1.png" alt="データのタイプによる特徴量重要度の分布。SVM は 50 個のランダムな特徴量を持つ 200 のインスタンスからなる回帰データセットで学習された。SVM は過学習しており、学習データに対する特徴量重要度では重要な特徴量がいくつか示されている。一方で、まだ見ぬテストデータに関して計算したところ、特徴量重要度は比率が 1 に近い値（＝重要ではない）となった。" width="1050" />
<p class="caption">
FIGURE 5.26: データのタイプによる特徴量重要度の分布。SVM は 50 個のランダムな特徴量を持つ 200 のインスタンスからなる回帰データセットで学習された。SVM は過学習しており、学習データに対する特徴量重要度では重要な特徴量がいくつか示されている。一方で、まだ見ぬテストデータに関して計算したところ、特徴量重要度は比率が 1 に近い値（＝重要ではない）となった。
</p>
</div>
<!--
It is unclear to me which of the two results is more desirable.
So I will try to make a case for both versions and let you decide for yourself.
-->
<p>2つの結果のどちらが望ましいかは明確ではありません。 そこで、双方のケースそれぞれについて述べてみますので、どちらがいいのかあなた自身で考えてみてください。</p>
<!--**The case for test data**-->
<p><strong>テストデータのケース</strong></p>
<!--
This is a simple case: 
Model error estimates based on training data are garbage -> feature importance relies on model error estimates -> feature importance based on training data is garbage.  
Really, it is one of the first things you learn in machine learning:
If you measure the model error (or performance) on the same data on which the model was trained, the measurement is usually too optimistic, which means that the model seems to work much better than it does in reality.
And since the permutation feature importance relies on measurements of the model error, we should use unseen test data.
The feature importance based on training data makes us mistakenly believe that features are important for the predictions, when in reality the model was just overfitting and the features were not important at all.
-->
<p>こちらについては単純です。 学習データに基づくモデル誤差の推定値は役に立たないということから、特徴量の重要度はモデル誤差の推定値に基づいているため、学習データに基づく特徴量重要度は役に立たないと言えます。 これは機械学習において一番最初に学ぶことですが、モデルを学習したときと同じデータでモデル誤差（もしくは性能）を測ると、それは常に楽観的すぎるものになり、実際よりもモデルの性能がよく思えてしまいます。 したがって、permutation feature importance はモデル誤差に基づいているので、まだ見ぬテストデータを用いるべきと言えます。 学習データに基づく特徴量重要度では、実際にはモデルがただ過学習しているだけで全く重要でない特徴量にもかかわらず、予測に重要であるという誤解が生まれてしまいます。</p>
<!--**The case for training data**-->
<p><strong>学習データのケース</strong></p>
<!--
The arguments for using training data are somewhat more difficult to formulate, but are IMHO just as compelling as the arguments for using test data.
We take another look at our garbage SVM.
Based on the training data, the most important feature was X42.
Let us look at a partial dependence plot of feature X42.
The partial dependence plot shows how the model output changes based on changes of the feature and does not rely on the generalization error.
It does not matter whether the PDP is computed with training or test data.
-->
<p>学習データを使用することに関する議論は形式化するのが難しいですが、私見ではテストデータに関する議論と同じくらい説得力があるように思えます。 違う観点からこの役に立たない SVM についてみていきましょう。 学習データによると、重要な特徴量は X42 でした。 X42 についての Partial Dependence Plot (PDP) について見ていきましょう。 PDP により、モデルの出力が特徴量の違いによってどのように変わるのかを見ることができ、これは汎化誤差に依存しません。 PDP では、学習データかテストデータかは問題ではないのです。</p>
<!--
fig.cap=sprintf("PDP of feature %s, which is the most important feature according to the feature importance based on the training data. The plot shows how the SVM depends on this feature to make predictions", max.imp$feature)
-->
<div class="figure"><span id="fig:garbage-svm-pdp"></span>
<img src="images/garbage-svm-pdp-1.png" alt="学習データに基づく最も重要な特徴量 X42 に関するPDP。このプロットによって、SVMが予測をする際にどのようにこの特徴量を用いているかが示されています。" width="1050" />
<p class="caption">
FIGURE 5.27: 学習データに基づく最も重要な特徴量 X42 に関するPDP。このプロットによって、SVMが予測をする際にどのようにこの特徴量を用いているかが示されています。
</p>
</div>
<!--
The plot clearly shows that the SVM has learned to rely on feature X42 for its predictions, but according to the feature importance based on the test data (1), it is not important.
Based on the training data, the importance is 1.19, reflecting that the model has learned to use this feature.
Feature importance based on the training data tells us which features are important for the model in the sense that it depends on them for making predictions.
-->
<p>このプロットによると明らかに SVM は特徴量X42 に依存して学習されていますが、テストデータに基づく特徴量重要度 (1) によると、この特徴量は重要ではありません。 学習データに基づく重要度は1.19であり、これはモデルがこの特徴量を用いるように学習されたことを示しています。 学習データに基づいた特徴量重要度は、予測をする際にそれらに依存しているという意味で、モデルにとって重要な特徴量はどれかを教えてくれます。</p>
<!--
As part of the case for using training data, I would like to introduce an argument against test data.
In practice, you want to use all your data to train your model to get the best possible model in the end.
This means no unused test data is left to compute the feature importance.
You have the same problem when you want to estimate the generalization error of your model.
If you would use (nested) cross-validation for the feature importance estimation, you would have the problem that the feature importance is not calculated on the final model with all the data, but on models with subsets of the data that might behave differently.
-->
<p>学習データを用いるケースの一環として、テストデータを使用することに対する反論を紹介したいと思います。 実際、最終的に可能な限り最良なモデルを得るために、すべてのデータを学習に使いたいと思うでしょう。 これは、特徴量重要度を計算するための未使用のテストデータが残されていないことを意味します。 モデルの汎化誤差を推定するときも同じような問題に直面します。 特徴量重要度を推定するために（ネストされた）交差検証を用いる場合、すべてのデータを学習に用いた最終的なモデルでは特徴量重要度が計算できず、一方でデータの一部を使ったモデルでは異なる振る舞いをしてしまうかもしれない、といった問題に直面するでしょう。</p>
<!--
In the end, you need to decide whether you want to know how much the model relies on each feature for making predictions (-> training data) or how much the feature contributes to the performance of the model on unseen data (-> test data).
To the best of my knowledge, there is no research addressing the question of training vs. test data.
It will require more thorough examination than my "garbage-SVM" example.
We need more research and more experience with these tools to gain a better understanding.
-->
<p>結局、モデルの予測がどの特徴量に頼っているのか（→学習データに基づく特徴量重要度）、もしくはどの特徴量がまだ見ぬデータに対するモデルの性能に寄与しているのか（→テストデータに基づく特徴量重要度）、どちらを知りたいのか決める必要があります。 私の知る限りでは、学習データとテストデータの疑問に関する研究はありません。 「役立たずの SVM」の例よりも徹底的な調査が必要でしょう。 より良い理解を得るためには、これらのツールについてより多くの研究と経験が必要になってくるでしょう。</p>
<!--
Next, we will look at some examples.
I based the importance computation on the training data, because I had to choose one and using the training data needed a few lines less code.
-->
<p>次に、例をいくつか見ていきましょう。 重要度の計算は、どちらか1つを選ばなければならず、必要となるコードが少なくて済むので、学習データに基づいています。</p>
<!--### Example and Interpretation-->
</div>
<div id="例と解釈" class="section level3">
<h3><span class="header-section-number">5.5.3</span> 例と解釈</h3>
<!--
I show examples for classification and regression. 
-->
<p>分類と回帰の例を示します。</p>
<!--**Cervical cancer (classification)**-->
<p><strong>子宮頸がん（分類）</strong></p>
<!--
We fit a random forest model to predict [cervical cancer](#cervical).
We measure the error increase by 1-AUC (1 minus the area under the ROC curve).
Features associated with a model error increase by a factor of 1 (= no change) were not important for predicting cervical cancer.
-->
<p><a href="cervical.html#cervical">子宮頸がん</a>を予測するためにランダムフォレストモデルを学習します。 誤差の増加を 1-AUC（1からROC曲線下面積を引いた値）で測定します。 並べ替えるとモデル誤差が 1 倍に増加する（＝変化なし）特徴量は、子宮頸がんの予測には重要ではありません。</p>
<!--
fig.cap = sprintf("The importance of each of the features for predicting cervical cancer with a random forest. The most important feature was %s. Permuting %s resulted in an increase in 1-AUC by a factor of %.2f", most_imp, most_imp, max(imp.dat$importance))
-->
<div class="figure"><span id="fig:importance-cervical"></span>
<img src="images/importance-cervical-1.png" alt="ランダムフォレストを用いて子宮頸がんを予測するための特徴量重要度。最も重要な特徴量は Hormonal.Contraceptives..years. で、1-AUCは 6.13 倍に増加した。" width="1050" />
<p class="caption">
FIGURE 5.28: ランダムフォレストを用いて子宮頸がんを予測するための特徴量重要度。最も重要な特徴量は Hormonal.Contraceptives..years. で、1-AUCは 6.13 倍に増加した。
</p>
</div>
<!--
The feature with the highest importance was Hormonal.Contraceptives..years. associated with an error increase of 6.13 after permutation.
-->
<p>最も重要度の高い特徴量は Hormonal.Contraceptives..years. であり、誤差は 6.13 増加することがわかりました。</p>
<!--**Bike sharing (regression)**-->
<p><strong>レンタル自転車（回帰）</strong></p>
<!--
We fit a support vector machine model to predict [the number of rented bikes](#bike-data), given weather conditions and calendar information.
As error measurement we use the mean absolute error.
-->
<p>SVM を用いて、気象条件とカレンダーの情報が与えられたときの<a href="bike-data.html#bike-data">レンタル自転車の台数</a>を予測します。 誤差の測定には平均絶対誤差を使用します。</p>
<!--
fig.cap = sprintf("The importance for each of the features in predicting bike counts with a support vector machine. The most important feature was %s, the least important was %s.", imp.dat$feature[best], imp.dat$feature[worst])
-->
<div class="figure"><span id="fig:importance-bike"></span>
<img src="images/importance-bike-1.png" alt="SVM を用いた自転車レンタル台数を予測したときの各特徴量の重要度。最も重要な特徴量は temp で、最も重要でない特徴量は holiday でした。" width="1050" />
<p class="caption">
FIGURE 5.29: SVM を用いた自転車レンタル台数を予測したときの各特徴量の重要度。最も重要な特徴量は temp で、最も重要でない特徴量は holiday でした。
</p>
</div>
<!--### Advantages-->
</div>
<div id="利点-2" class="section level3">
<h3><span class="header-section-number">5.5.4</span> 利点</h3>
<!--
**Nice interpretation**: Feature importance is the increase in model error when the feature's information is destroyed.
-->
<p>特徴量重要度が高いということは、特徴量の情報が破壊されたときにモデル誤差が増加するという <strong>すばらしい解釈</strong> が可能です。</p>
<!--
Feature importance provides a **highly compressed, global insight** into the model's behavior. 
-->
<p>特徴量重要度は、モデルの振る舞いについて <strong>高度に圧縮されたグローバルな洞察</strong> を提供します。</p>
<!--
A positive aspect of using the error ratio instead of the error difference is that the feature importance measurements are **comparable across different problems**. 
-->
<p>誤差の差分の代わりに誤差の比を使用することの良い側面は、特徴量重要度の測定値が<strong>異なる問題間で比較可能</strong>であることです。</p>
<!--
The importance measure automatically **takes into account all interactions** with other features.
By permuting the feature you also destroy the interaction effects with other features. 
This means that the permutation feature importance takes into account both the  main feature effect and the interaction effects on model performance.
This is also a disadvantage because the importance of the interaction between two features is included in the importance measurements of both features.
This means that the feature importances do not add up to the total drop in performance, but the sum is larger.
Only if there is no interaction between the features, as in a linear model, the importances add up approximately.
-->
<p>重要度の計算は、自動的に他の特徴量との<strong>すべての相互作用を考慮に入れます</strong>。 特徴量を並べ替えることは、他の特徴量との相互作用の効果も破壊します。 これは、並べ替えた特徴量の重要度が，モデル性能における特徴量としての主効果と相互作用による効果の両方を考慮に入れることを意味します。 このことは、2つの特徴量間の相互作用の重要度が、両方の特徴量の重要度測定に含まれているため、欠点でもあります。 これは、特徴量重要度が性能の全体的な低下の合計ではなく、総和が大きくなってしまうことを意味します。 線形モデルのように特徴間の相互作用がない場合にのみ、重要度はほぼ総和になります。</p>
<!--
Permutation feature importance **does not require retraining the model**.
Some other methods suggest deleting a feature, retraining the model and then comparing the model error.
Since the retraining of a machine learning model can take a long time, "only" permuting a feature can save a lot of time.
Importance methods that retrain the model with a subset of features appear intuitive at first glance, but the model with the reduced data is meaningless for the feature importance.
We are interested in the feature importance of a fixed model.
Retraining with a reduced dataset creates a different model than the one we are interested in.
Suppose you train a sparse linear model (with Lasso) with a fixed number of features with a non-zero weight.
The dataset has 100 features, you set the number of non-zero weights to 5.
You analyze the importance of one of the features that have a non-zero weight.
You remove the feature and retrain the model.
The model performance remains the same because another equally good feature gets a non-zero weight and your conclusion would be that the feature was not important.
Another example:
The model is a decision tree and we analyze the importance of the feature that was chosen as the first split.
You remove the feature and retrain the model. 
Since another feature is chosen as the first split, the whole tree can be very different, which means that we compare the error rates of (potentially) completely different trees to decide how important that feature is for one of the trees.
-->
<p>Permutation feature importance は <strong>モデルの再学習を必要としません</strong>。 他の方法の中には，特徴量を削除してモデルを再学習し、モデル誤差を比較するものもあります。 機械学習モデルの再学習には長い時間がかかるので、特徴量を並べ替える &quot;だけ&quot; で済めば多くの時間を節約できます。 特徴量の一部でモデルを再学習する方法は一見すると直感的に見えますが、データを減らしたモデルでは特徴量重要度に意味がありません。 我々が知りたいのは、モデルを固定して得られる特徴量重要度なのです。 削減されたデータセットで再学習すると、我々が興味を持っているモデルとは異なるモデルが得られます。 例えば、スパースな線形モデル（Lassoを使用）を、使用する特徴量の数を固定して重みを学習したとします。 データセットには 100 個の特徴量があり、非ゼロの重みの数を 5 に設定します。 非ゼロの重みを持つ特徴量のうちの 1 つの重要度を分析します。 次に、その特徴量を削除してモデルを再学習します。 その結果、同じように良い特徴量に非ゼロの重みが与えら、モデルの性能が変わらなかったとすると、先ほどの特徴量は重要ではないという結論に至ります。 また、こんなことも起こり得ます。 モデルは決定木で、最初の分割として選ばれた特徴量の重要度を分析します。 その特徴量を削除してモデルを再学習します。 別の特徴量が最初の分割として選択されるので、木全体が全く異なったものになります。つまり、（潜在的に）全く異なる木の誤差率を比較して、その特徴量が元の木にとって、どれだけ重要かを決定することになってしまうのです。</p>
<!--### Disadvantages-->
</div>
<div id="欠点-2" class="section level3">
<h3><span class="header-section-number">5.5.5</span> 欠点</h3>
<!--
It is very **unclear whether you should use training or test data** to compute the feature importance.
-->
<p>特徴量重要度を計算するために<strong>学習データを用いるべきか、テストデータを用いるべきかは明らかではありません</strong>。</p>
<!--
Permutation feature importance is **linked to the error of the model**.
This is not inherently bad, but in some cases not what you need.
In some cases, you might prefer to know how much the model's output varies for a feature without considering what it means for performance.
For example, you want to find out how robust your model's output is when someone manipulates the features. 
In this case, you would not be interested in how much the model performance decreases when a feature is permuted, but how much of the model's output variance is explained by each feature. 
Model variance (explained by the features) and feature importance correlate strongly when the model generalizes well (i.e. it does not overfit).
-->
<p>Permutation feature importance は <strong>モデル誤差と関連しています </strong>。 これは本質的に悪いことではありませんが、場合によっては我々が求めるものと異なります。 ある特徴量がモデルの性能に対しどういう意味を持つかを考えることなく、その特徴量によってモデルの出力がどう変わるかを知りたいケースもあります。 例えば、誰かによって特徴量が改ざんされた場合にモデルの出力がどの程度頑健か知りたい場合です。 この場合には、特徴量が並べ替えられた場合にどの程度モデルの性能が落ちるかではなく、モデルの出力の変化が各特徴量によってどの程度説明されるのかを知りたいはずです。 （特徴量によって説明される）モデルの出力変化と特徴量重要度は、モデルが良く汎化されている（つまり過学習していない）時には、強く相関します。</p>
<!--
You **need access to the true outcome**. 
If someone only provides you with the model and unlabeled data -- but not the true outcome -- you cannot compute the permutation feature importance.
-->
<p><strong>真の結果にアクセスできる必要があります</strong>。 もしモデルとラベル付けされていないデータのみが与えられ、真の結果がなかったならば、並べ替えた特徴量の重要度は計算できません。</p>
<!--
The permutation feature importance depends on shuffling the feature, which adds randomness to the measurement.
When the permutation is repeated, the **results might vary greatly**.
Repeating the permutation and averaging the importance measures over repetitions stabilizes the measure, but increases the time of computation.
-->
<p>permutation feature importance は特徴量のシャッフルに依存しているため、推定結果にランダム性を持ちます。 何度も計算すると、結果が大きく変わるかもしれません。 重要度の測定値を繰り返し平均すれば、推定結果は安定しますが、計算にかかる時間は増大します。</p>
<!--
If features are correlated, the permutation feature importance **can be biased by unrealistic data instances**. 
The problem is the same as with [partial dependence plots](#pdp):
The permutation of features produces unlikely data instances when two or more features are correlated.
When they are positively correlated (like height and weight of a person) and I shuffle one of the features, I create new instances that are unlikely or even physically impossible (2 meter person weighing 30 kg for example), yet I use these new instances to measure the importance.
In other words, for the permutation feature importance of a correlated feature, we consider how much the model performance decreases when we exchange the feature with values we would never observe in reality.
Check if the features are strongly correlated and be careful about the interpretation of the feature importance if they are.
-->
<p>もし特徴量に相関があるのならば、permutation feature importance は現実的ではないインスタンスによって結果が歪められるかもしれません。 これは、<a href="pdp.html#pdp">Partial Dependence Plots</a> と同じ問題です。 2つ以上の特徴量に相関がある場合、特徴量の並べ替えを行うことで、起こりえないデータを作り出してしまいます。 （人間の体重と身長のように）正の相関があり、その1つをシャッフルするとき、ありえないインスタンス（例えば身長 2m, 体重 30kg）を作り出してしまいますが、そのようなインスタンスも特徴量重要度の計算に使用されてしまいます。 言い換えれば、他の特徴量と相関のある場合の permutation feature importance は、特徴量の入れ替えによって得られた現実に観測しえないようなデータに対してモデルの性能がどの程度落ちたかについて考えていると言えます。 特徴量が強く相関しているかどうかをチェックし、相関がある場合は、特徴量重要度の解釈に注意するようにしましょう。</p>
<!--
Another tricky thing:
**Adding a correlated feature can decrease the importance of the associated feature** by splitting the importance between both features.
Let me give you an example of what I mean by "splitting" feature importance:
We want to predict the probability of rain and use the temperature at 8:00 AM of the day before as a feature along with other uncorrelated features.
I train a random forest and it turns out that the temperature is the most important feature and all is well and I sleep well the next night.
Now imagine another scenario in which I additionally include the temperature at 9:00 AM as a feature that is strongly correlated with the temperature at 8:00 AM.
The temperature at 9:00 AM does not give me much additional information if I already know the temperature at 8:00 AM.
But having more features is always good, right?
I train a random forest with the two temperature features and the uncorrelated features.
Some of the trees in the random forest pick up the 8:00 AM temperature, others the 9:00 AM temperature, again others both and again others none. 
The two temperature features together have a bit more importance than the single temperature feature before, but instead of being at the top of the list of important features, each temperature is now somewhere in the middle.
By introducing a correlated feature, I kicked the most important feature from the top of the importance ladder to mediocrity.
On one hand this is fine, because it simply reflects the behavior of the underlying machine learning model, here the random forest. 
The 8:00 AM temperature has simply become less important because the model can now rely on the 9:00 AM measurement as well.
On the other hand, it makes the interpretation of the feature importance considerably more difficult.
Imagine you want to check the features for measurement errors.
The check is expensive and you decide to check only the top 3 of the most important features.
In the first case you would check the temperature, in the second case you would not include any temperature feature just because they now share the importance.
Even though the importance values might make sense at the level of model behavior, it is confusing if you have correlated features.
-->
<p>もう1つ、トリッキーなことがあります。 <strong>相関した特徴量を追加すると</strong>、両方の特徴量の間で重要度を分割することで、<strong>関連する特徴量の重要度が低下する</strong>、ということが起こり得ます。 特徴量重要度を「分割する」とはどういうことか、例を挙げてみましょう。 他の相関のない特徴量とともに前日の午前8時の気温を特徴量として使用し、降水確率を予測します。 ランダムフォレストで学習し、気温が最も重要な特徴量だとわかり、うまくいったので夜はよく眠れました。 では、午前8時の気温と強く相関するような午前9時の気温を追加したときを想像してみましょう。 午前8時の気温をすでに知っていたならば、午前9時の気温はそれほど多くの情報を与えません。 でも、特徴量はたくさんあるほうが良いはずです。 ランダムフォレストを2つの気温の特徴量と、他の相関のない特徴量から学習しました。 ランダムフォレストの個々の木は午前8時の気温を選んだり、午前9時の気温を選んだり、あるいは両方選んだりどちらも選ばなかったりしました。 2つの気温の特徴量のどちらもは、1つだけで学習したときよりも少しだけ重要度が高くなっていますが、特徴量を重要な順にリストかすると、どちらの気温も真ん中ぐらいの順位となっていました。 相関する特徴量を入れることによって、最も重要な特徴量をトップから平凡なものへと蹴り落してしまいました。 ある意味では、これは単に機械学習モデル（ここではランダムフォレスト）の動作を反映しているだけなので、問題ありません。 モデルが午前9時の気温にも頼ることができるようになったので、単純に午前8時の気温の重要度が低くなっただけです。 一方で、これによって特徴量重要度の解釈がかなり難しいものになってしまいます。 計測誤差について特徴量をチェックすることを想像してみてください。 チェックは大変なので、重要な特徴量を上位3つだけチェックすることに決めたとしましょう。 最初のケース(午前8時の気温のみ)では、最も重要な特徴量である気温をチェックしますが、2つ目のケース(午前9時の気温も含む) では、重要度がシェアされてしまっているので重要度の上位3つに気温の特徴量は1つも含まれません。 相関のある特徴量がある場合、重要度の値はモデルの動作レベルでは理解しうるものであったとしても、混乱を招く恐れがあります。</p>
<!--### Software and Alternatives-->
</div>
<div id="ソフトウェアと代替手法-4" class="section level3">
<h3><span class="header-section-number">5.5.6</span> ソフトウェアと代替手法</h3>
<!--
The `iml` R package was used for the examples.
The R packages `DALEX` and `vip`, as well as the Python library `alibi`, also implement model-agnostic permutation feature importance.
-->
<p>例では、R パッケージの <code>iml</code> を用いました。 R パッケージ <code>DALEX</code> や <code>vip</code>、Pythonライブラリ <code>alibi</code> もまた、モデルに依存しない、permutation feature importance の実装があります。</p>
<!--
An algorithm called [PIMP](https://academic.oup.com/bioinformatics/article/26/10/1340/193348) adapts the feature importance algorithm to provide p-values for the importances.
-->
<p><a href="https://academic.oup.com/bioinformatics/article/26/10/1340/193348">PIMP</a> と呼ばれるアルゴリズムは、重要度の p値 を提供するために、特徴量重要度アルゴリズムを改良しています。</p>

<!--{pagebreak}-->
</div>
</div>
<div class="footnotes">
<hr />
<ol start="34">
<li id="fn34"><p>Breiman, Leo.“Random Forests.” Machine Learning 45 (1). Springer: 5-32 (2001).<a href="feature-importance.html#fnref34">↩</a></p></li>
<li id="fn35"><p>Fisher, Aaron, Cynthia Rudin, and Francesca Dominici. “Model Class Reliance: Variable importance measures for any machine learning model class, from the ‘Rashomon’ perspective.” <a href="http://arxiv.org/abs/1801.01489" class="uri">http://arxiv.org/abs/1801.01489</a> (2018).<a href="feature-importance.html#fnref35">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="interaction.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="global.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/christophM/interpretable-ml-book/edit/master/05.6-agnostic-permfeatimp.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
