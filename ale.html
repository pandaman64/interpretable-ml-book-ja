<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>5.3 Accumulated Local Effects (ALE) Plot | Interpretable Machine Learning</title>
  <meta name="description" content="Machine learning algorithms usually operate as black boxes and it is unclear how they derived a certain decision. This book is a guide for practitioners to make machine learning decisions interpretable." />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="5.3 Accumulated Local Effects (ALE) Plot | Interpretable Machine Learning" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Machine learning algorithms usually operate as black boxes and it is unclear how they derived a certain decision. This book is a guide for practitioners to make machine learning decisions interpretable." />
  <meta name="github-repo" content="christophM/interpretable-ml-book" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="5.3 Accumulated Local Effects (ALE) Plot | Interpretable Machine Learning" />
  
  <meta name="twitter:description" content="Machine learning algorithms usually operate as black boxes and it is unclear how they derived a certain decision. This book is a guide for practitioners to make machine learning decisions interpretable." />
  

<meta name="author" content="Christoph Molnar" />


<meta name="date" content="2021-04-20" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="ice.html"/>
<link rel="next" href="interaction.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<!-- Global site tag (gtag.js) - Google Analytics -->
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-110543840-1', 'https://christophm.github.io/interpretable-ml-book/', {
  'anonymizeIp': true
  , 'storage': 'none'
  , 'clientId': window.localStorage.getItem('ga_clientId')
});
ga(function(tracker) {
  window.localStorage.setItem('ga_clientId', tracker.get('clientId'));
});
ga('send', 'pageview');
</script>

<link rel="stylesheet" type="text/css" href="css/cookieconsent.min.css" />
<script src="javascript/cookieconsent.min.js"></script>
<script>
window.addEventListener("load", function(){
window.cookieconsent.initialise({
  "palette": {
    "popup": {
      "background": "#000"
    },
    "button": {
      "background": "#f1d600"
    }
  },
  "position": "bottom-right",
  "content": {
    "message": "This website uses cookies for Google Analytics so that I know how many people are reading the book and which chapters are the most popular. The book website doesn't collect any personal data."
  }
})});
</script>



<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Interpretable machine learning</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>要約</a></li>
<li class="chapter" data-level="" data-path="著者による序文.html"><a href="著者による序文.html"><i class="fa fa-check"></i>著者による序文</a></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> イントロダクション</a><ul>
<li class="chapter" data-level="1.1" data-path="storytime.html"><a href="storytime.html"><i class="fa fa-check"></i><b>1.1</b> 物語の時間</a><ul>
<li class="chapter" data-level="" data-path="storytime.html"><a href="storytime.html#稲妻は二度と打たない"><i class="fa fa-check"></i>稲妻は二度と打たない</a></li>
<li class="chapter" data-level="" data-path="storytime.html"><a href="storytime.html#信用失墜"><i class="fa fa-check"></i>信用失墜</a></li>
<li class="chapter" data-level="" data-path="storytime.html"><a href="storytime.html#フェルミのペーパークリップ"><i class="fa fa-check"></i>フェルミのペーパー・クリップ</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="機械学習とは何か.html"><a href="機械学習とは何か.html"><i class="fa fa-check"></i><b>1.2</b> 機械学習とは何か？</a></li>
<li class="chapter" data-level="1.3" data-path="terminology.html"><a href="terminology.html"><i class="fa fa-check"></i><b>1.3</b> 専門用語</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="interpretability.html"><a href="interpretability.html"><i class="fa fa-check"></i><b>2</b> 解釈可能性</a><ul>
<li class="chapter" data-level="2.1" data-path="interpretability-importance.html"><a href="interpretability-importance.html"><i class="fa fa-check"></i><b>2.1</b> 解釈可能性の重要性</a></li>
<li class="chapter" data-level="2.2" data-path="解釈可能な手法の分類.html"><a href="解釈可能な手法の分類.html"><i class="fa fa-check"></i><b>2.2</b> 解釈可能な手法の分類</a></li>
<li class="chapter" data-level="2.3" data-path="解釈可能性の範囲.html"><a href="解釈可能性の範囲.html"><i class="fa fa-check"></i><b>2.3</b> 解釈可能性の範囲</a><ul>
<li class="chapter" data-level="2.3.1" data-path="解釈可能性の範囲.html"><a href="解釈可能性の範囲.html#アルゴリズムの透明性"><i class="fa fa-check"></i><b>2.3.1</b> アルゴリズムの透明性</a></li>
<li class="chapter" data-level="2.3.2" data-path="解釈可能性の範囲.html"><a href="解釈可能性の範囲.html#全体的なモデルの解釈可能性"><i class="fa fa-check"></i><b>2.3.2</b> 全体的なモデルの解釈可能性</a></li>
<li class="chapter" data-level="2.3.3" data-path="解釈可能性の範囲.html"><a href="解釈可能性の範囲.html#モジュールレベルのモデルの全体的な解釈可能性"><i class="fa fa-check"></i><b>2.3.3</b> モジュールレベルのモデルの全体的な解釈可能性</a></li>
<li class="chapter" data-level="2.3.4" data-path="解釈可能性の範囲.html"><a href="解釈可能性の範囲.html#単一の予測に対する局所的な解釈"><i class="fa fa-check"></i><b>2.3.4</b> 単一の予測に対する局所的な解釈</a></li>
<li class="chapter" data-level="2.3.5" data-path="解釈可能性の範囲.html"><a href="解釈可能性の範囲.html#予測のグループに対する局所的な解釈"><i class="fa fa-check"></i><b>2.3.5</b> 予測のグループに対する局所的な解釈</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="解釈可能性の評価.html"><a href="解釈可能性の評価.html"><i class="fa fa-check"></i><b>2.4</b> 解釈可能性の評価</a></li>
<li class="chapter" data-level="2.5" data-path="properties.html"><a href="properties.html"><i class="fa fa-check"></i><b>2.5</b> 説明に関する性質</a></li>
<li class="chapter" data-level="2.6" data-path="explanation.html"><a href="explanation.html"><i class="fa fa-check"></i><b>2.6</b> 人間に優しい説明</a><ul>
<li class="chapter" data-level="2.6.1" data-path="explanation.html"><a href="explanation.html#説明とはなにか"><i class="fa fa-check"></i><b>2.6.1</b> 説明とはなにか</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="data.html"><a href="data.html"><i class="fa fa-check"></i><b>3</b> データセット</a><ul>
<li class="chapter" data-level="3.1" data-path="bike-data.html"><a href="bike-data.html"><i class="fa fa-check"></i><b>3.1</b> 自転車レンタル (回帰)</a></li>
<li class="chapter" data-level="3.2" data-path="spam-data.html"><a href="spam-data.html"><i class="fa fa-check"></i><b>3.2</b> YouTube スパムコメント (テキスト分類)</a></li>
<li class="chapter" data-level="3.3" data-path="cervical.html"><a href="cervical.html"><i class="fa fa-check"></i><b>3.3</b> 子宮頸がんのリスク要因(クラス分類)</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="simple.html"><a href="simple.html"><i class="fa fa-check"></i><b>4</b> 解釈可能なモデル</a><ul>
<li class="chapter" data-level="4.1" data-path="limo.html"><a href="limo.html"><i class="fa fa-check"></i><b>4.1</b> 線形回帰</a><ul>
<li class="chapter" data-level="4.1.1" data-path="limo.html"><a href="limo.html#解釈"><i class="fa fa-check"></i><b>4.1.1</b> 解釈</a></li>
<li class="chapter" data-level="4.1.2" data-path="limo.html"><a href="limo.html#例"><i class="fa fa-check"></i><b>4.1.2</b> 例</a></li>
<li class="chapter" data-level="4.1.3" data-path="limo.html"><a href="limo.html#可視化による解釈"><i class="fa fa-check"></i><b>4.1.3</b> 可視化による解釈</a></li>
<li class="chapter" data-level="4.1.4" data-path="limo.html"><a href="limo.html#個々の予測に対する説明"><i class="fa fa-check"></i><b>4.1.4</b> 個々の予測に対する説明</a></li>
<li class="chapter" data-level="4.1.5" data-path="limo.html"><a href="limo.html#カテゴリカル特徴量のエンコーディング"><i class="fa fa-check"></i><b>4.1.5</b> カテゴリカル特徴量のエンコーディング</a></li>
<li class="chapter" data-level="4.1.6" data-path="limo.html"><a href="limo.html#線形モデルは良い説明を与えるか"><i class="fa fa-check"></i><b>4.1.6</b> 線形モデルは良い説明を与えるか?</a></li>
<li class="chapter" data-level="4.1.7" data-path="limo.html"><a href="limo.html#sparse-linear"><i class="fa fa-check"></i><b>4.1.7</b> スパースな線形モデル</a></li>
<li class="chapter" data-level="4.1.8" data-path="limo.html"><a href="limo.html#長所"><i class="fa fa-check"></i><b>4.1.8</b> 長所</a></li>
<li class="chapter" data-level="4.1.9" data-path="limo.html"><a href="limo.html#短所"><i class="fa fa-check"></i><b>4.1.9</b> 短所</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="logistic.html"><a href="logistic.html"><i class="fa fa-check"></i><b>4.2</b> ロジスティック回帰</a><ul>
<li class="chapter" data-level="4.2.1" data-path="logistic.html"><a href="logistic.html#線形回帰モデルを分類のために使うと何がいけないか"><i class="fa fa-check"></i><b>4.2.1</b> 線形回帰モデルを分類のために使うと何がいけないか。</a></li>
<li class="chapter" data-level="4.2.2" data-path="logistic.html"><a href="logistic.html#理論"><i class="fa fa-check"></i><b>4.2.2</b> 理論</a></li>
<li class="chapter" data-level="4.2.3" data-path="logistic.html"><a href="logistic.html#解釈性"><i class="fa fa-check"></i><b>4.2.3</b> 解釈性</a></li>
<li class="chapter" data-level="4.2.4" data-path="logistic.html"><a href="logistic.html#例-1"><i class="fa fa-check"></i><b>4.2.4</b> 例</a></li>
<li class="chapter" data-level="4.2.5" data-path="logistic.html"><a href="logistic.html#長所と短所"><i class="fa fa-check"></i><b>4.2.5</b> 長所と短所</a></li>
<li class="chapter" data-level="4.2.6" data-path="logistic.html"><a href="logistic.html#ソフトウェア"><i class="fa fa-check"></i><b>4.2.6</b> ソフトウェア</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="extend-lm.html"><a href="extend-lm.html"><i class="fa fa-check"></i><b>4.3</b> GLM、GAM、その他</a><ul>
<li class="chapter" data-level="4.3.1" data-path="extend-lm.html"><a href="extend-lm.html#glm"><i class="fa fa-check"></i><b>4.3.1</b> 結果が正規分布に従わない場合 - GLMs</a></li>
<li class="chapter" data-level="4.3.2" data-path="extend-lm.html"><a href="extend-lm.html#lm-interact"><i class="fa fa-check"></i><b>4.3.2</b> 相互作用</a></li>
<li class="chapter" data-level="4.3.3" data-path="extend-lm.html"><a href="extend-lm.html#gam"><i class="fa fa-check"></i><b>4.3.3</b> 非線形効果 - GAM</a></li>
<li class="chapter" data-level="4.3.4" data-path="extend-lm.html"><a href="extend-lm.html#長所-1"><i class="fa fa-check"></i><b>4.3.4</b> 長所</a></li>
<li class="chapter" data-level="4.3.5" data-path="extend-lm.html"><a href="extend-lm.html#短所-1"><i class="fa fa-check"></i><b>4.3.5</b> 短所</a></li>
<li class="chapter" data-level="4.3.6" data-path="extend-lm.html"><a href="extend-lm.html#ソフトウェア-1"><i class="fa fa-check"></i><b>4.3.6</b> ソフトウェア</a></li>
<li class="chapter" data-level="4.3.7" data-path="extend-lm.html"><a href="extend-lm.html#more-lm-extension"><i class="fa fa-check"></i><b>4.3.7</b> さらなる拡張</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="tree.html"><a href="tree.html"><i class="fa fa-check"></i><b>4.4</b> 決定木</a><ul>
<li class="chapter" data-level="4.4.1" data-path="tree.html"><a href="tree.html#決定木の解釈"><i class="fa fa-check"></i><b>4.4.1</b> 決定木の解釈</a></li>
<li class="chapter" data-level="4.4.2" data-path="tree.html"><a href="tree.html#例-2"><i class="fa fa-check"></i><b>4.4.2</b> 例</a></li>
<li class="chapter" data-level="4.4.3" data-path="tree.html"><a href="tree.html#長所-2"><i class="fa fa-check"></i><b>4.4.3</b> 長所</a></li>
<li class="chapter" data-level="4.4.4" data-path="tree.html"><a href="tree.html#短所-2"><i class="fa fa-check"></i><b>4.4.4</b> 短所</a></li>
<li class="chapter" data-level="4.4.5" data-path="tree.html"><a href="tree.html#ソフトウェア-2"><i class="fa fa-check"></i><b>4.4.5</b> ソフトウェア</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="rules.html"><a href="rules.html"><i class="fa fa-check"></i><b>4.5</b> 決定規則</a><ul>
<li class="chapter" data-level="4.5.1" data-path="rules.html"><a href="rules.html#単一の特徴量による規則学習-oner"><i class="fa fa-check"></i><b>4.5.1</b> 単一の特徴量による規則学習 (OneR)</a></li>
<li class="chapter" data-level="4.5.2" data-path="rules.html"><a href="rules.html#sequential-covering"><i class="fa fa-check"></i><b>4.5.2</b> Sequential Covering</a></li>
<li class="chapter" data-level="4.5.3" data-path="rules.html"><a href="rules.html#bayesian-rule-lists"><i class="fa fa-check"></i><b>4.5.3</b> Bayesian Rule Lists</a></li>
<li class="chapter" data-level="4.5.4" data-path="rules.html"><a href="rules.html#長所-3"><i class="fa fa-check"></i><b>4.5.4</b> 長所</a></li>
<li class="chapter" data-level="4.5.5" data-path="rules.html"><a href="rules.html#短所-3"><i class="fa fa-check"></i><b>4.5.5</b> 短所</a></li>
<li class="chapter" data-level="4.5.6" data-path="rules.html"><a href="rules.html#ソフトウェアと代替手法"><i class="fa fa-check"></i><b>4.5.6</b> ソフトウェアと代替手法</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="rulefit.html"><a href="rulefit.html"><i class="fa fa-check"></i><b>4.6</b> RuleFit</a><ul>
<li class="chapter" data-level="4.6.1" data-path="rulefit.html"><a href="rulefit.html#解釈と例"><i class="fa fa-check"></i><b>4.6.1</b> 解釈と例</a></li>
<li class="chapter" data-level="4.6.2" data-path="rulefit.html"><a href="rulefit.html#理論-1"><i class="fa fa-check"></i><b>4.6.2</b> 理論</a></li>
<li class="chapter" data-level="4.6.3" data-path="rulefit.html"><a href="rulefit.html#長所-4"><i class="fa fa-check"></i><b>4.6.3</b> 長所</a></li>
<li class="chapter" data-level="4.6.4" data-path="rulefit.html"><a href="rulefit.html#短所-4"><i class="fa fa-check"></i><b>4.6.4</b> 短所</a></li>
<li class="chapter" data-level="4.6.5" data-path="rulefit.html"><a href="rulefit.html#ソフトウェアと代替手法-1"><i class="fa fa-check"></i><b>4.6.5</b> ソフトウェアと代替手法</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="other-interpretable.html"><a href="other-interpretable.html"><i class="fa fa-check"></i><b>4.7</b> その他の解釈可能なモデル</a><ul>
<li class="chapter" data-level="4.7.1" data-path="other-interpretable.html"><a href="other-interpretable.html#単純ベイズ分類器-naive-bayes-classifier"><i class="fa fa-check"></i><b>4.7.1</b> 単純ベイズ分類器 (Naive Bayes Classifier)</a></li>
<li class="chapter" data-level="4.7.2" data-path="other-interpretable.html"><a href="other-interpretable.html#k近傍法"><i class="fa fa-check"></i><b>4.7.2</b> k近傍法</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="agnostic.html"><a href="agnostic.html"><i class="fa fa-check"></i><b>5</b> モデル非依存(Model-Agnostic)な手法</a><ul>
<li class="chapter" data-level="5.1" data-path="pdp.html"><a href="pdp.html"><i class="fa fa-check"></i><b>5.1</b> Partial Dependence Plot (PDP)</a><ul>
<li class="chapter" data-level="5.1.1" data-path="pdp.html"><a href="pdp.html#例-3"><i class="fa fa-check"></i><b>5.1.1</b> 例</a></li>
<li class="chapter" data-level="5.1.2" data-path="pdp.html"><a href="pdp.html#長所-5"><i class="fa fa-check"></i><b>5.1.2</b> 長所</a></li>
<li class="chapter" data-level="5.1.3" data-path="pdp.html"><a href="pdp.html#短所-5"><i class="fa fa-check"></i><b>5.1.3</b> 短所</a></li>
<li class="chapter" data-level="5.1.4" data-path="pdp.html"><a href="pdp.html#ソフトウェアと代替手法-2"><i class="fa fa-check"></i><b>5.1.4</b> ソフトウェアと代替手法</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="ice.html"><a href="ice.html"><i class="fa fa-check"></i><b>5.2</b> Individual Conditional Expectation (ICE)</a><ul>
<li class="chapter" data-level="5.2.1" data-path="ice.html"><a href="ice.html#例-4"><i class="fa fa-check"></i><b>5.2.1</b> 例</a></li>
<li class="chapter" data-level="5.2.2" data-path="ice.html"><a href="ice.html#長所-6"><i class="fa fa-check"></i><b>5.2.2</b> 長所</a></li>
<li class="chapter" data-level="5.2.3" data-path="ice.html"><a href="ice.html#短所-6"><i class="fa fa-check"></i><b>5.2.3</b> 短所</a></li>
<li class="chapter" data-level="5.2.4" data-path="ice.html"><a href="ice.html#ソフトウェアと代替手法-3"><i class="fa fa-check"></i><b>5.2.4</b> ソフトウェアと代替手法</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="ale.html"><a href="ale.html"><i class="fa fa-check"></i><b>5.3</b> Accumulated Local Effects (ALE) Plot</a><ul>
<li class="chapter" data-level="5.3.1" data-path="ale.html"><a href="ale.html#モチベーションと直感"><i class="fa fa-check"></i><b>5.3.1</b> モチベーションと直感</a></li>
<li class="chapter" data-level="5.3.2" data-path="ale.html"><a href="ale.html#理論-2"><i class="fa fa-check"></i><b>5.3.2</b> 理論</a></li>
<li class="chapter" data-level="5.3.3" data-path="ale.html"><a href="ale.html#予測"><i class="fa fa-check"></i><b>5.3.3</b> 予測</a></li>
<li class="chapter" data-level="5.3.4" data-path="ale.html"><a href="ale.html#例-6"><i class="fa fa-check"></i><b>5.3.4</b> 例</a></li>
<li class="chapter" data-level="5.3.5" data-path="ale.html"><a href="ale.html#利点"><i class="fa fa-check"></i><b>5.3.5</b> 利点</a></li>
<li class="chapter" data-level="5.3.6" data-path="ale.html"><a href="ale.html#欠点"><i class="fa fa-check"></i><b>5.3.6</b> 欠点</a></li>
<li class="chapter" data-level="5.3.7" data-path="ale.html"><a href="ale.html#実装と代替手法"><i class="fa fa-check"></i><b>5.3.7</b> 実装と代替手法</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="interaction.html"><a href="interaction.html"><i class="fa fa-check"></i><b>5.4</b> 特徴量の相互作用</a><ul>
<li class="chapter" data-level="5.4.1" data-path="interaction.html"><a href="interaction.html#特徴量の相互作用とは"><i class="fa fa-check"></i><b>5.4.1</b> 特徴量の相互作用とは</a></li>
<li class="chapter" data-level="5.4.2" data-path="interaction.html"><a href="interaction.html#friedman-の-h統計量の理論"><i class="fa fa-check"></i><b>5.4.2</b> Friedman の H統計量の理論</a></li>
<li class="chapter" data-level="5.4.3" data-path="interaction.html"><a href="interaction.html#例-7"><i class="fa fa-check"></i><b>5.4.3</b> 例</a></li>
<li class="chapter" data-level="5.4.4" data-path="interaction.html"><a href="interaction.html#利点-1"><i class="fa fa-check"></i><b>5.4.4</b> 利点</a></li>
<li class="chapter" data-level="5.4.5" data-path="interaction.html"><a href="interaction.html#欠点-1"><i class="fa fa-check"></i><b>5.4.5</b> 欠点</a></li>
<li class="chapter" data-level="5.4.6" data-path="interaction.html"><a href="interaction.html#実装"><i class="fa fa-check"></i><b>5.4.6</b> 実装</a></li>
<li class="chapter" data-level="5.4.7" data-path="interaction.html"><a href="interaction.html#代替手法"><i class="fa fa-check"></i><b>5.4.7</b> 代替手法</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="feature-importance.html"><a href="feature-importance.html"><i class="fa fa-check"></i><b>5.5</b> Permutation Feature Importance</a><ul>
<li class="chapter" data-level="5.5.1" data-path="feature-importance.html"><a href="feature-importance.html#理論-3"><i class="fa fa-check"></i><b>5.5.1</b> 理論</a></li>
<li class="chapter" data-level="5.5.2" data-path="feature-importance.html"><a href="feature-importance.html#feature-importance-data"><i class="fa fa-check"></i><b>5.5.2</b> 特徴量の重要度は、学習データとテストデータのどちらで計算するべきか</a></li>
<li class="chapter" data-level="5.5.3" data-path="feature-importance.html"><a href="feature-importance.html#例と解釈"><i class="fa fa-check"></i><b>5.5.3</b> 例と解釈</a></li>
<li class="chapter" data-level="5.5.4" data-path="feature-importance.html"><a href="feature-importance.html#利点-2"><i class="fa fa-check"></i><b>5.5.4</b> 利点</a></li>
<li class="chapter" data-level="5.5.5" data-path="feature-importance.html"><a href="feature-importance.html#欠点-2"><i class="fa fa-check"></i><b>5.5.5</b> 欠点</a></li>
<li class="chapter" data-level="5.5.6" data-path="feature-importance.html"><a href="feature-importance.html#ソフトウェアと代替手法-4"><i class="fa fa-check"></i><b>5.5.6</b> ソフトウェアと代替手法</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="global.html"><a href="global.html"><i class="fa fa-check"></i><b>5.6</b> グローバルサロゲート (Global Surrogate)</a><ul>
<li class="chapter" data-level="5.6.1" data-path="global.html"><a href="global.html#理論-4"><i class="fa fa-check"></i><b>5.6.1</b> 理論</a></li>
<li class="chapter" data-level="5.6.2" data-path="global.html"><a href="global.html#例-8"><i class="fa fa-check"></i><b>5.6.2</b> 例</a></li>
<li class="chapter" data-level="5.6.3" data-path="global.html"><a href="global.html#長所-7"><i class="fa fa-check"></i><b>5.6.3</b> 長所</a></li>
<li class="chapter" data-level="5.6.4" data-path="global.html"><a href="global.html#短所-7"><i class="fa fa-check"></i><b>5.6.4</b> 短所</a></li>
<li class="chapter" data-level="5.6.5" data-path="global.html"><a href="global.html#ソフトウェア-3"><i class="fa fa-check"></i><b>5.6.5</b> ソフトウェア</a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="lime.html"><a href="lime.html"><i class="fa fa-check"></i><b>5.7</b> Local Surrogate (LIME)</a><ul>
<li class="chapter" data-level="5.7.1" data-path="lime.html"><a href="lime.html#表形式データにおける-lime"><i class="fa fa-check"></i><b>5.7.1</b> 表形式データにおける LIME</a></li>
<li class="chapter" data-level="5.7.2" data-path="lime.html"><a href="lime.html#テキストデータに対するlime"><i class="fa fa-check"></i><b>5.7.2</b> テキストデータに対するLIME</a></li>
<li class="chapter" data-level="5.7.3" data-path="lime.html"><a href="lime.html#images-lime"><i class="fa fa-check"></i><b>5.7.3</b> 画像データに対するLIME</a></li>
<li class="chapter" data-level="5.7.4" data-path="lime.html"><a href="lime.html#長所-8"><i class="fa fa-check"></i><b>5.7.4</b> 長所</a></li>
<li class="chapter" data-level="5.7.5" data-path="lime.html"><a href="lime.html#短所-8"><i class="fa fa-check"></i><b>5.7.5</b> 短所</a></li>
</ul></li>
<li class="chapter" data-level="5.8" data-path="anchors.html"><a href="anchors.html"><i class="fa fa-check"></i><b>5.8</b> Scoped Rules (Anchors)</a><ul>
<li class="chapter" data-level="5.8.1" data-path="anchors.html"><a href="anchors.html#anchor-の発見"><i class="fa fa-check"></i><b>5.8.1</b> Anchor の発見</a></li>
<li class="chapter" data-level="5.8.2" data-path="anchors.html"><a href="anchors.html#複雑性と実行時間"><i class="fa fa-check"></i><b>5.8.2</b> 複雑性と実行時間</a></li>
<li class="chapter" data-level="5.8.3" data-path="anchors.html"><a href="anchors.html#表形式データの例"><i class="fa fa-check"></i><b>5.8.3</b> 表形式データの例</a></li>
<li class="chapter" data-level="5.8.4" data-path="anchors.html"><a href="anchors.html#長所-9"><i class="fa fa-check"></i><b>5.8.4</b> 長所</a></li>
<li class="chapter" data-level="5.8.5" data-path="anchors.html"><a href="anchors.html#短所-9"><i class="fa fa-check"></i><b>5.8.5</b> 短所</a></li>
<li class="chapter" data-level="5.8.6" data-path="anchors.html"><a href="anchors.html#ソフトウェアと代替手法-5"><i class="fa fa-check"></i><b>5.8.6</b> ソフトウェアと代替手法</a></li>
</ul></li>
<li class="chapter" data-level="5.9" data-path="shapley.html"><a href="shapley.html"><i class="fa fa-check"></i><b>5.9</b> シャープレイ値 (Shapley Values)</a><ul>
<li class="chapter" data-level="5.9.1" data-path="shapley.html"><a href="shapley.html#一般的なアイデア"><i class="fa fa-check"></i><b>5.9.1</b> 一般的なアイデア</a></li>
<li class="chapter" data-level="5.9.2" data-path="shapley.html"><a href="shapley.html#例と解釈-1"><i class="fa fa-check"></i><b>5.9.2</b> 例と解釈</a></li>
<li class="chapter" data-level="5.9.3" data-path="shapley.html"><a href="shapley.html#シャープレイ値の詳細"><i class="fa fa-check"></i><b>5.9.3</b> シャープレイ値の詳細</a></li>
<li class="chapter" data-level="5.9.4" data-path="shapley.html"><a href="shapley.html#長所-10"><i class="fa fa-check"></i><b>5.9.4</b> 長所</a></li>
<li class="chapter" data-level="5.9.5" data-path="shapley.html"><a href="shapley.html#短所-10"><i class="fa fa-check"></i><b>5.9.5</b> 短所</a></li>
<li class="chapter" data-level="5.9.6" data-path="shapley.html"><a href="shapley.html#ソフトウェアと代替手法-6"><i class="fa fa-check"></i><b>5.9.6</b> ソフトウェアと代替手法</a></li>
</ul></li>
<li class="chapter" data-level="5.10" data-path="shap.html"><a href="shap.html"><i class="fa fa-check"></i><b>5.10</b> SHAP (SHapley Additive exPlanations)</a><ul>
<li class="chapter" data-level="5.10.1" data-path="shap.html"><a href="shap.html#定義"><i class="fa fa-check"></i><b>5.10.1</b> 定義</a></li>
<li class="chapter" data-level="5.10.2" data-path="shap.html"><a href="shap.html#kernelshap"><i class="fa fa-check"></i><b>5.10.2</b> KernelSHAP</a></li>
<li class="chapter" data-level="5.10.3" data-path="shap.html"><a href="shap.html#treeshap"><i class="fa fa-check"></i><b>5.10.3</b> TreeSHAP</a></li>
<li class="chapter" data-level="5.10.4" data-path="shap.html"><a href="shap.html#例-12"><i class="fa fa-check"></i><b>5.10.4</b> 例</a></li>
<li class="chapter" data-level="5.10.5" data-path="shap.html"><a href="shap.html#shap-特徴量重要度-shap-feature-importance"><i class="fa fa-check"></i><b>5.10.5</b> SHAP 特徴量重要度 (SHAP Feature Importance)</a></li>
<li class="chapter" data-level="5.10.6" data-path="shap.html"><a href="shap.html#shap-summary-plot"><i class="fa fa-check"></i><b>5.10.6</b> SHAP Summary Plot</a></li>
<li class="chapter" data-level="5.10.7" data-path="shap.html"><a href="shap.html#shap-dependence-plot"><i class="fa fa-check"></i><b>5.10.7</b> SHAP Dependence Plot</a></li>
<li class="chapter" data-level="5.10.8" data-path="shap.html"><a href="shap.html#shap-相互作用値-shap-interaction-values"><i class="fa fa-check"></i><b>5.10.8</b> SHAP 相互作用値 (SHAP Interaction Values)</a></li>
<li class="chapter" data-level="5.10.9" data-path="shap.html"><a href="shap.html#clustering-shap-values"><i class="fa fa-check"></i><b>5.10.9</b> Clustering SHAP values</a></li>
<li class="chapter" data-level="5.10.10" data-path="shap.html"><a href="shap.html#長所-11"><i class="fa fa-check"></i><b>5.10.10</b> 長所</a></li>
<li class="chapter" data-level="5.10.11" data-path="shap.html"><a href="shap.html#短所-11"><i class="fa fa-check"></i><b>5.10.11</b> 短所</a></li>
<li class="chapter" data-level="5.10.12" data-path="shap.html"><a href="shap.html#ソフトウェア-4"><i class="fa fa-check"></i><b>5.10.12</b> ソフトウェア</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="example-based.html"><a href="example-based.html"><i class="fa fa-check"></i><b>6</b> 例示に基づいた説明手法</a><ul>
<li class="chapter" data-level="6.1" data-path="反事実的.html"><a href="反事実的.html"><i class="fa fa-check"></i><b>6.1</b> 反事実的説明 (Counterfactual Explanations)</a><ul>
<li class="chapter" data-level="6.1.1" data-path="反事実的.html"><a href="反事実的.html#反事実的説明の生成"><i class="fa fa-check"></i><b>6.1.1</b> 反事実的説明の生成</a></li>
<li class="chapter" data-level="6.1.2" data-path="反事実的.html"><a href="反事実的.html#例-13"><i class="fa fa-check"></i><b>6.1.2</b> 例</a></li>
<li class="chapter" data-level="6.1.3" data-path="反事実的.html"><a href="反事実的.html#長所-12"><i class="fa fa-check"></i><b>6.1.3</b> 長所</a></li>
<li class="chapter" data-level="6.1.4" data-path="反事実的.html"><a href="反事実的.html#短所-12"><i class="fa fa-check"></i><b>6.1.4</b> 短所</a></li>
<li class="chapter" data-level="6.1.5" data-path="反事実的.html"><a href="反事実的.html#ソフトウェアと代替手法-7"><i class="fa fa-check"></i><b>6.1.5</b> ソフトウェアと代替手法</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="adversarial.html"><a href="adversarial.html"><i class="fa fa-check"></i><b>6.2</b> 敵対的サンプル (Adversarial Examples)</a><ul>
<li class="chapter" data-level="6.2.1" data-path="adversarial.html"><a href="adversarial.html#手法及び例"><i class="fa fa-check"></i><b>6.2.1</b> 手法及び例</a></li>
<li class="chapter" data-level="6.2.2" data-path="adversarial.html"><a href="adversarial.html#サイバーセキュリティーの観点"><i class="fa fa-check"></i><b>6.2.2</b> サイバーセキュリティーの観点</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="proto.html"><a href="proto.html"><i class="fa fa-check"></i><b>6.3</b> prototype と criticism</a><ul>
<li class="chapter" data-level="6.3.1" data-path="proto.html"><a href="proto.html#理論-5"><i class="fa fa-check"></i><b>6.3.1</b> 理論</a></li>
<li class="chapter" data-level="6.3.2" data-path="proto.html"><a href="proto.html#例-14"><i class="fa fa-check"></i><b>6.3.2</b> 例</a></li>
<li class="chapter" data-level="6.3.3" data-path="proto.html"><a href="proto.html#長所-13"><i class="fa fa-check"></i><b>6.3.3</b> 長所</a></li>
<li class="chapter" data-level="6.3.4" data-path="proto.html"><a href="proto.html#短所-13"><i class="fa fa-check"></i><b>6.3.4</b> 短所</a></li>
<li class="chapter" data-level="6.3.5" data-path="proto.html"><a href="proto.html#コードと代替手法"><i class="fa fa-check"></i><b>6.3.5</b> コードと代替手法</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="influential.html"><a href="influential.html"><i class="fa fa-check"></i><b>6.4</b> Influential Instances</a><ul>
<li class="chapter" data-level="6.4.1" data-path="influential.html"><a href="influential.html#deletion-diagnostics"><i class="fa fa-check"></i><b>6.4.1</b> Deletion Diagnostics</a></li>
<li class="chapter" data-level="6.4.2" data-path="influential.html"><a href="influential.html#影響関数-influence-functions"><i class="fa fa-check"></i><b>6.4.2</b> 影響関数 (Influence Functions)</a></li>
<li class="chapter" data-level="6.4.3" data-path="influential.html"><a href="influential.html#長所-14"><i class="fa fa-check"></i><b>6.4.3</b> 長所</a></li>
<li class="chapter" data-level="6.4.4" data-path="influential.html"><a href="influential.html#短所-14"><i class="fa fa-check"></i><b>6.4.4</b> 短所</a></li>
<li class="chapter" data-level="6.4.5" data-path="influential.html"><a href="influential.html#ソフトウェアと代替手法-8"><i class="fa fa-check"></i><b>6.4.5</b> ソフトウェアと代替手法</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="neural-networks.html"><a href="neural-networks.html"><i class="fa fa-check"></i><b>7</b> Neural Network Interpretation</a><ul>
<li class="chapter" data-level="7.1" data-path="cnn-features.html"><a href="cnn-features.html"><i class="fa fa-check"></i><b>7.1</b> 学習された特徴量</a><ul>
<li class="chapter" data-level="7.1.1" data-path="cnn-features.html"><a href="cnn-features.html#特徴量の可視化"><i class="fa fa-check"></i><b>7.1.1</b> 特徴量の可視化</a></li>
<li class="chapter" data-level="7.1.2" data-path="cnn-features.html"><a href="cnn-features.html#ネットワークの解剖"><i class="fa fa-check"></i><b>7.1.2</b> ネットワークの解剖</a></li>
<li class="chapter" data-level="7.1.3" data-path="cnn-features.html"><a href="cnn-features.html#利点-3"><i class="fa fa-check"></i><b>7.1.3</b> 利点</a></li>
<li class="chapter" data-level="7.1.4" data-path="cnn-features.html"><a href="cnn-features.html#欠点-3"><i class="fa fa-check"></i><b>7.1.4</b> 欠点</a></li>
<li class="chapter" data-level="7.1.5" data-path="cnn-features.html"><a href="cnn-features.html#ソフトウェアとその他の資料"><i class="fa fa-check"></i><b>7.1.5</b> ソフトウェアとその他の資料</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="future.html"><a href="future.html"><i class="fa fa-check"></i><b>8</b> 解釈可能な機械学習の未来</a><ul>
<li class="chapter" data-level="8.1" data-path="機械学習の未来.html"><a href="機械学習の未来.html"><i class="fa fa-check"></i><b>8.1</b> 機械学習の未来</a></li>
<li class="chapter" data-level="8.2" data-path="解釈性の未来.html"><a href="解釈性の未来.html"><i class="fa fa-check"></i><b>8.2</b> 解釈性の未来</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="contribute.html"><a href="contribute.html"><i class="fa fa-check"></i><b>9</b> 著者貢献</a></li>
<li class="chapter" data-level="10" data-path="cite.html"><a href="cite.html"><i class="fa fa-check"></i><b>10</b> この本の引用</a></li>
<li class="chapter" data-level="11" data-path="translations.html"><a href="translations.html"><i class="fa fa-check"></i><b>11</b> 翻訳</a></li>
<li class="chapter" data-level="12" data-path="謝辞.html"><a href="謝辞.html"><i class="fa fa-check"></i><b>12</b> 謝辞</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a><ul>
<li class="chapter" data-level="" data-path="r-packages-used-for-examples.html"><a href="r-packages-used-for-examples.html"><i class="fa fa-check"></i>R Packages Used for Examples</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Interpretable Machine Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="ale" class="section level2">
<h2><span class="header-section-number">5.3</span> Accumulated Local Effects (ALE) Plot</h2>
<!--
Accumulated local effects[^ALE] describe how features influence the prediction of a machine learning model on average.
ALE plots are a faster and unbiased alternative to partial dependence plots (PDPs).
-->
<p>Accumulated local effects<a href="#fn30" class="footnoteRef" id="fnref30"><sup>30</sup></a> は、特徴量が機械学習モデルの予測に対して、平均的にどの程度影響を与えているか示します。 ALE plot は、partial dependence plot (PDP) と比べて高速で偏りがありません。</p>
<!-- *Keywords: ALE plots, partial dependence plots, marginal means, predictive margins, marginal effects* -->
<!--
I recommend reading the [chapter on partial dependence plots](#pdp) first, as they are easier to understand and both methods share the same goal:
Both describe how a feature affects the prediction on average.
In the following section, I want to convince you that partial dependence plots have a serious problem when the features are correlated.
-->
<p>PDP と ALE は目的が同じ手法なので、理解のしやすさのために、<a href="pdp.html#pdp">partial dependence plots の章</a> を先に読むことをおすすめします。 どちらも、ある特徴量が予測に対して平均的にどの程度の影響を与えるかを説明します。 次のセクションでは、特徴量が相関しているときに、partial dependence plot では深刻な問題があることを紹介します。</p>
<!-- ### Motivation and Intuition -->
<div id="モチベーションと直感" class="section level3">
<h3><span class="header-section-number">5.3.1</span> モチベーションと直感</h3>
<!--
If features of a machine learning model are correlated, the partial dependence plot cannot be trusted.
The computation of a partial dependence plot for a feature that is strongly correlated with other features involves averaging predictions of artificial data instances that are unlikely in reality.
This can greatly bias the estimated feature effect.
-->
<p>もし機械学習モデルの特徴量が相関しているとき、partial dependence plot は信用できません。 他の特徴量と強く相関する特徴量に対する partial dependence plot の計算では、現実的に起こり得ない人工的なインスタンスの予測結果が含まれます。 これが、特徴量の効果を推定するときの大きなバイアスになります。</p>
<!--
Imagine calculating partial dependence plots for a machine learning model that predicts the value of a house depending on the number of rooms and the size of the living area.
We are interested in the effect of the living area on the predicted value.
-->
<p>部屋の数とリビングのサイズによって家の値段を予測する機械学習モデルで partial dependence plot を計算することを考えてみてください。 今、リビングが、予測した値段に与える影響に関心があるとします。</p>
<!--
As a reminder, the recipe for partial dependence plots is: 1) Select feature. 2) Define grid. 3) Per grid value: a) Replace feature with grid value and b) average predictions. 4) Draw curve.
For the calculation of the first grid value of the PDP -- say 30 m^2^ -- we replace the living area for **all** instances by 30 m^2^, even for houses with 10 rooms.
Sounds to me like a very unusual house.
The partial dependence plot includes these unrealistic houses in the feature effect estimation and pretends that everything is fine.
The following figure illustrates two correlated features and how it comes that the partial dependence plot method averages predictions of unlikely instances.
-->
<p>Partial dependence plot の手順は 1) 特徴量を選ぶ 2) グリッドを決める 3) それぞれのグリッドの値に対して、 a) 特徴量をその値に変換し b) 予測を平均化する 4) 曲線を描く でした。 PDP の始めのグリッドの値(例えば 30m^2 )を計算するために、たとえ10部屋ある家でも「全ての」インスタンスに対して、30m^2 と置き換えます。 これは不自然な家です。 Partial dependence plot では、このような不自然な家があったとしても、それは問題がないとして特徴量の効果を推定します。 次の図は、2つの相関した特徴量と、partial dependence plot で現実的でないインスタンスを予測した平均の結果を示しています。</p>
<!--
"Strongly correlated features x1 and x2. To calculate the feature effect of x1 at 0.75, the PDP replaces x1 of all instances with 0.75, falsely assuming that the distribution of x2 at x1 = 0.75 is the same as the marginal distribution of x2 (vertical line). This results in unlikely combinations of x1 and x2 (e.g. x2=0.2 at x1=0.75), which the PDP uses for the calculation of the average effect."
-->
<div class="figure"><span id="fig:aleplot-motivation1"></span>
<img src="images/aleplot-motivation1-1.png" alt="強く相関している特徴量 x1 と x2。 x1 が 0.75 の特徴量の影響を計算するために、PDP は全てのインスタンスの x1 を 0.75 に置き換え、x1=0.75 のときの x2 の分布が、全体の x2(縦軸) の分布と同じであると誤った仮定をしている。 これによって不自然な x1 と x2 の組み合わせ(例えば x2=0.2 で x1=0.75)が生まれ、PDP ではこれも平均効果の計算に含まれる。" width="1050" />
<p class="caption">
FIGURE 5.10: 強く相関している特徴量 x1 と x2。 x1 が 0.75 の特徴量の影響を計算するために、PDP は全てのインスタンスの x1 を 0.75 に置き換え、x1=0.75 のときの x2 の分布が、全体の x2(縦軸) の分布と同じであると誤った仮定をしている。 これによって不自然な x1 と x2 の組み合わせ(例えば x2=0.2 で x1=0.75)が生まれ、PDP ではこれも平均効果の計算に含まれる。
</p>
</div>
<!--
What can we do to get a feature effect estimate that respects the correlation of the features?
We could average over the conditional distribution of the feature, meaning at a grid value of x1, we average the predictions of instances with a similar x1 value.
The solution for calculating feature effects using the conditional distribution is called Marginal Plots, or M-Plots (confusing name, since they are based on the conditional, not the marginal distribution).
-->
<p>特徴量同士の相関を保ったまま特徴量の影響の推定を得るためにはどうしたら良いでしょうか。 特徴量の条件付き分布上で周辺化します。つまり、x1 のグリッドの値では、x1 と似たような値を持つインスタンスの予測のみを用いて平均化します。 条件付き分布を使用して、特徴量の影響を計算する方法を Marginal Plot や M-Plot と呼びます(marginal distribution ではなく conditional distribution に基づいているのでややこしい名前です)。</p>
<!--
Wait, did I not promise you to talk about ALE plots?
M-Plots are not the solution we are looking for.
Why do M-Plots not solve our problem?
-->
<p>待ってください、ALE plot について説明すると言っていたのではありませんでしたか。 実は、M-Plot は我々が求めている解決策ではありません。 なぜ M-Plot では問題を解決できないのでしょうか。</p>
<!--
If we average the predictions of all houses of about 30 m^2^, we estimate the **combined** effect of living area and of number of rooms, because of their correlation.
Suppose that the living area has no effect on the predicted value of a house, only the number of rooms has.
The M-Plot would still show that the size of the living area increases the predicted value, since the number of rooms increases with the living area.
The following plot shows for two correlated features how M-Plots work.
-->
<p>約 30 m<sup>2</sup> の全ての家の予測を平均化してしまうと、相関が原因でリビングや部屋の数の「混合された」影響を推定してしてしまいます。 リビングが予測された家の価値に全く影響を与えないと仮定すると、部屋の数だけが価値に影響を与えます。 M-Plot はリビングのサイズが予測される価値を上げると示すでしょう、なぜなら部屋の数はリビングのサイズにしたがって増えていくからです。 次の図は2つの相関した特徴量に対して M-Plot がどう動くかを示しています。</p>
<!--
fig.cap = "Strongly correlated features x1 and x2. M-Plots average over the conditional distribution. Here the conditional distribution of x2 at x1 = 0.75. Averaging the local predictions leads to mixing the effects of both features."
-->
<div class="figure"><span id="fig:aleplot-motivation2"></span>
<img src="images/aleplot-motivation2-1.png" alt="強く相関した特徴量 x1 と x2。 M-Plot は条件付き分布上で平均化する。ここで x1=0.75 のときの x2 の条件付き分布が示されている。局所的な予測を平均化することは両方の特徴量の影響を混ぜることにつながる。" width="1050" />
<p class="caption">
FIGURE 5.11: 強く相関した特徴量 x1 と x2。 M-Plot は条件付き分布上で平均化する。ここで x1=0.75 のときの x2 の条件付き分布が示されている。局所的な予測を平均化することは両方の特徴量の影響を混ぜることにつながる。
</p>
</div>
<!--
M-Plots avoid averaging predictions of unlikely data instances, but they mix the effect of a feature with the effects of all correlated features.
ALE plots solve this problem by calculating -- also based on the conditional distribution of the features --  **differences in predictions instead of averages**.
For the effect of living area at 30 m^2^, the ALE method uses all houses with about 30 m^2^, gets the model predictions pretending these houses were 31 m^2^ minus the prediction pretending they were 29 m^2^.
This gives us the pure effect of the living area and is not mixing the effect with the effects of correlated features.
The use of differences blocks the effect of other features.
The following graphic provides intuition how ALE plots are calculated.
-->
<p>M-Plot は現実的ではないインスタンスの予測を含めることを防ぎますが、ある特徴量の影響を他の相関する全ての特徴量の影響と混ぜてしまいます。 ALE plot では、 -- これも特徴量の条件付き分布に基づいている -- <strong>予測の平均ではなく差分</strong>を計算することによってこの問題を解決しています。 30m<sup>2</sup> のリビングの影響に対して、ALE はリビングが約 30m<sup>2</sup> の家全てを使い、それらの家が31m<sup>2</sup> だったとしたときの予測値からそれらの家が29m<sup>2</sup> だったとしたときの予測値をひいたモデルの予測値を得ます。 これによって、リビングの広さの純粋な影響が得られ、かつ、他の相関した特徴量の影響とも混ざっていません。 差分を使うことにより他の特徴量の影響を受けないようにできます。 次の図から ALE plot がどのように計算されるか直感的に理解できるでしょう。</p>
<!--
fig.cap = "Calculation of ALE for feature x1, which is correlated with x2. First, we divide the feature into intervals (vertical lines). For the data instances (points) in an interval, we calculate the difference in the prediction when we replace the feature with the upper and lower limit of the interval (horizontal lines). These differences are later accumulated and centered, resulting in the ALE curve."
-->
<div class="figure"><span id="fig:aleplot-computation"></span>
<img src="images/aleplot-computation-1.png" alt="x2 と相関している特徴量 x1 の ALE の計算。はじめに x1 を区間(縦の線)に分ける。ある区間内のデータインスタンス(点)に対して、その特徴量をその区間の上限と下限の値に置き換えたときとの予測値の差分を計算する(横の線)。この差分は後に蓄積され、中心化して ALE 曲線となる。" width="1050" />
<p class="caption">
FIGURE 5.12: x2 と相関している特徴量 x1 の ALE の計算。はじめに x1 を区間(縦の線)に分ける。ある区間内のデータインスタンス(点)に対して、その特徴量をその区間の上限と下限の値に置き換えたときとの予測値の差分を計算する(横の線)。この差分は後に蓄積され、中心化して ALE 曲線となる。
</p>
</div>
<!--
To summarize how each type of plot (PDP, M, ALE) calculates the effect of a feature at a certain grid value v:
**Partial Dependence Plots**: "Let me show you what the model predicts on average when each data instance has the value v for that feature.
I ignore whether the value v makes sense for all data instances."
**M-Plots**: "Let me show you what the model predicts on average for data instances that have values close to v for that feature.
The effect could be due to that feature, but also due to correlated features."
**ALE plots**: "Let me show you how the model predictions change in a small "window" of the feature around v for data instances in that window."
-->
<p>それぞれの プロット (PDP, M, ALE) が、ある特定のグリッドの値 v での、特徴量の影響をどのように計算するかを以下にまとめます。</p>
<p><strong>Partial Dependence Plots</strong>: 「それぞれのインスタンスの特徴量の値を v としたときの、モデルの予測の平均を示します。ただし、特徴量の値を v にしたときのインスタンスが現実的であるかどうかは無視します。」</p>
<p><strong>M-Plots</strong>: 「その特徴量が v に近い値を持つインスタンスに対する、モデルの予測の平均を示します。影響はその特徴量に起因しますが、相関する特徴量にも起因してしまいます。」</p>
<p><strong>ALE plots</strong>: 「その特徴量が約 v であるインスタンスに対して、小さな&quot;窓&quot;の中でモデルの予測がどう変化するかを示します。」</p>
<!--
### Theory
-->
</div>
<div id="理論-2" class="section level3">
<h3><span class="header-section-number">5.3.2</span> 理論</h3>
<!--
How do PD, M and ALE plots differ mathematically?
Common to all three methods is that they reduce the complex prediction function f to a function that depends on only one (or two) features.
All three methods reduce the function by averaging the effects of the other features, but they differ in whether averages of predictions or of **differences in predictions** are calculated and whether averaging is done over the marginal or conditional distribution.

Partial dependence plots average the predictions over the marginal distribution.
-->
<p>数学的に PDP, M-plot, ALE plot はどのように異なっているのでしょうか。 これら全てに共通していることは、複雑な予測関数 f を、1つか2つの特徴量に依存する関数に削減します。 3つの方法は全て、他の特徴量の効果を均すことによって関数を削減しますが、予測の平均、または<strong>予測の差</strong>を計算するかどうか、および平均を周辺分布、もしくは条件付き分布に対して行うかが異なります。</p>
<p>PDP は、周辺分布に対して予測を平均化します。</p>
<p><span class="math display">\[\begin{align*}\hat{f}_{x_S,PDP}(x_S)&amp;=E_{X_C}\left[\hat{f}(x_S,X_C)\right]\\&amp;=\int_{x_C}\hat{f}(x_S,x_C)\mathbb{P}(x_C)d{}x_C\end{align*}\]</span></p>
<!--
This is the value of the prediction function f, at feature value(s) $x_S$, averaged over all features in $x_C$.
Averaging means calculating the marginal expectation E over the features in set C, which is the integral over the predictions weighted by the probability distribution.
Sounds fancy, but to calculate the expected value over the marginal distribution, we simply take all our data instances, force them to have a certain grid value for the features in set S, and average the predictions for this manipulated dataset.
This procedure ensures that we average over the marginal distribution of the features.
-->
<p>これは、<span class="math inline">\(x_C\)</span> の全ての特徴量で平均化された、特徴量 <span class="math inline">\(x_S\)</span> での予測関数 f の値です。 平均化は、集合 C の特徴量に対する周辺化期待値 (marginal expectation) E を計算することを意味しており、これは、確率分布によって重み付けされた予測の積分です。 難しく聞こえますよね、しかし、周辺分布の期待値を計算するためには、単純に、全てのインスタンスを取り出して、これらの、集合 S 内の特徴量を特定のグリッドの値に書き換えます。そして、人工的に作ったデータセットの予測の平均を求めるだけです。 この手順が、特徴量の周辺分布に対して平均化した事を保証しています。</p>
<!--
M-plots average the predictions over the conditional distribution.
-->
<p>M-plots は条件付き分布に対して予測を平均化します。</p>
<p><span class="math display">\[\begin{align*}\hat{f}_{x_S,M}(x_S)&amp;=E_{X_C|X_S}\left[\hat{f}(X_S,X_C)|X_S=x_s\right]\\&amp;=\int_{x_C}\hat{f}(x_S,x_C)\mathbb{P}(x_C|x_S)d{}x_C\end{align*}\]</span></p>
<!--
The only thing that changes compared to PDPs is that we average the predictions conditional on each grid value of the feature of interest, instead of assuming the marginal distribution at each grid value.
In practice, this means that we have to define a neighborhood, for example for the calculation of the effect of 30 m^2^ on the predicted house value, we could average the predictions of all houses between 28 and 32 m^2^.
-->
<p>PDP と比較した時の唯一の違いは、興味のある特徴量の各グリッドの値で、周辺分布を仮定するのではなく、条件付き分布に対する予測の平均を計算することです。 実際には、これは近傍を定義しなけらばらない事を意味します。 例えば、 予測された家の価値に 30 m<sup>2</sup> が与えた影響を計算したい場合、全ての 28 m<sup>2</sup> から 32 m<sup>2</sup> の間にある家の予測を平均化します。</p>
<!--
ALE plots average the changes in the predictions and accumulate them over the grid (more on the calculation later).
-->
<p>ALE plot は、予測の変化を平均し、グリッド上で累積します（計算の詳細は後述）。</p>
<p><span class="math display">\[\begin{align*}\hat{f}_{x_S,ALE}(x_S)=&amp;\int_{z_{0,1}}^{x_S}E_{X_C|X_S}\left[\hat{f}^S(X_s,X_c)|X_S=z_S\right]dz_S-\text{constant}\\=&amp;\int_{z_{0,1}}^{x_S}\int_{x_C}\hat{f}^S(z_s,x_c)\mathbb{P}(x_C|z_S)d{}x_C{}dz_S-\text{constant}\end{align*}\]</span></p>
<!--
The formula reveals three differences to M-Plots.
First, we average the changes of predictions, not the predictions itself.
The change is defined as the gradient (but later, for the actual computation, replaced by the differences in the predictions over an interval).
-->
<p>この式は M-Plots との3つの違いを明らかにします。 1つ目は、予測値そのものではなく、予測値の変化を平均化することです。 変化は、勾配として定義されます（しかし、後ほど、実際の計算では、区間上の予測の差と置き換えられます)。</p>
<p><span class="math display">\[\hat{f}^S(x_s,x_c)=\frac{\delta\hat{f}(x_S,x_C)}{\delta{}x_S}\]</span></p>
<!--
The second difference is the additional integral over z.
We accumulate the local gradients over the range of features in set S, which gives us the effect of the feature on the prediction.
For the actual computation, the z's are replaced by a grid of intervals over which we compute the changes in the prediction.
Instead of directly averaging the predictions, the ALE method calculates the prediction differences conditional on features S and integrates the derivative over features S to estimate the effect.
Well, that sounds stupid.
Derivation and integration usually cancel each other out, like first subtracting, then adding the same number.
Why does it make sense here?
The derivative (or interval difference) isolates the effect of the feature of interest and blocks the effect of correlated features.
-->
<p>2つ目の違いは、z に対するもう1つの積分です。 集合 S の特徴量の範囲にわたって局所勾配を累積することで、予測に対する特徴量の影響を知ることができます。 実際の計算では、 z は予測の変化を計算するために、区間のグリッドに置き換えられます。 直接、予測を平均化する代わりに、ALE では、特徴量 S で条件づけされた予測の差を計算し、効果を推定するために特徴量 S 上の勾配を積分します。 これは、馬鹿げた様に聞こえます。 微分と積分は、通常、互いに打ち消し合います。例えば、最初に引き算をして、同じ数字を足し算する様な物です。 なぜ、これが、ここでは意味をなすのでしょうか。 微分（または、区間の差）は対象の特徴量の効果を分離し、そして、相関した特徴量の影響を遮断するのです。</p>
<!--
The third difference of ALE plots to M-plots is that we subtract a constant from the results.
This step centers the ALE plot so that the average effect over the data is zero.

One problem remains:
Not all models come with a gradient, for example random forests have no gradient.
But as you will see, the actual computation works without gradients and uses intervals.
Let us delve a little deeper into the estimation of ALE plots.
-->
<p>ALE plots と M-plotsの3つ目の違いは、結果から定数を引く事です。 このステップによって、データに対しての平均的な影響がゼロになるよう ALE plot が中心化されます。</p>
<p>問題が1つ残ります。 全てのモデルで勾配が使用できる訳ではありません。例えば、ランダムフォレストは勾配が計算できません。 しかし、実際の計算では、勾配を使うのではなく、区間を使用します。 もう少し、ALE plots の推定を深く見てみましょう。</p>
<!--
### Estimation
-->
</div>
<div id="予測" class="section level3">
<h3><span class="header-section-number">5.3.3</span> 予測</h3>
<!--
First I will describe how ALE plots are estimated for a single numerical feature, later for two numerical features and for a single categorical feature.
To estimate local effects, we divide the feature into many intervals and compute the differences in the predictions.
This procedure approximates the gradients and also works for models without gradients.

First we estimate the uncentered effect:

$$\hat{\tilde{f}}_{j,ALE}(x)=\sum_{k=1}^{k_j(x)}\frac{1}{n_j(k)}\sum_{i:x_{j}^{(i)}\in{}N_j(k)}\left[f(z_{k,j},x^{(i)}_{\setminus{}j})-f(z_{k-1,j},x^{(i)}_{\setminus{}j})\right]$$
-->
<p>最初に、どの様に ALE plots が1つの量的特徴量に対して予測されるのかを説明します。次に、2つの量的特徴量に対して、そして、最後にカテゴリカル特徴量について説明します。 局所的な効果を予測するために、特徴量をいくつかの区間に分割し、そして、予測の差を計算します。</p>
<p><span class="math display">\[\hat{\tilde{f}}_{j,ALE}(x)=\sum_{k=1}^{k_j(x)}\frac{1}{n_j(k)}\sum_{i:x_{j}^{(i)}\in{}N_j(k)}\left[f(z_{k,j},x^{(i)}_{\setminus{}j})-f(z_{k-1,j},x^{(i)}_{\setminus{}j})\right]\]</span></p>
<!--
Let us break this formula down, starting from the right side.
The name **Accumulated Local Effects** nicely reflects all the individual components of this formula.
At its core, the ALE method calculates the differences in predictions, whereby we replace the feature of interest with grid values z.
The difference in prediction is the **Effect** the feature has for an individual instance in a certain interval.
The sum on the right adds up the effects of all instances within an interval which appears in the formula as neighborhood $N_j(k)$.
We divide this sum by the number of instances in this interval to obtain the average difference of the predictions for this interval.
This average in the interval is covered by the term **Local** in the name ALE.
The left sum symbol means that we accumulate the average effects across all intervals.
The (uncentered) ALE of a feature value that lies, for example, in the third interval is the sum of the effects of the first, second and third intervals.
The word **Accumulated** in ALE reflects this.


This effect is centered so that the mean effect is zero.
-->
<p>この方程式を右端から読み解いてみましょう。 Accumulated local effect という名前が、この方程式の個々の要素を、とてもよく表しています。 基本的に ALE method は予測の差を計算します。それにより、対象の特徴量をグリッド値 z と置き換えます。 予測の差は、ある区間の中の個々のインスタンスに対して、特徴量が持つ<strong>効果</strong>となります。 右側の合計は、区間内の全てのインスタンスの効果を足し合わせたものであり、式の中では、近傍 <span class="math inline">\(N_j(k)\)</span> として書かれています。 この区間内の予測の差の平均値を計算するために、合計値を区間内のインスタンス数で割ります。 区間内の平均値は、ALE という名前の <strong>Local</strong> という用語で表現されます。 左の総和の記号は、全ての区間において、平均効果を累積する事を意味します。 例えば、（中心化されていない）3番目のALEの特徴量は、1番目、2番目そして3番目の区間の効果の合計です。 ALE の <strong>Accumulated</strong> という用語はこれを表しています。</p>
<p>この効果は、平均がゼロとなるように中心化されます。</p>
<p><span class="math display">\[\hat{f}_{j,ALE}(x)=\hat{\tilde{f}}_{j,ALE}(x)-\frac{1}{n}\sum_{i=1}^{n}\hat{\tilde{f}}_{j,ALE}(x^{(i)}_{j})\]</span></p>
<!--
The value of the ALE can be interpreted as the main effect of the feature at a certain  value compared to the average prediction of the data.
For example, an ALE estimate of -2 at $x_j=3$ means that when the j-th feature has value 3, then the prediction is lower by 2 compared to the average prediction.

The quantiles of the distribution of the feature are used as the grid that defines the intervals.
Using the quantiles ensures that there is the same number of data instances in each of the intervals.
Quantiles have the disadvantage that the intervals can have very different lengths.
This can lead to some weird ALE plots if the feature of interest is very skewed, for example many low values and only a few very high values.
-->
<p>ALEの値は、データの特定の値における予測の平均と比較したときの、特徴量の主な影響として解釈する事が出来ます。 例えば、<span class="math inline">\(x_j=3\)</span> において、ALEの推定が -2 であるとは、j番目の特徴量が 3 のとき、予測は予測の平均よりも 2 下がるということを意味します。</p>
<p>特徴量の分布の分位数は、区間を決めるグリッドとして使用されます。 分位数を使用することで、各区間内に同じ数のインスタンスが存在することを保証できます。 ただし、分位数は、区間の長さがばらつく可能性があるという欠点もあります。 例えば、低い値が多く、高い値が少数のみであるような、対象の特徴量が特に偏っている場合、不自然な ALE plot になる可能性があります。</p>
<p><strong>2つの特徴量の相互作用に対するALEプロット</strong> <!--
**ALE plots for the interaction of two features**
--></p>
<!--
ALE plots can also show the interaction effect of two features.
The calculation principles are the same as for a single feature, but we work with rectangular cells instead of intervals, because we have to accumulate the effects in two dimensions.
In addition to adjusting for the overall mean effect, we also adjust for the main effects of both features.
This means that ALE for two features estimate the second-order effect, which does not include the main effects of the features.
In other words, ALE for two features only shows the additional interaction effect of the two features.
I spare you the formulas for 2D ALE plots because they are long and unpleasant to read.
If you are interested in the calculation, I refer you to the paper, formulas (13) -- (16).
I will rely on visualizations to develop intuition about the second-order ALE calculation.
-->
<p>ALEプロットは2つの特徴量の相互作用の影響も表すことができます。 計算方法は単一の特徴量に対するのものと同じですが、2次元における影響を考慮しなければならないため、インターバルの代わりに長方形セル(2次元の区間)を使用します。 また、全体の平均の効果を調整することに加えて、それぞれの特徴量の効果の調整も行います。 これは2つの特徴量に対するALEは、それぞれの特徴量の効果を含まない2次の効果を推定するということです。 言い換えると、2つの特徴量に対するALEは、2つの特徴量の追加の相互作用のみを表すということです。 2次元のALEプロットの式は長くて読みづらいため、掲載を控えます。 計算式に興味がある人は、原論文の式 (13) -- (16) を参照ください。 ここでは、2次のALEの計算は、直感を養うための視覚的な説明にとどめます。</p>
<!--
fig.cap = 'Calculation of 2D-ALE. We place a grid over the two features. In each grid cell we calculate the 2nd-order differences for all instance within. We first replace values of x1 and x2 with the values from the cell corners. If a, b, c and d represent the "corner"-predictions of a manipulated instance (as labeled in the graphic), then the 2nd-order difference is (d - c) - (b - a). The mean 2nd-order difference in each cell is accumulated over the grid and centered.'
-->
<div class="figure"><span id="fig:aleplot-computation-2d"></span>
<img src="images/aleplot-computation-2d-1.png" alt="2次元ALEの計算。2つの特徴量に渡ってグリッド作成。それぞれのセルにおいて、グリッド内の全てのインスタンスに対して2次の差分を計算する。初めに x1 と x2 の値をセルの角の値で置き換える。a, b, c および d が(図中でラベル付けされているように)人工的なインスタンスの&quot;角&quot;での予測を表すとき、2次の差分は (d - c) - (b - a)となる。各セルにおける2次の差分の平均は、グリッド上のそれぞれのセルで累積されて中心化される。" width="1050" />
<p class="caption">
FIGURE 5.13: 2次元ALEの計算。2つの特徴量に渡ってグリッド作成。それぞれのセルにおいて、グリッド内の全てのインスタンスに対して2次の差分を計算する。初めに x1 と x2 の値をセルの角の値で置き換える。a, b, c および d が(図中でラベル付けされているように)人工的なインスタンスの&quot;角&quot;での予測を表すとき、2次の差分は (d - c) - (b - a)となる。各セルにおける2次の差分の平均は、グリッド上のそれぞれのセルで累積されて中心化される。
</p>
</div>
<!--
In the previous figure, many cells are empty due to the correlation.
In the ALE plot this can be visualized with a grayed out or darkened box.
Alternatively, you can replace the missing ALE estimate of an empty cell with the ALE estimate of the nearest non-empty cell.
-->
<p>前の図では、多くのセルが相関によって空となっていました。 ALEプロットでは、これがグレーまたは暗い色のボックスで可視化されます。 その代わりに、空のセルのALEの推定結果を、最も近い空でないセルの ALE の推定結果で置き換えることもできます。</p>
<!--
Since the ALE estimates for two features only show the second-order effect of the features, the interpretation requires special attention.
The second-order effect is the additional interaction effect of the features after we have accounted for the main effects of the features.
Suppose two features do not interact, but each has a linear effect on the predicted outcome.
In the 1D ALE plot for each feature, we would see a straight line as the estimated ALE curve.
But when we plot the 2D ALE estimates, they should be close to zero, because the second-order effect is only the additional effect of the interaction.
ALE plots and PD plots differ in this regard:
PDPs always show the total effect, ALE plots show the first- or second-order effect.
These are design decisions that do not depend on the underlying math.
You can subtract the lower-order effects in a partial dependence plot to get the pure main or second-order effects or, you can get an estimate of the total ALE plots by refraining from subtracting the lower-order effects.
-->
<p>2つの特徴量に対するALEの推定では、2つの特徴量の2次の効果のみを示しているため、解釈には特に注意が必要です。 2次の効果は、特徴量の主な効果を考慮した後の、追加の相互作用による影響です。 2つの特徴量は相互作用しないが、それぞれが予測結果に対して線形の影響を持つと仮定してください。 それぞれの特徴量に対する1次元のALEプロットでは、推定されたALE曲線は直線となるでしょう。 しかし2次元のALEの推定結果をプロットすると、2次の効果は相互効果による追加の影響のみを表すため、それらはゼロに近くなるはずです。 このような場合、ALEプロットとPDPは異なります。 PDPはいつも総合的な影響を可視化しますが、ALEプロットは1次または2次の影響のみ見せます。 それらは根底にある数学には依らない、設計における決まり事です。 純粋な主効果や2次の影響はPDPから低次の効果を差し引くことで計算可能であり、総合的な効果はALEプロットで低次の影響を引くのをやめることで得ることができます。</p>
<!--
The accumulated local effects could also be calculated for arbitrarily higher orders (interactions of three or more features), but as argued in the [PDP chapter] only up to two features makes sense, because higher interactions cannot be visualized or even interpreted meaningfully.
-->
<p>ALEは、任意の高次の計算 (3つ以上の相互作用) も可能ですが、<a href="pdp.html#pdp">PDPの章</a> でも触れたとおり、計算結果自体は意味の解釈が可能であっても可視化ができないので、2つの特徴量までにするべきです。</p>
<p><strong>カテゴリカル特徴量に対するALE</strong> <!--
**ALE for categorical features**
--></p>
<!--
The accumulated local effects method needs -- by definition -- the feature values to have an order, because the method accumulates effects in a certain direction.
Categorical features do not have any natural order.
To compute an ALE plot for a categorical feature we have to somehow create or find an order.
The order of the categories influences the calculation and interpretation of the accumulated local effects.
-->
<p>ALEプロットは、-- 定義によると -- ある方向の中での影響を累積する手法であるため、特徴量の値の順序関係を要求します。 カテゴリカル特徴量は、自然な順序を持ちません。 カテゴリカル特徴量に対してALEプロットを計算するためには、何らかの方法で順序関係を作り出す、もしくは見出す必要があります。 カテゴリの順序は、ALEの解釈や計算方法に影響を与えます。</p>
<!--
One solution is to order the categories according to their similarity based on the other features.
The distance between two categories is the sum over the distances of each feature.
The feature-wise distance compares either the cumulative distribution in both categories, also called Kolmogorov-Smirnov distance (for numerical features) or the relative frequency tables (for categorical features).
Once we have the distances between all categories, we use multi-dimensional scaling to reduce the distance matrix to a one-dimensional distance measure.
This gives us a similarity-based order of the categories.
-->
<p>1つの解決法は、カテゴリを他の特徴量に対する類似度に従って順序づけることです。 2つのカテゴリ間の距離を、それぞれの特徴の距離の総和とします。 特徴量ごとの距離は、（量的特徴量における）Kolmogorov-Smirnov 距離 や (カテゴリカル特徴量における) 相対頻度表の両方のカテゴリの累積分布を比較します。 一度すべてのカテゴリ間の距離を計算してしまえば、multi-dimensional scaling を使って距離行列を1次元の距離尺度に削減できます。 これにより、カテゴリの類似度を基にした順序を得ることができます。</p>
<!--
To make this a little bit clearer, here is one example:
Let us assume we have the two categorical features "season" and "weather" and a numerical feature "temperature".
For the first categorical feature (season) we want to calculate the ALEs.
The feature has the categories "spring", "summer", "fall", "winter".
We start to calculate the distance between categories "spring" and "summer".
The distance is the sum of distances over the features temperature and weather.
For the temperature, we take all instances with season "spring", calculate the empirical cumulative distribution function and do the same for instances with season "summer" and measure their distance with the Kolmogorov-Smirnov statistic.
For the weather feature we calculate for all "spring" instances the probabilities for each weather type, do the same for the "summer" instances and sum up the absolute distances in the probability distribution.
If "spring" and "summer" have very different temperatures and weather, the total category-distance is large.
We repeat the procedure with the other seasonal pairs and reduce the resulting distance matrix to a single dimension by multi-dimensional scaling.
-->
<p>例を使用して、もう少し詳しくみてみましょう。 2つのカテゴリカル特徴量 &quot;season&quot; 、 &quot;weather&quot;、と 1つの量的特徴量 &quot;temperature&quot; を持つ場合を想定します。 最初のカテゴリカル特徴量 (season) に対して、ALEプロットを計算したいとします。 特徴量はカテゴリ、&quot;spring&quot;, &quot;summer&quot;, &quot;fall&quot;, &quot;winter&quot; を持ちます。 &quot;春&quot; と &quot;夏&quot; のカテゴリ間の距離の計算を始めます。 距離は、気温と天気の特徴量の距離の総和です。 気温に関して、季節が &quot;spring&quot; の全てのインスタンスを取得し、累積分布関数を計算します。 &quot;summer&quot; を持つインスタンスに対しても同様に計算します。そして、それらの距離を Kolmogorov-Smirnov 統計により算出します。 天気に関しても、全ての &quot;spring&quot; のインスタンスに対して各天気ごとの確率を計算し、&quot;summer&quot; のインスタンスに対しても同様の計算を行ます。 その後、確率分布の絶対距離の総和を求めます。 もし、&quot;spring&quot; と &quot;summer&quot; が大きく異なる気温と天気を持つならば、合計のカテゴリの距離は大きくなります。 この処理を他の季節のペアでも繰り返し、得られた距離の行列を multi-dimensional scaling によって、1次元に削減します。</p>
<!--
### Examples
-->
</div>
<div id="例-6" class="section level3">
<h3><span class="header-section-number">5.3.4</span> 例</h3>
<!--
Let us see ALE plots in action.
I have constructed a scenario in which partial dependence plots fail.
The scenario consists of a prediction model and two strongly correlated features.
The prediction model is mostly a linear regression model, but does something weird at a combination of the two features for which we have never observed instances.
-->
<p>ALE プロットを実際に使用してみましょう。 PDP が失敗する状況を準備しました。 この状況では、予測モデルが2つの強く相関する特徴量から構成されています。 予測モデルはほとんど線形回帰モデルですが、インスタンスが観測されていない特徴量の組合わせの部分では奇妙なことになっています。</p>
<!--
fig.cap = “Two features and the predicted outcome. The model predicts the sum of the two features (shaded background), with the exception that if x1 is greater than 0.7 and x2 less than 0.3, the model always predicts 2. This area is far from the distribution of data (point cloud) and does not affect the performance of the model and also should not affect its interpretation.”
-->
<div class="figure"><span id="fig:correlation-problem"></span>
<img src="images/correlation-problem-1.png" alt="2つの特徴量と予測結果。このモデルは2つの特徴量の和を予測している(影の背景)が、x1&gt;0.7, x2&lt;0.3 の範囲では常に2と予測するという例外があります。この領域はデータの分布(点群)とかけ離れており、モデルの性能には影響しないが、解釈にも影響を与えるべきではない。" width="1050" />
<p class="caption">
FIGURE 5.14: 2つの特徴量と予測結果。このモデルは2つの特徴量の和を予測している(影の背景)が、x1&gt;0.7, x2&lt;0.3 の範囲では常に2と予測するという例外があります。この領域はデータの分布(点群)とかけ離れており、モデルの性能には影響しないが、解釈にも影響を与えるべきではない。
</p>
</div>
<!--
Is this a realistic, relevant scenario at all?
When you train a model, the learning algorithm minimizes the loss for the existing training data instances.
Weird stuff can happen outside the distribution of training data, because the model is not penalized for doing weird stuff in these areas.
Leaving the data distribution is called extrapolation, which can also be used to fool machine learning models, described in the [chapter on adversarial examples](#adversarial).
See in our little example how the partial dependence plots behave compared to ALE plots.
-->
<p>これは現実的な状況でしょうか。 モデルを学習するとき、学習アルゴリズムは学習データの存在するインスタンスに対して誤差を最小化します。 奇妙な部分が学習データの分布の外側では起こりえます、なぜならモデルはその領域の奇妙なデータに対してペナルティを与えていないからです。 データ分布から離れることは外挿と呼ばれ、<a href="adversarial.html#adversarial">敵対的 サンプルの章</a>で説明されているように、機械学習モデルをだますことにも 使用されます。 この例で、partial dependence plots が ALE plots と比べてどのように振る舞うか見てみましょう。</p>
<!--
fig.cap = "Comparison of the feature effects computed with PDP (upper row) and ALE (lower row). The PDP estimates are influenced by the odd behavior of the model outside the data distribution (steep jumps in the plots). The ALE plots correctly identify that the machine learning model has a linear relationship between features and prediction, ignoring areas without data."
-->
<div class="figure"><span id="fig:correlation-pdp-ale-plot"></span>
<img src="images/correlation-pdp-ale-plot-1.png" alt="PDP(上の段)とALE(下の段)で計算される特徴量の影響の比較。 PDP の推定はデータ分布の外でモデルの奇妙な挙動に影響を受けている (プロットが跳ね上がっている)。ALE plot は機械学習モデルは特徴量と予測の間で関係が線形になっていると正しく認識できており、データがない領域を無視している。" width="1050" />
<p class="caption">
FIGURE 5.15: PDP(上の段)とALE(下の段)で計算される特徴量の影響の比較。 PDP の推定はデータ分布の外でモデルの奇妙な挙動に影響を受けている (プロットが跳ね上がっている)。ALE plot は機械学習モデルは特徴量と予測の間で関係が線形になっていると正しく認識できており、データがない領域を無視している。
</p>
</div>
<!--
But is it not interesting to see that our model behaves oddly at x1 > 0.7 and x2 < 0.3?
Well, yes and no.
Since these are data instances that might be physically impossible or at least extremely unlikely, it is usually irrelevant to look into these instances.
But if you suspect that your test distribution might be slightly different and some instances are actually in that range, then it would be interesting to include this area in the calculation of feature effects.
But it has to be a conscious decision to include areas where we have not observed data yet and it should not be a side-effect of the method of choice like PDP.
If you suspect that the model will later be used with differently distributed data, I recommend to use ALE plots and simulate the distribution of data you are expecting.
-->
<p>しかし、モデルが x1 &gt; 0.7, x2 &lt; 0.3 の領域での奇妙な振る舞いを知ることは、興味深いと言えないのでしょうか。 これは、どちらとも言えます。 なぜなら、これらのインスタンスは、現実的に起こり得ないのか、それともごく稀に起こりうるかであるので、一般的にはこのようなインスタンスを見るのは意味がありません。 テストデータの分布が多少異なり、いくつかのインスタンスがこの範囲に含まれるのであれば、この領域も特徴量の効果を計算する上で含めることをお勧めします。 ただし、このような、まだ観測がされていないデータの領域を含めるかどうかは意識的に決定できる必要があり、PDP のように手法の副作用となるべきではありません。 もし、モデルが異なる分布のデータで使われる可能性があるのであれば、ALE プロットを使用して可能性のあるデータ分布を用いてシミュレーションしてみることをおすすめします。</p>
<!--
Turning to a real dataset, let us predict the [number of rented bikes](#bike-data) based on weather and day and check if the ALE plots really work as well as promised.
We train a regression tree to predict the number of rented bicycles on a given day and use ALE plots to analyze how temperature, relative humidity and wind speed influence the predictions.
Let us look at what the ALE plots say:  -->
<p>次は、現実のデータセットである、<a href="bike-data.html#bike-data">レンタル自転車数</a>を天候と曜日に基づいて予測し、ALEプロットが本当に想定した通りに動いているか見てみましょう。 与えられた日のレンタル自転車数を予測するために回帰木を学習し、気温や相対湿度、風速が予測にどの程度影響を与えているか ALEプロットで分析してみます。 ALEプロットの結果を見てみましょう。</p>
<!--
fig.cap = “ALE plots for the bike prediction model by temperature, humidity and wind speed. The temperature has a strong effect on the prediction. The average prediction rises with increasing temperature, but falls again above 25 degrees Celsius. Humidity has a negative effect: When above 60%, the higher the relative humidity, the lower the prediction. The wind speed does not affect the predictions much.”
-->
<div class="figure"><span id="fig:ale-bike"></span>
<img src="images/ale-bike-1.png" alt="気温、湿度、風速による予測モデルに対する ALEプロット。気温は予測に大きく影響を与えています。予測の平均は気温の上昇に従って増えていますが摂氏25度を超えると下がる。湿度は負の影響を与えていて、60%以上では、相対湿度が高くなればなるほど、予測結果は小さくなる。風速は予測にはあまり影響を与えていない。" width="1050" />
<p class="caption">
FIGURE 5.16: 気温、湿度、風速による予測モデルに対する ALEプロット。気温は予測に大きく影響を与えています。予測の平均は気温の上昇に従って増えていますが摂氏25度を超えると下がる。湿度は負の影響を与えていて、60%以上では、相対湿度が高くなればなるほど、予測結果は小さくなる。風速は予測にはあまり影響を与えていない。
</p>
</div>
<!--
Let us look at the correlation between temperature, humidity and wind speed and all other features.
Since the data also contains categorical features, we cannot only use the Pearson correlation coefficient, which only works if both features are numerical.
Instead, I train a linear model to predict, for example, temperature based on one of the other features as input.
Then I measure how much variance the other feature in the linear model explains and take the square root.
If the other feature was numerical, then the result is equal to the absolute value of the standard Pearson correlation coefficient.
But this model-based approach of "variance-explained" (also called ANOVA, which stands for ANalysis Of VAriance) works even if the other feature is categorical.
The "variance-explained" measure lies always between 0 (no association) and 1 (temperature can be perfectly predicted from the other feature).
We calculate the explained variance of temperature, humidity and wind speed with all the other features.
The higher the explained variance (correlation), the more (potential) problems with PD plots.
The following figure visualizes how strongly the weather features are correlated with other features.
-->
<p>気温、湿度、風速とその他全ての特徴量との相関を見てみましょう。 データはカテゴリカル特徴量を含んでいるため、量的特徴量間でのみ有効なピアソンの相関係数のみを使うことはできません。 代わりに、例えば気温に対して予測する線形モデルを他の特徴量を入力として学習させます。 そして、線形モデルの他の特徴量がどの程度分散を説明するか計算し、平方根を取ります。 もし、他の特徴量が量的特徴量であれば、結果はピアソンの相関係数の絶対値と一致します。 しかし、この &quot;分散による説明&quot; のモデルに基づくアプローチ (ANalysis Of VArianceを表してANOVAとも呼ばれます) は、カテゴリカル特徴量の場合でも有効です。 &quot;分散による説明&quot; の値は、0(無関係)と1(気温を他の特徴量から完全に予測できる) の間の値となります。 気温、湿度、風速と、その他の特徴量との間の因子寄与を計算します。 因子寄与(相関)が高くなればなるほど、PDP では大きな(潜在的な)問題となります。 次の図では、天候がその他の特徴量とどの程度強く相関しているかを可視化しています。</p>
<!--
fig.cap = "The strength of the correlation between temperature, humidity and wind speed with all features, measured as the amount of variance explained, when we train a linear model with e.g. temperature to predict and season as feature. For temperature we observe -- not surprisingly -- a high correlation with season and month. Humidity correlates with weather situation."
-->
<div class="figure"><span id="fig:ale-bike-cor"></span>
<img src="images/ale-bike-cor-1.png" alt="季節を特徴量として気温を予測する線形モデルを学習したときの、気温、湿度、風速とその他全ての特徴量との相関の強さを因子寄与で計算。気温については、-- 当然ながら -- 季節や月との高い相関が見られる。湿度は天候状況と相関している。" width="1050" />
<p class="caption">
FIGURE 5.17: 季節を特徴量として気温を予測する線形モデルを学習したときの、気温、湿度、風速とその他全ての特徴量との相関の強さを因子寄与で計算。気温については、-- 当然ながら -- 季節や月との高い相関が見られる。湿度は天候状況と相関している。
</p>
</div>
<!--
This correlation analysis reveals that we may encounter problems with partial dependence plots, especially for the temperature feature.
Well, see for yourself:
-->
<p>この相関分析は 特に気温の特徴量に対して、partial dependence plots では、問題に直面するであろうことを明らかにしています。</p>
<!--
fig.cap = 'PDPs for temperature, humidity and wind speed. Compared to the ALE plots, the PDPs show a smaller decrease in predicted number of bikes for high temperature or high humidity. The PDP uses all data instances to calculate the effect of high temperatures, even if they are, for example, instances with the season "winter". The ALE plots are more reliable.
-->
<div class="figure"><span id="fig:pdp-bike-compare"></span>
<img src="images/pdp-bike-compare-1.png" alt="気温、湿度、風速に対する PDP。ALEプロットと比較すると、PDP は高気温または高湿度において予測自転車数が小さな減少を示している。PDPは、高気温の影響を計算するために、たとえそれが&quot;冬&quot;のインスタンスであっても全てのインスタンスを使う。ALEプロットの方が信頼性がある。" width="1050" />
<p class="caption">
FIGURE 5.18: 気温、湿度、風速に対する PDP。ALEプロットと比較すると、PDP は高気温または高湿度において予測自転車数が小さな減少を示している。PDPは、高気温の影響を計算するために、たとえそれが&quot;冬&quot;のインスタンスであっても全てのインスタンスを使う。ALEプロットの方が信頼性がある。
</p>
</div>
<!--
Next, let us see ALE plots in action for a categorical feature.
The month is a categorical feature for which we want to analyze the effect on the predicted number of bikes.
Arguably, the months already have a certain order (January to December), but let us try to see what happens if we first reorder the categories by similarity and then compute the effects.
The months are ordered by the similarity of days of each month based on the other features, such as temperature or whether it is a holiday.
-->
<p>次に、カテゴリカル特徴量に対するでの ALE プロットについて見てみましょう。 月は、レンタル自転車数への影響を分析したいカテゴリカル特徴量です。 ほぼ間違いなく、月は一定の並び(1月から12月まで)を既に持っていますが、初めに類似度によってカテゴリを並べ替えて、それから影響を計算したらどうなるか見てみましょう。 月は、気温や休日であるかといった他の特徴量に基づき、各月の日々の類似度によって並べられます。</p>
<!--
fig.cap = 'ALE plot for the categorical feature month. The months are ordered by their similarity to each other, based on the distributions of the other features by month. We observe that January, March and April, but especially December and November, have a lower effect on the predicted number of rented bikes compared to the other months.
-->
<div class="figure"><span id="fig:ale-bike-cat"></span>
<img src="images/ale-bike-cat-1.png" alt="カテゴリカル特徴量である月についての ALE プロット。月はその他の特徴量における分布に基づく類似度によって並べられている。他の月と比較して、1月、3月、4月、特に12月と11月は、予測自転車数に低い影響を持つことが見て取れる。" width="1050" />
<p class="caption">
FIGURE 5.19: カテゴリカル特徴量である月についての ALE プロット。月はその他の特徴量における分布に基づく類似度によって並べられている。他の月と比較して、1月、3月、4月、特に12月と11月は、予測自転車数に低い影響を持つことが見て取れる。
</p>
</div>
<!--
Since many of the features are related to weather, the order of the months strongly reflects how similar the weather is between the months.
All colder months are on the left side (February to April) and the warmer months on the right side (October to August).
Keep in mind that non-weather features have also been included in the similarity calculation, for example relative frequency of holidays has the same weight as the temperature for calculating the similarity between the months.
-->
<p>多くの特徴量は天候に関連することから、月の並びには月同士でいかに天候が似ているかが強く反映されています。 全ての寒い月は左側に(2月から4月)、暖かい月は右側に(10月から8月)あります。 ただし、天候以外の特徴量も類似度計算に含まれることに注意してください。例えば、休日の相対頻度は各月間の類似度計算において気温と同じ重みを持っています。</p>
<!--
Next, we consider the second-order effect of humidity and temperature on the predicted number of bikes.
Remember that the second-order effect is the additional interaction effect of the two features and does not include the main effects.
This means that, for example, you will not see the main effect that high humidity leads to a lower number of predicted bikes on average in the second-order ALE plot.
-->
<p>次に、予測自転車数における湿度と気温の2次効果を考えます。 2次効果は2つの特徴量における追加的な相互作用の影響であり、主効果を含まないことを思い出してください。 これは、例えば、高い湿度になると自転車レンタル予測数が平均的に下がるという効果が2次の ALE プロットでは見られなくなるということです。</p>
<!--
fig.cap = 'ALE plot for the 2nd-order effect of humidity and temperature on the predicted number of rented bikes. Lighter shade indicates an above average and darker shade a below average prediction when the main effects are already taken into account. The plot reveals an interaction between temperature and humidity: Hot and humid weather increases the prediction. In cold and humid weather an additional negative effect on the number of predicted bikes is shown.
-->
<div class="figure"><span id="fig:ale-bike-2d"></span>
<img src="images/ale-bike-2d-1.png" alt="予測自転車数における湿度と気温の2次効果に対する ALE プロット。主効果を考慮した上で、明るい影は平均より上、暗い影は平均より下の予測であることを示す。プロットは気温と湿度の相互作用を明らかにする。暑くて湿った天候は予測数を増加させ、寒くて湿った天候は予測数に対して負の影響を示す。" width="1050" />
<p class="caption">
FIGURE 5.20: 予測自転車数における湿度と気温の2次効果に対する ALE プロット。主効果を考慮した上で、明るい影は平均より上、暗い影は平均より下の予測であることを示す。プロットは気温と湿度の相互作用を明らかにする。暑くて湿った天候は予測数を増加させ、寒くて湿った天候は予測数に対して負の影響を示す。
</p>
</div>
<!--
Keep in mind that both main effects of humidity and temperature say that the predicted number of bikes decreases in very hot and humid weather.
In hot and humid weather, the combined effect of temperature and humidity is therefore not the sum of the main effects, but larger than the sum.
To emphasize the difference between the pure second-order effect (the 2D ALE plot you just saw) and the total effect, let us look at the partial dependence plot.
The PDP shows the total effect, which combines the mean prediction, the two main effects and the second-order effect (the interaction).
-->
<p>気温と湿度の両方の主効果は、とても暑くて湿気のある天候では、予測されたバイクの数が減少する事を覚えておいてください。 したがって、暑くて湿気の多い天候において、気温と湿度は主効果の総和ではなく、それより大きいのです。 純粋な二次効果 (私たちが先ほど見た the 2D ALE plot) と全ての効果の間の差を強調するために、PDP を見てみましょう。 PDP は予測の平均、2つの主効果、二次効果 (相互作用) の全ての効果を表しています。</p>
<!--
fig.cap = "PDP of the total effect of temperature and humidity on the predicted number of bikes. The plot combines the main effect of each of the features and their interaction effect, as opposed to the 2D-ALE plot which only shows the interaction."
-->
<div class="figure"><span id="fig:pdp-bike-vs-ale-2D"></span>
<img src="images/pdp-bike-vs-ale-2D-1.png" alt="予測された自転車数に対する気温と湿度の全ての効果の PDP。プロットは、相互作用のみを示す 2DーALE plots とは対照的に、それぞれの特徴量の主効果と相互作用の影響が組み合わされている。" width="1050" />
<p class="caption">
FIGURE 5.21: 予測された自転車数に対する気温と湿度の全ての効果の PDP。プロットは、相互作用のみを示す 2DーALE plots とは対照的に、それぞれの特徴量の主効果と相互作用の影響が組み合わされている。
</p>
</div>
<!--
If you are only interested in the interaction, you should look at the second-order effects, because the total effect mixes the main effects into the plot.
But if you want to know the combined effect of the features, you should look at the total effect (which the PDP shows).
For example, if you want to know the expected number of bikes at 30 degrees Celsius and 80 percent humidity, you can read it directly from the 2D PDP.
If you want to read the same from the ALE plots, you need to look at three plots:
The ALE plot for temperature, for humidity and for temperature + humidity and you also need to know the overall mean prediction.
In a scenario where two features have no interaction, the total effect plot of the two features could be misleading because it probably shows a complex landscape, suggesting some interaction, but it is simply the product of the two main effects.
The second-order effect would immediately show that there is no interaction.
-->
<p>もし相互作用のみに関心があるなら、主効果が混ざった全ての効果をみるのではなく、2次効果のみを見るべきです。 しかし、特徴量の組み合わさった影響を知りたいのであれば、PDP が示す全ての効果を見るべきです。 例えば、摂氏30度で湿度80%のときの自転車の予測台数を知りたいなら、2D PDP から直接読み取ることができます。 同じものを ALE plots から読み取ろうとすると、3つの plots を見る必要があります。 気温のみ、湿度のみ、気温と湿度の ALE plot から全体の予測平均を知る必要があります。 2つの特徴量の相互作用がない場合、単純な2つの主効果の積であるにもかかわらず、全ての効果は複雑な見た目になる可能性があり、誤解を招く恐れがあります。ただし、二次効果のみを見ることで、ただちに、相互作用がないことが示せるでしょう。</p>
<!--
Enough bicycles for now, let's turn to a classification task.
We train a random forest to predict the probability of [cervical cancer](#cervical) based on risk factors.
We visualize the accumulated local effects for two of the features: -->
<p>自転車の例はこれぐらいにして、クラス分類のタスクに移りましょう。 リスクの要因から <a href="cervical.html#cervical">子宮頸がん</a> の可能性を予測するランダムフォレストを学習します。 特徴量のうちの2つの ALE を可視化しましょう。</p>
<!--
fig.cap = ”ALE plots for the effect of age and years with hormonal contraceptives on the predicted probability of cervical cancer. For the age feature, the ALE plot shows that the predicted cancer probability is low on average up to age 40 and increases after that. The number of years with hormonal contraceptives is associated with a higher predicted cancer risk after 8 years.”
-->
<pre><code>{r ale-cervical-1D, fig.cap = &quot;子宮頸がんになる予測確率における年齢とホルモン避妊の年数の影響の ALE プロット。年齢の特徴量に関して、ALE plot は40歳までは平均してがんの確率が低く、それ以降は上昇することを示している。ホルモン避妊の年数が8年以降になると予測されるがんのリスクが高くなる傾向にある。&quot;}
data(cervical)
cervical.task = makeClassifTask(data = cervical, target = &quot;Biopsy&quot;)
mod = mlr::train(mlr::makeLearner(cl = &#39;classif.randomForest&#39;, id = &#39;cervical-rf&#39;, predict.type = &#39;prob&#39;), cervical.task)

pred.cervical = Predictor$new(mod, data = cervical, class = &quot;Cancer&quot;)
ale1 = FeatureEffect$new(pred.cervical, &quot;Age&quot;, method = &quot;ale&quot;)$plot()
ale2 = FeatureEffect$new(pred.cervical, &quot;Hormonal.Contraceptives..years.&quot;, method = &quot;ale&quot;)$plot() +
  scale_x_continuous(&quot;Years with hormonal contraceptives&quot;) +
  scale_y_continuous(&quot;&quot;)
gridExtra::grid.arrange(ale1, ale2, ncol = 2)</code></pre>
<!--
Next, we look at the interaction between number of pregnancies and age.
-->
<p>次に、妊娠回数と年齢の相互作用を見ます。</p>
<!--
fig.cap = 'ALE plot of the 2nd-order effect of number of pregnancies and age. The interpretation of the plot is a bit inconclusive, showing what seems like overfitting. For example, the plot shows an odd model behavior at age of 18-20 and more than 3 pregnancies (up to 5 percentage point increase in cancer probability). There are not many women in the data with this constellation of age and number of pregnancies (actual data are displayed as points), so the model is not severely penalized during the training for making mistakes for those women.
-->
<div class="figure"><span id="fig:ale-cervical-2d"></span>
<img src="images/ale-cervical-2d-1.png" alt="妊娠回数と年齢の2次効果に対するALEプロット。プロットの解釈は要領を得ず、過学習が発生している様子。例えば、18-20歳で3回以上の妊娠回数の場合に、プロットは妙なモデルの振る舞いを見せている(癌の確率が5%までの上昇をしている)。この区間の年齢と妊娠回数(点として表示しているデータ)の女性はデータ中で多くないため、学習時にこれらの女性に対して誤ったときの罰則が強く反映されていない。" width="1050" />
<p class="caption">
FIGURE 5.22: 妊娠回数と年齢の2次効果に対するALEプロット。プロットの解釈は要領を得ず、過学習が発生している様子。例えば、18-20歳で3回以上の妊娠回数の場合に、プロットは妙なモデルの振る舞いを見せている(癌の確率が5%までの上昇をしている)。この区間の年齢と妊娠回数(点として表示しているデータ)の女性はデータ中で多くないため、学習時にこれらの女性に対して誤ったときの罰則が強く反映されていない。
</p>
</div>
<!--
### Advantages
-->
</div>
<div id="利点" class="section level3">
<h3><span class="header-section-number">5.3.5</span> 利点</h3>
<!--
**ALE plots are unbiased**, which means they still work when features are correlated.
Partial dependence plots fail in this scenario because they marginalize over unlikely or even physically impossible combinations of feature values.

**ALE plots are faster to compute** than PDPs and scale with O(n), since the largest possible number of intervals is the number of instances with one interval per instance.
The PDP requires n times the number of grid points estimations.
For 20 grid points, PDPs require 20 times more predictions than the worst case ALE plot where as many intervals as instances are used.
-->
<p><strong>ALE plots は偏らない</strong>、つまり、ALE プロット は特徴量が相関している時でも機能します。 PDP は、相互作用があると、ありそうもない、または、現実的に不可能な特徴量の組み合わせを周辺化により作り出してしまうため、失敗します。 <strong>ALE プロットは PDP よりも計算が高速</strong> で、最大の区間の数はインスタンスの数となるため、O(n) となります。PDP は、推定するグリッド点の数の n 倍の計算が必要です。 例えば、20 個のグリッド点の場合、ALE プロットでは、最悪でもインスタンスが含まれている区間の数だけ予測すればいいのに対して、PDPでは、20倍の予測を必要とします。</p>
<!--
The **interpretation of ALE plots is clear**: Conditional on a given value, the relative effect of changing the feature on the prediction can be read from the ALE plot.
**ALE plots are centered at zero**.
This makes their interpretation nice, because the value at each point of the ALE curve is the difference to the mean prediction.
**The 2D ALE plot only shows the interaction**:
If two features do not interact, the plot shows nothing.

All in all, in most situations I would **prefer ALE plots over PDPs**, because features are usually correlated to some extent.
-->
<p><strong>ALE プロットの解釈は明白です</strong>。与えられた値の条件の元、予測においての特徴量の変化の相対的な影響はALE plotsから読み解けます。 <strong>ALE プロット0に中心化されています</strong>。 ALE 曲線の各点での値は、予測平均との差なので、解釈がしやすくなります。 <strong>2D ALE プロットは相互作用のみ示します</strong>。 もし、2つの特徴量に相互作用がない場合、プロットは何も表示しません。</p>
<p>特徴量はたいてい、何らかの形で相関しているため、私は<strong>PDPよりも、ALE プロット</strong>を用いることを好みます。</p>
<!--
### Disadvantages
-->
</div>
<div id="欠点" class="section level3">
<h3><span class="header-section-number">5.3.6</span> 欠点</h3>
<!--
**ALE plots can become a bit shaky** (many small ups and downs) with a high number of intervals.
In this case, reducing the number of intervals makes the estimates more stable, but also smoothes out and hides some of the true complexity of the prediction model.
There is **no perfect solution for setting the number of intervals**.
If the number is too small, the ALE plots might not be very accurate.
If the number is too high, the curve can become shaky.
-->
<p><strong>ALE プロットは区間数が多いとき、少し不安定(値が上下する)になることがあります</strong>。 この場合、区間数を減らすことで、推定結果を安定にできますが、予測モデルの真の複雑さのいくつかを平坦化し隠してしまう恐れがあります。 <strong>区間の数を決める完璧な解決法はありません</strong>。 もし、インターバルの数が極端に少なけば、ALE プロットは正確ではないかもしれません。 もし、インターバルの数が極端に多いと、曲線はとても不安定になる事があります。</p>
<!--
Unlike PDPs, **ALE plots are not accompanied by ICE curves**.
For PDPs, ICE curves are great because they can reveal heterogeneity in the feature effect, which means that the effect of a feature looks different for subsets of the data.
For ALE plots you can only check per interval whether the effect is different between the instances, but each interval has different instances so it is not the same as ICE curves.
-->
<p>PDP とは違って、ALE プロットには ICE 曲線が付随していません。 PDP とっては、ICE curves は偉大です。なぜなら、特徴量中の不均一性を明らかにできるためであり、これは特徴量の効果がデータの一部で異なって見えるという事を意味します。 ALE プロットに対しては、インスタンス間で効果が異なるかを区間ごとに確認するしかありませんが、各区間には異なるインスタンスがあるため、ICE 曲線と同じではありません。</p>
<!--
**Second-order ALE estimates have a varying stability across the feature space, which is not visualized in any way.**
The reason for this is that each estimation of a local effect in a cell uses a different number of data instances.
As a result, all estimates have a different accuracy (but they are still the best possible estimates).
The problem exists in a less severe version for main effect ALE plots.
The number of instances is the same in all intervals, thanks to the use of quantiles as grid, but in some areas there will be many short intervals and the ALE curve will consist of many more estimates.
But for long intervals, which can make up a big part of the entire curve, there are comparatively fewer instances.
This happened in the cervical cancer prediction ALE plot for high age for example.
-->
<p>二次の ALE プロットは、特徴量の空間での安定性が異なっており、どの様な手法でもこれを可視化できません。 この理由としては、区間内の局所効果の推定時に、使用されるインスタンス数が異なっているからです。 結果として、全ての推定は異なる精度を持っています。（ただし、これは、最良の推定です。) この問題は、主効果ALEプロットのそれほど深刻ではないバージョンに存在します。 グリッドとして、分位数を使用しているため、全ての区間で同数のインスタンスが含まれています。しかし、いくつかの領域では、多くの短い区間になり、ALE 曲線はさらに多くの推定値から構成されます。 また、曲線の大部分を占める長い区間においては、インスタンス数は比較的少なくなります。 これは、例えば、年齢が高いときの子宮頸がん予測の ALE プロットで発生しました。</p>
<!--
**Second-order effect plots can be a bit annoying to interpret**, as you always have to keep the main effects in mind.
It is tempting to read the heat maps as the total effect of the two features, but it is only the additional effect of the interaction.
The pure second-order effect is interesting for discovering and exploring interactions, but for interpreting what the effect looks like, I think it makes more sense to integrate the main effects into the plot.

The **implementation of ALE plots is much more complex** and less intuitive compared to partial dependence plots.
-->
<p>二次のALEプロットは、主効果を常に覚えておかなければいけないため、解釈するのに煩わしく感じるかもしれません。 ヒートマップを2つの特徴量の全体効果として読みたくなりますが、これは単に、相互作用による追加の効果でしかありません。 純粋な二次効果は、相互作用を発見するためには興味深いですが、その効果を理解するためには、主効果もプロットに統合する方が合理的だと考えます。</p>
<p>ALE plots の実装は、PDP に比べて、より複雑で、直感的でもありません。</p>
<!--
Even though ALE plots are not biased in case of correlated features, **interpretation remains difficult when features are strongly correlated**.
Because if they have a very strong correlation, it only makes sense to analyze the effect of changing both features together and not in isolation.
This disadvantage is not specific to ALE plots, but a general problem of strongly correlated features.

If the features are uncorrelated and computation time is not a problem, PDPs are slightly preferable because they are easier to understand and can be plotted along with ICE curves.


The list of disadvantages has become quite long, but do not be fooled by the number of words I use:
As a rule of thumb: Use ALE instead of PDP.
-->
<p>特徴量が相関している場合、ALE plots はバイアスにかかっていませんが、特徴量が強く相関している場合、解釈は困難なままです。 なぜなら、もし、それらが強い相関を持っていた場合、個々の特徴量をそれぞれ変えていくのではなく、両方を同時に変える方が、効果を分析するためには、理にかなっているからです。 この欠点は ALE plots だけではなく、特徴量が強く相関している場合の一般的な問題です。</p>
<p>もし、特徴量が相関しておらず、計算にかかる時間が問題ではない時、PDPs が少し好まれます、なぜなら、PDP は理解しやすく、そして、ICE 曲線と共にプロットできるからです。</p>
<!--
### Implementation and Alternatives
-->
</div>
<div id="実装と代替手法" class="section level3">
<h3><span class="header-section-number">5.3.7</span> 実装と代替手法</h3>
<!--
Did I mention that [partial dependence plots](#pdp) and [individual conditional expectation curves](#ice) are an alternative? =)
-->
<p><a href="pdp.html#pdp">partial dependence plots</a> と <a href="ice.html#ice">individual conditional expectation curves</a> が代替手法であることについて説明していましたか？</p>
<!--
To the best of my knowledge, ALE plots are currently only implemented in R, once in the [ALEPlot R package](https://cran.r-project.org/web/packages/ALEPlot/index.html) by the inventor himself and once in the [iml package](https://cran.r-project.org/web/packages/iml/index.html).
-->
<p>私の知る限り、ALEプロットの実装は、R言語で、提唱者によって実装された<a href="https://cran.r-project.org/web/packages/ALEPlot/index.html">ALEPlot R package</a>と<a href="https://cran.r-project.org/web/packages/iml/index.html">iml package</a>があります。</p>

<!--{pagebreak}-->
</div>
</div>
<div class="footnotes">
<hr />
<ol start="30">
<li id="fn30"><p>Apley, Daniel W. &quot;Visualizing the effects of predictor variables in black box supervised learning models.&quot; arXiv preprint arXiv:1612.08468 (2016).<a href="ale.html#fnref30">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="ice.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="interaction.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/christophM/interpretable-ml-book/edit/master/05.4-agnostic-ale.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
