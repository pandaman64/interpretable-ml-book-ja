<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>4.6 RuleFit | Interpretable Machine Learning</title>
  <meta name="description" content="Machine learning algorithms usually operate as black boxes and it is unclear how they derived a certain decision. This book is a guide for practitioners to make machine learning decisions interpretable." />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="4.6 RuleFit | Interpretable Machine Learning" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Machine learning algorithms usually operate as black boxes and it is unclear how they derived a certain decision. This book is a guide for practitioners to make machine learning decisions interpretable." />
  <meta name="github-repo" content="christophM/interpretable-ml-book" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="4.6 RuleFit | Interpretable Machine Learning" />
  
  <meta name="twitter:description" content="Machine learning algorithms usually operate as black boxes and it is unclear how they derived a certain decision. This book is a guide for practitioners to make machine learning decisions interpretable." />
  

<meta name="author" content="Christoph Molnar" />


<meta name="date" content="2021-04-20" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="rules.html"/>
<link rel="next" href="other-interpretable.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<!-- Global site tag (gtag.js) - Google Analytics -->
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-110543840-1', 'https://christophm.github.io/interpretable-ml-book/', {
  'anonymizeIp': true
  , 'storage': 'none'
  , 'clientId': window.localStorage.getItem('ga_clientId')
});
ga(function(tracker) {
  window.localStorage.setItem('ga_clientId', tracker.get('clientId'));
});
ga('send', 'pageview');
</script>

<link rel="stylesheet" type="text/css" href="css/cookieconsent.min.css" />
<script src="javascript/cookieconsent.min.js"></script>
<script>
window.addEventListener("load", function(){
window.cookieconsent.initialise({
  "palette": {
    "popup": {
      "background": "#000"
    },
    "button": {
      "background": "#f1d600"
    }
  },
  "position": "bottom-right",
  "content": {
    "message": "This website uses cookies for Google Analytics so that I know how many people are reading the book and which chapters are the most popular. The book website doesn't collect any personal data."
  }
})});
</script>



<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Interpretable machine learning</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>要約</a></li>
<li class="chapter" data-level="" data-path="著者による序文.html"><a href="著者による序文.html"><i class="fa fa-check"></i>著者による序文</a></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> イントロダクション</a><ul>
<li class="chapter" data-level="1.1" data-path="storytime.html"><a href="storytime.html"><i class="fa fa-check"></i><b>1.1</b> 物語の時間</a><ul>
<li class="chapter" data-level="" data-path="storytime.html"><a href="storytime.html#稲妻は二度と打たない"><i class="fa fa-check"></i>稲妻は二度と打たない</a></li>
<li class="chapter" data-level="" data-path="storytime.html"><a href="storytime.html#信用失墜"><i class="fa fa-check"></i>信用失墜</a></li>
<li class="chapter" data-level="" data-path="storytime.html"><a href="storytime.html#フェルミのペーパークリップ"><i class="fa fa-check"></i>フェルミのペーパー・クリップ</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="機械学習とは何か.html"><a href="機械学習とは何か.html"><i class="fa fa-check"></i><b>1.2</b> 機械学習とは何か？</a></li>
<li class="chapter" data-level="1.3" data-path="terminology.html"><a href="terminology.html"><i class="fa fa-check"></i><b>1.3</b> 専門用語</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="interpretability.html"><a href="interpretability.html"><i class="fa fa-check"></i><b>2</b> 解釈可能性</a><ul>
<li class="chapter" data-level="2.1" data-path="interpretability-importance.html"><a href="interpretability-importance.html"><i class="fa fa-check"></i><b>2.1</b> 解釈可能性の重要性</a></li>
<li class="chapter" data-level="2.2" data-path="解釈可能な手法の分類.html"><a href="解釈可能な手法の分類.html"><i class="fa fa-check"></i><b>2.2</b> 解釈可能な手法の分類</a></li>
<li class="chapter" data-level="2.3" data-path="解釈可能性の範囲.html"><a href="解釈可能性の範囲.html"><i class="fa fa-check"></i><b>2.3</b> 解釈可能性の範囲</a><ul>
<li class="chapter" data-level="2.3.1" data-path="解釈可能性の範囲.html"><a href="解釈可能性の範囲.html#アルゴリズムの透明性"><i class="fa fa-check"></i><b>2.3.1</b> アルゴリズムの透明性</a></li>
<li class="chapter" data-level="2.3.2" data-path="解釈可能性の範囲.html"><a href="解釈可能性の範囲.html#全体的なモデルの解釈可能性"><i class="fa fa-check"></i><b>2.3.2</b> 全体的なモデルの解釈可能性</a></li>
<li class="chapter" data-level="2.3.3" data-path="解釈可能性の範囲.html"><a href="解釈可能性の範囲.html#モジュールレベルのモデルの全体的な解釈可能性"><i class="fa fa-check"></i><b>2.3.3</b> モジュールレベルのモデルの全体的な解釈可能性</a></li>
<li class="chapter" data-level="2.3.4" data-path="解釈可能性の範囲.html"><a href="解釈可能性の範囲.html#単一の予測に対する局所的な解釈"><i class="fa fa-check"></i><b>2.3.4</b> 単一の予測に対する局所的な解釈</a></li>
<li class="chapter" data-level="2.3.5" data-path="解釈可能性の範囲.html"><a href="解釈可能性の範囲.html#予測のグループに対する局所的な解釈"><i class="fa fa-check"></i><b>2.3.5</b> 予測のグループに対する局所的な解釈</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="解釈可能性の評価.html"><a href="解釈可能性の評価.html"><i class="fa fa-check"></i><b>2.4</b> 解釈可能性の評価</a></li>
<li class="chapter" data-level="2.5" data-path="properties.html"><a href="properties.html"><i class="fa fa-check"></i><b>2.5</b> 説明に関する性質</a></li>
<li class="chapter" data-level="2.6" data-path="explanation.html"><a href="explanation.html"><i class="fa fa-check"></i><b>2.6</b> 人間に優しい説明</a><ul>
<li class="chapter" data-level="2.6.1" data-path="explanation.html"><a href="explanation.html#説明とはなにか"><i class="fa fa-check"></i><b>2.6.1</b> 説明とはなにか</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="data.html"><a href="data.html"><i class="fa fa-check"></i><b>3</b> データセット</a><ul>
<li class="chapter" data-level="3.1" data-path="bike-data.html"><a href="bike-data.html"><i class="fa fa-check"></i><b>3.1</b> 自転車レンタル (回帰)</a></li>
<li class="chapter" data-level="3.2" data-path="spam-data.html"><a href="spam-data.html"><i class="fa fa-check"></i><b>3.2</b> YouTube スパムコメント (テキスト分類)</a></li>
<li class="chapter" data-level="3.3" data-path="cervical.html"><a href="cervical.html"><i class="fa fa-check"></i><b>3.3</b> 子宮頸がんのリスク要因(クラス分類)</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="simple.html"><a href="simple.html"><i class="fa fa-check"></i><b>4</b> 解釈可能なモデル</a><ul>
<li class="chapter" data-level="4.1" data-path="limo.html"><a href="limo.html"><i class="fa fa-check"></i><b>4.1</b> 線形回帰</a><ul>
<li class="chapter" data-level="4.1.1" data-path="limo.html"><a href="limo.html#解釈"><i class="fa fa-check"></i><b>4.1.1</b> 解釈</a></li>
<li class="chapter" data-level="4.1.2" data-path="limo.html"><a href="limo.html#例"><i class="fa fa-check"></i><b>4.1.2</b> 例</a></li>
<li class="chapter" data-level="4.1.3" data-path="limo.html"><a href="limo.html#可視化による解釈"><i class="fa fa-check"></i><b>4.1.3</b> 可視化による解釈</a></li>
<li class="chapter" data-level="4.1.4" data-path="limo.html"><a href="limo.html#個々の予測に対する説明"><i class="fa fa-check"></i><b>4.1.4</b> 個々の予測に対する説明</a></li>
<li class="chapter" data-level="4.1.5" data-path="limo.html"><a href="limo.html#カテゴリカル特徴量のエンコーディング"><i class="fa fa-check"></i><b>4.1.5</b> カテゴリカル特徴量のエンコーディング</a></li>
<li class="chapter" data-level="4.1.6" data-path="limo.html"><a href="limo.html#線形モデルは良い説明を与えるか"><i class="fa fa-check"></i><b>4.1.6</b> 線形モデルは良い説明を与えるか?</a></li>
<li class="chapter" data-level="4.1.7" data-path="limo.html"><a href="limo.html#sparse-linear"><i class="fa fa-check"></i><b>4.1.7</b> スパースな線形モデル</a></li>
<li class="chapter" data-level="4.1.8" data-path="limo.html"><a href="limo.html#長所"><i class="fa fa-check"></i><b>4.1.8</b> 長所</a></li>
<li class="chapter" data-level="4.1.9" data-path="limo.html"><a href="limo.html#短所"><i class="fa fa-check"></i><b>4.1.9</b> 短所</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="logistic.html"><a href="logistic.html"><i class="fa fa-check"></i><b>4.2</b> ロジスティック回帰</a><ul>
<li class="chapter" data-level="4.2.1" data-path="logistic.html"><a href="logistic.html#線形回帰モデルを分類のために使うと何がいけないか"><i class="fa fa-check"></i><b>4.2.1</b> 線形回帰モデルを分類のために使うと何がいけないか。</a></li>
<li class="chapter" data-level="4.2.2" data-path="logistic.html"><a href="logistic.html#理論"><i class="fa fa-check"></i><b>4.2.2</b> 理論</a></li>
<li class="chapter" data-level="4.2.3" data-path="logistic.html"><a href="logistic.html#解釈性"><i class="fa fa-check"></i><b>4.2.3</b> 解釈性</a></li>
<li class="chapter" data-level="4.2.4" data-path="logistic.html"><a href="logistic.html#例-1"><i class="fa fa-check"></i><b>4.2.4</b> 例</a></li>
<li class="chapter" data-level="4.2.5" data-path="logistic.html"><a href="logistic.html#長所と短所"><i class="fa fa-check"></i><b>4.2.5</b> 長所と短所</a></li>
<li class="chapter" data-level="4.2.6" data-path="logistic.html"><a href="logistic.html#ソフトウェア"><i class="fa fa-check"></i><b>4.2.6</b> ソフトウェア</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="extend-lm.html"><a href="extend-lm.html"><i class="fa fa-check"></i><b>4.3</b> GLM、GAM、その他</a><ul>
<li class="chapter" data-level="4.3.1" data-path="extend-lm.html"><a href="extend-lm.html#glm"><i class="fa fa-check"></i><b>4.3.1</b> 結果が正規分布に従わない場合 - GLMs</a></li>
<li class="chapter" data-level="4.3.2" data-path="extend-lm.html"><a href="extend-lm.html#lm-interact"><i class="fa fa-check"></i><b>4.3.2</b> 相互作用</a></li>
<li class="chapter" data-level="4.3.3" data-path="extend-lm.html"><a href="extend-lm.html#gam"><i class="fa fa-check"></i><b>4.3.3</b> 非線形効果 - GAM</a></li>
<li class="chapter" data-level="4.3.4" data-path="extend-lm.html"><a href="extend-lm.html#長所-1"><i class="fa fa-check"></i><b>4.3.4</b> 長所</a></li>
<li class="chapter" data-level="4.3.5" data-path="extend-lm.html"><a href="extend-lm.html#短所-1"><i class="fa fa-check"></i><b>4.3.5</b> 短所</a></li>
<li class="chapter" data-level="4.3.6" data-path="extend-lm.html"><a href="extend-lm.html#ソフトウェア-1"><i class="fa fa-check"></i><b>4.3.6</b> ソフトウェア</a></li>
<li class="chapter" data-level="4.3.7" data-path="extend-lm.html"><a href="extend-lm.html#more-lm-extension"><i class="fa fa-check"></i><b>4.3.7</b> さらなる拡張</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="tree.html"><a href="tree.html"><i class="fa fa-check"></i><b>4.4</b> 決定木</a><ul>
<li class="chapter" data-level="4.4.1" data-path="tree.html"><a href="tree.html#決定木の解釈"><i class="fa fa-check"></i><b>4.4.1</b> 決定木の解釈</a></li>
<li class="chapter" data-level="4.4.2" data-path="tree.html"><a href="tree.html#例-2"><i class="fa fa-check"></i><b>4.4.2</b> 例</a></li>
<li class="chapter" data-level="4.4.3" data-path="tree.html"><a href="tree.html#長所-2"><i class="fa fa-check"></i><b>4.4.3</b> 長所</a></li>
<li class="chapter" data-level="4.4.4" data-path="tree.html"><a href="tree.html#短所-2"><i class="fa fa-check"></i><b>4.4.4</b> 短所</a></li>
<li class="chapter" data-level="4.4.5" data-path="tree.html"><a href="tree.html#ソフトウェア-2"><i class="fa fa-check"></i><b>4.4.5</b> ソフトウェア</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="rules.html"><a href="rules.html"><i class="fa fa-check"></i><b>4.5</b> 決定規則</a><ul>
<li class="chapter" data-level="4.5.1" data-path="rules.html"><a href="rules.html#単一の特徴量による規則学習-oner"><i class="fa fa-check"></i><b>4.5.1</b> 単一の特徴量による規則学習 (OneR)</a></li>
<li class="chapter" data-level="4.5.2" data-path="rules.html"><a href="rules.html#sequential-covering"><i class="fa fa-check"></i><b>4.5.2</b> Sequential Covering</a></li>
<li class="chapter" data-level="4.5.3" data-path="rules.html"><a href="rules.html#bayesian-rule-lists"><i class="fa fa-check"></i><b>4.5.3</b> Bayesian Rule Lists</a></li>
<li class="chapter" data-level="4.5.4" data-path="rules.html"><a href="rules.html#長所-3"><i class="fa fa-check"></i><b>4.5.4</b> 長所</a></li>
<li class="chapter" data-level="4.5.5" data-path="rules.html"><a href="rules.html#短所-3"><i class="fa fa-check"></i><b>4.5.5</b> 短所</a></li>
<li class="chapter" data-level="4.5.6" data-path="rules.html"><a href="rules.html#ソフトウェアと代替手法"><i class="fa fa-check"></i><b>4.5.6</b> ソフトウェアと代替手法</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="rulefit.html"><a href="rulefit.html"><i class="fa fa-check"></i><b>4.6</b> RuleFit</a><ul>
<li class="chapter" data-level="4.6.1" data-path="rulefit.html"><a href="rulefit.html#解釈と例"><i class="fa fa-check"></i><b>4.6.1</b> 解釈と例</a></li>
<li class="chapter" data-level="4.6.2" data-path="rulefit.html"><a href="rulefit.html#理論-1"><i class="fa fa-check"></i><b>4.6.2</b> 理論</a></li>
<li class="chapter" data-level="4.6.3" data-path="rulefit.html"><a href="rulefit.html#長所-4"><i class="fa fa-check"></i><b>4.6.3</b> 長所</a></li>
<li class="chapter" data-level="4.6.4" data-path="rulefit.html"><a href="rulefit.html#短所-4"><i class="fa fa-check"></i><b>4.6.4</b> 短所</a></li>
<li class="chapter" data-level="4.6.5" data-path="rulefit.html"><a href="rulefit.html#ソフトウェアと代替手法-1"><i class="fa fa-check"></i><b>4.6.5</b> ソフトウェアと代替手法</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="other-interpretable.html"><a href="other-interpretable.html"><i class="fa fa-check"></i><b>4.7</b> その他の解釈可能なモデル</a><ul>
<li class="chapter" data-level="4.7.1" data-path="other-interpretable.html"><a href="other-interpretable.html#単純ベイズ分類器-naive-bayes-classifier"><i class="fa fa-check"></i><b>4.7.1</b> 単純ベイズ分類器 (Naive Bayes Classifier)</a></li>
<li class="chapter" data-level="4.7.2" data-path="other-interpretable.html"><a href="other-interpretable.html#k近傍法"><i class="fa fa-check"></i><b>4.7.2</b> k近傍法</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="agnostic.html"><a href="agnostic.html"><i class="fa fa-check"></i><b>5</b> モデル非依存(Model-Agnostic)な手法</a><ul>
<li class="chapter" data-level="5.1" data-path="pdp.html"><a href="pdp.html"><i class="fa fa-check"></i><b>5.1</b> Partial Dependence Plot (PDP)</a><ul>
<li class="chapter" data-level="5.1.1" data-path="pdp.html"><a href="pdp.html#例-3"><i class="fa fa-check"></i><b>5.1.1</b> 例</a></li>
<li class="chapter" data-level="5.1.2" data-path="pdp.html"><a href="pdp.html#長所-5"><i class="fa fa-check"></i><b>5.1.2</b> 長所</a></li>
<li class="chapter" data-level="5.1.3" data-path="pdp.html"><a href="pdp.html#短所-5"><i class="fa fa-check"></i><b>5.1.3</b> 短所</a></li>
<li class="chapter" data-level="5.1.4" data-path="pdp.html"><a href="pdp.html#ソフトウェアと代替手法-2"><i class="fa fa-check"></i><b>5.1.4</b> ソフトウェアと代替手法</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="ice.html"><a href="ice.html"><i class="fa fa-check"></i><b>5.2</b> Individual Conditional Expectation (ICE)</a><ul>
<li class="chapter" data-level="5.2.1" data-path="ice.html"><a href="ice.html#例-4"><i class="fa fa-check"></i><b>5.2.1</b> 例</a></li>
<li class="chapter" data-level="5.2.2" data-path="ice.html"><a href="ice.html#長所-6"><i class="fa fa-check"></i><b>5.2.2</b> 長所</a></li>
<li class="chapter" data-level="5.2.3" data-path="ice.html"><a href="ice.html#短所-6"><i class="fa fa-check"></i><b>5.2.3</b> 短所</a></li>
<li class="chapter" data-level="5.2.4" data-path="ice.html"><a href="ice.html#ソフトウェアと代替手法-3"><i class="fa fa-check"></i><b>5.2.4</b> ソフトウェアと代替手法</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="ale.html"><a href="ale.html"><i class="fa fa-check"></i><b>5.3</b> Accumulated Local Effects (ALE) Plot</a><ul>
<li class="chapter" data-level="5.3.1" data-path="ale.html"><a href="ale.html#モチベーションと直感"><i class="fa fa-check"></i><b>5.3.1</b> モチベーションと直感</a></li>
<li class="chapter" data-level="5.3.2" data-path="ale.html"><a href="ale.html#理論-2"><i class="fa fa-check"></i><b>5.3.2</b> 理論</a></li>
<li class="chapter" data-level="5.3.3" data-path="ale.html"><a href="ale.html#予測"><i class="fa fa-check"></i><b>5.3.3</b> 予測</a></li>
<li class="chapter" data-level="5.3.4" data-path="ale.html"><a href="ale.html#例-6"><i class="fa fa-check"></i><b>5.3.4</b> 例</a></li>
<li class="chapter" data-level="5.3.5" data-path="ale.html"><a href="ale.html#利点"><i class="fa fa-check"></i><b>5.3.5</b> 利点</a></li>
<li class="chapter" data-level="5.3.6" data-path="ale.html"><a href="ale.html#欠点"><i class="fa fa-check"></i><b>5.3.6</b> 欠点</a></li>
<li class="chapter" data-level="5.3.7" data-path="ale.html"><a href="ale.html#実装と代替手法"><i class="fa fa-check"></i><b>5.3.7</b> 実装と代替手法</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="interaction.html"><a href="interaction.html"><i class="fa fa-check"></i><b>5.4</b> 特徴量の相互作用</a><ul>
<li class="chapter" data-level="5.4.1" data-path="interaction.html"><a href="interaction.html#特徴量の相互作用とは"><i class="fa fa-check"></i><b>5.4.1</b> 特徴量の相互作用とは</a></li>
<li class="chapter" data-level="5.4.2" data-path="interaction.html"><a href="interaction.html#friedman-の-h統計量の理論"><i class="fa fa-check"></i><b>5.4.2</b> Friedman の H統計量の理論</a></li>
<li class="chapter" data-level="5.4.3" data-path="interaction.html"><a href="interaction.html#例-7"><i class="fa fa-check"></i><b>5.4.3</b> 例</a></li>
<li class="chapter" data-level="5.4.4" data-path="interaction.html"><a href="interaction.html#利点-1"><i class="fa fa-check"></i><b>5.4.4</b> 利点</a></li>
<li class="chapter" data-level="5.4.5" data-path="interaction.html"><a href="interaction.html#欠点-1"><i class="fa fa-check"></i><b>5.4.5</b> 欠点</a></li>
<li class="chapter" data-level="5.4.6" data-path="interaction.html"><a href="interaction.html#実装"><i class="fa fa-check"></i><b>5.4.6</b> 実装</a></li>
<li class="chapter" data-level="5.4.7" data-path="interaction.html"><a href="interaction.html#代替手法"><i class="fa fa-check"></i><b>5.4.7</b> 代替手法</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="feature-importance.html"><a href="feature-importance.html"><i class="fa fa-check"></i><b>5.5</b> Permutation Feature Importance</a><ul>
<li class="chapter" data-level="5.5.1" data-path="feature-importance.html"><a href="feature-importance.html#理論-3"><i class="fa fa-check"></i><b>5.5.1</b> 理論</a></li>
<li class="chapter" data-level="5.5.2" data-path="feature-importance.html"><a href="feature-importance.html#feature-importance-data"><i class="fa fa-check"></i><b>5.5.2</b> 特徴量の重要度は、学習データとテストデータのどちらで計算するべきか</a></li>
<li class="chapter" data-level="5.5.3" data-path="feature-importance.html"><a href="feature-importance.html#例と解釈"><i class="fa fa-check"></i><b>5.5.3</b> 例と解釈</a></li>
<li class="chapter" data-level="5.5.4" data-path="feature-importance.html"><a href="feature-importance.html#利点-2"><i class="fa fa-check"></i><b>5.5.4</b> 利点</a></li>
<li class="chapter" data-level="5.5.5" data-path="feature-importance.html"><a href="feature-importance.html#欠点-2"><i class="fa fa-check"></i><b>5.5.5</b> 欠点</a></li>
<li class="chapter" data-level="5.5.6" data-path="feature-importance.html"><a href="feature-importance.html#ソフトウェアと代替手法-4"><i class="fa fa-check"></i><b>5.5.6</b> ソフトウェアと代替手法</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="global.html"><a href="global.html"><i class="fa fa-check"></i><b>5.6</b> グローバルサロゲート (Global Surrogate)</a><ul>
<li class="chapter" data-level="5.6.1" data-path="global.html"><a href="global.html#理論-4"><i class="fa fa-check"></i><b>5.6.1</b> 理論</a></li>
<li class="chapter" data-level="5.6.2" data-path="global.html"><a href="global.html#例-8"><i class="fa fa-check"></i><b>5.6.2</b> 例</a></li>
<li class="chapter" data-level="5.6.3" data-path="global.html"><a href="global.html#長所-7"><i class="fa fa-check"></i><b>5.6.3</b> 長所</a></li>
<li class="chapter" data-level="5.6.4" data-path="global.html"><a href="global.html#短所-7"><i class="fa fa-check"></i><b>5.6.4</b> 短所</a></li>
<li class="chapter" data-level="5.6.5" data-path="global.html"><a href="global.html#ソフトウェア-3"><i class="fa fa-check"></i><b>5.6.5</b> ソフトウェア</a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="lime.html"><a href="lime.html"><i class="fa fa-check"></i><b>5.7</b> Local Surrogate (LIME)</a><ul>
<li class="chapter" data-level="5.7.1" data-path="lime.html"><a href="lime.html#表形式データにおける-lime"><i class="fa fa-check"></i><b>5.7.1</b> 表形式データにおける LIME</a></li>
<li class="chapter" data-level="5.7.2" data-path="lime.html"><a href="lime.html#テキストデータに対するlime"><i class="fa fa-check"></i><b>5.7.2</b> テキストデータに対するLIME</a></li>
<li class="chapter" data-level="5.7.3" data-path="lime.html"><a href="lime.html#images-lime"><i class="fa fa-check"></i><b>5.7.3</b> 画像データに対するLIME</a></li>
<li class="chapter" data-level="5.7.4" data-path="lime.html"><a href="lime.html#長所-8"><i class="fa fa-check"></i><b>5.7.4</b> 長所</a></li>
<li class="chapter" data-level="5.7.5" data-path="lime.html"><a href="lime.html#短所-8"><i class="fa fa-check"></i><b>5.7.5</b> 短所</a></li>
</ul></li>
<li class="chapter" data-level="5.8" data-path="anchors.html"><a href="anchors.html"><i class="fa fa-check"></i><b>5.8</b> Scoped Rules (Anchors)</a><ul>
<li class="chapter" data-level="5.8.1" data-path="anchors.html"><a href="anchors.html#anchor-の発見"><i class="fa fa-check"></i><b>5.8.1</b> Anchor の発見</a></li>
<li class="chapter" data-level="5.8.2" data-path="anchors.html"><a href="anchors.html#複雑性と実行時間"><i class="fa fa-check"></i><b>5.8.2</b> 複雑性と実行時間</a></li>
<li class="chapter" data-level="5.8.3" data-path="anchors.html"><a href="anchors.html#表形式データの例"><i class="fa fa-check"></i><b>5.8.3</b> 表形式データの例</a></li>
<li class="chapter" data-level="5.8.4" data-path="anchors.html"><a href="anchors.html#長所-9"><i class="fa fa-check"></i><b>5.8.4</b> 長所</a></li>
<li class="chapter" data-level="5.8.5" data-path="anchors.html"><a href="anchors.html#短所-9"><i class="fa fa-check"></i><b>5.8.5</b> 短所</a></li>
<li class="chapter" data-level="5.8.6" data-path="anchors.html"><a href="anchors.html#ソフトウェアと代替手法-5"><i class="fa fa-check"></i><b>5.8.6</b> ソフトウェアと代替手法</a></li>
</ul></li>
<li class="chapter" data-level="5.9" data-path="shapley.html"><a href="shapley.html"><i class="fa fa-check"></i><b>5.9</b> シャープレイ値 (Shapley Values)</a><ul>
<li class="chapter" data-level="5.9.1" data-path="shapley.html"><a href="shapley.html#一般的なアイデア"><i class="fa fa-check"></i><b>5.9.1</b> 一般的なアイデア</a></li>
<li class="chapter" data-level="5.9.2" data-path="shapley.html"><a href="shapley.html#例と解釈-1"><i class="fa fa-check"></i><b>5.9.2</b> 例と解釈</a></li>
<li class="chapter" data-level="5.9.3" data-path="shapley.html"><a href="shapley.html#シャープレイ値の詳細"><i class="fa fa-check"></i><b>5.9.3</b> シャープレイ値の詳細</a></li>
<li class="chapter" data-level="5.9.4" data-path="shapley.html"><a href="shapley.html#長所-10"><i class="fa fa-check"></i><b>5.9.4</b> 長所</a></li>
<li class="chapter" data-level="5.9.5" data-path="shapley.html"><a href="shapley.html#短所-10"><i class="fa fa-check"></i><b>5.9.5</b> 短所</a></li>
<li class="chapter" data-level="5.9.6" data-path="shapley.html"><a href="shapley.html#ソフトウェアと代替手法-6"><i class="fa fa-check"></i><b>5.9.6</b> ソフトウェアと代替手法</a></li>
</ul></li>
<li class="chapter" data-level="5.10" data-path="shap.html"><a href="shap.html"><i class="fa fa-check"></i><b>5.10</b> SHAP (SHapley Additive exPlanations)</a><ul>
<li class="chapter" data-level="5.10.1" data-path="shap.html"><a href="shap.html#定義"><i class="fa fa-check"></i><b>5.10.1</b> 定義</a></li>
<li class="chapter" data-level="5.10.2" data-path="shap.html"><a href="shap.html#kernelshap"><i class="fa fa-check"></i><b>5.10.2</b> KernelSHAP</a></li>
<li class="chapter" data-level="5.10.3" data-path="shap.html"><a href="shap.html#treeshap"><i class="fa fa-check"></i><b>5.10.3</b> TreeSHAP</a></li>
<li class="chapter" data-level="5.10.4" data-path="shap.html"><a href="shap.html#例-12"><i class="fa fa-check"></i><b>5.10.4</b> 例</a></li>
<li class="chapter" data-level="5.10.5" data-path="shap.html"><a href="shap.html#shap-特徴量重要度-shap-feature-importance"><i class="fa fa-check"></i><b>5.10.5</b> SHAP 特徴量重要度 (SHAP Feature Importance)</a></li>
<li class="chapter" data-level="5.10.6" data-path="shap.html"><a href="shap.html#shap-summary-plot"><i class="fa fa-check"></i><b>5.10.6</b> SHAP Summary Plot</a></li>
<li class="chapter" data-level="5.10.7" data-path="shap.html"><a href="shap.html#shap-dependence-plot"><i class="fa fa-check"></i><b>5.10.7</b> SHAP Dependence Plot</a></li>
<li class="chapter" data-level="5.10.8" data-path="shap.html"><a href="shap.html#shap-相互作用値-shap-interaction-values"><i class="fa fa-check"></i><b>5.10.8</b> SHAP 相互作用値 (SHAP Interaction Values)</a></li>
<li class="chapter" data-level="5.10.9" data-path="shap.html"><a href="shap.html#clustering-shap-values"><i class="fa fa-check"></i><b>5.10.9</b> Clustering SHAP values</a></li>
<li class="chapter" data-level="5.10.10" data-path="shap.html"><a href="shap.html#長所-11"><i class="fa fa-check"></i><b>5.10.10</b> 長所</a></li>
<li class="chapter" data-level="5.10.11" data-path="shap.html"><a href="shap.html#短所-11"><i class="fa fa-check"></i><b>5.10.11</b> 短所</a></li>
<li class="chapter" data-level="5.10.12" data-path="shap.html"><a href="shap.html#ソフトウェア-4"><i class="fa fa-check"></i><b>5.10.12</b> ソフトウェア</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="example-based.html"><a href="example-based.html"><i class="fa fa-check"></i><b>6</b> 例示に基づいた説明手法</a><ul>
<li class="chapter" data-level="6.1" data-path="反事実的.html"><a href="反事実的.html"><i class="fa fa-check"></i><b>6.1</b> 反事実的説明 (Counterfactual Explanations)</a><ul>
<li class="chapter" data-level="6.1.1" data-path="反事実的.html"><a href="反事実的.html#反事実的説明の生成"><i class="fa fa-check"></i><b>6.1.1</b> 反事実的説明の生成</a></li>
<li class="chapter" data-level="6.1.2" data-path="反事実的.html"><a href="反事実的.html#例-13"><i class="fa fa-check"></i><b>6.1.2</b> 例</a></li>
<li class="chapter" data-level="6.1.3" data-path="反事実的.html"><a href="反事実的.html#長所-12"><i class="fa fa-check"></i><b>6.1.3</b> 長所</a></li>
<li class="chapter" data-level="6.1.4" data-path="反事実的.html"><a href="反事実的.html#短所-12"><i class="fa fa-check"></i><b>6.1.4</b> 短所</a></li>
<li class="chapter" data-level="6.1.5" data-path="反事実的.html"><a href="反事実的.html#ソフトウェアと代替手法-7"><i class="fa fa-check"></i><b>6.1.5</b> ソフトウェアと代替手法</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="adversarial.html"><a href="adversarial.html"><i class="fa fa-check"></i><b>6.2</b> 敵対的サンプル (Adversarial Examples)</a><ul>
<li class="chapter" data-level="6.2.1" data-path="adversarial.html"><a href="adversarial.html#手法及び例"><i class="fa fa-check"></i><b>6.2.1</b> 手法及び例</a></li>
<li class="chapter" data-level="6.2.2" data-path="adversarial.html"><a href="adversarial.html#サイバーセキュリティーの観点"><i class="fa fa-check"></i><b>6.2.2</b> サイバーセキュリティーの観点</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="proto.html"><a href="proto.html"><i class="fa fa-check"></i><b>6.3</b> prototype と criticism</a><ul>
<li class="chapter" data-level="6.3.1" data-path="proto.html"><a href="proto.html#理論-5"><i class="fa fa-check"></i><b>6.3.1</b> 理論</a></li>
<li class="chapter" data-level="6.3.2" data-path="proto.html"><a href="proto.html#例-14"><i class="fa fa-check"></i><b>6.3.2</b> 例</a></li>
<li class="chapter" data-level="6.3.3" data-path="proto.html"><a href="proto.html#長所-13"><i class="fa fa-check"></i><b>6.3.3</b> 長所</a></li>
<li class="chapter" data-level="6.3.4" data-path="proto.html"><a href="proto.html#短所-13"><i class="fa fa-check"></i><b>6.3.4</b> 短所</a></li>
<li class="chapter" data-level="6.3.5" data-path="proto.html"><a href="proto.html#コードと代替手法"><i class="fa fa-check"></i><b>6.3.5</b> コードと代替手法</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="influential.html"><a href="influential.html"><i class="fa fa-check"></i><b>6.4</b> Influential Instances</a><ul>
<li class="chapter" data-level="6.4.1" data-path="influential.html"><a href="influential.html#deletion-diagnostics"><i class="fa fa-check"></i><b>6.4.1</b> Deletion Diagnostics</a></li>
<li class="chapter" data-level="6.4.2" data-path="influential.html"><a href="influential.html#影響関数-influence-functions"><i class="fa fa-check"></i><b>6.4.2</b> 影響関数 (Influence Functions)</a></li>
<li class="chapter" data-level="6.4.3" data-path="influential.html"><a href="influential.html#長所-14"><i class="fa fa-check"></i><b>6.4.3</b> 長所</a></li>
<li class="chapter" data-level="6.4.4" data-path="influential.html"><a href="influential.html#短所-14"><i class="fa fa-check"></i><b>6.4.4</b> 短所</a></li>
<li class="chapter" data-level="6.4.5" data-path="influential.html"><a href="influential.html#ソフトウェアと代替手法-8"><i class="fa fa-check"></i><b>6.4.5</b> ソフトウェアと代替手法</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="neural-networks.html"><a href="neural-networks.html"><i class="fa fa-check"></i><b>7</b> ニューラルネットワークの解釈</a><ul>
<li class="chapter" data-level="7.1" data-path="cnn-features.html"><a href="cnn-features.html"><i class="fa fa-check"></i><b>7.1</b> 学習された特徴量</a><ul>
<li class="chapter" data-level="7.1.1" data-path="cnn-features.html"><a href="cnn-features.html#特徴量の可視化"><i class="fa fa-check"></i><b>7.1.1</b> 特徴量の可視化</a></li>
<li class="chapter" data-level="7.1.2" data-path="cnn-features.html"><a href="cnn-features.html#ネットワークの解剖"><i class="fa fa-check"></i><b>7.1.2</b> ネットワークの解剖</a></li>
<li class="chapter" data-level="7.1.3" data-path="cnn-features.html"><a href="cnn-features.html#利点-3"><i class="fa fa-check"></i><b>7.1.3</b> 利点</a></li>
<li class="chapter" data-level="7.1.4" data-path="cnn-features.html"><a href="cnn-features.html#欠点-3"><i class="fa fa-check"></i><b>7.1.4</b> 欠点</a></li>
<li class="chapter" data-level="7.1.5" data-path="cnn-features.html"><a href="cnn-features.html#ソフトウェアとその他の資料"><i class="fa fa-check"></i><b>7.1.5</b> ソフトウェアとその他の資料</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="future.html"><a href="future.html"><i class="fa fa-check"></i><b>8</b> 解釈可能な機械学習の未来</a><ul>
<li class="chapter" data-level="8.1" data-path="機械学習の未来.html"><a href="機械学習の未来.html"><i class="fa fa-check"></i><b>8.1</b> 機械学習の未来</a></li>
<li class="chapter" data-level="8.2" data-path="解釈性の未来.html"><a href="解釈性の未来.html"><i class="fa fa-check"></i><b>8.2</b> 解釈性の未来</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="contribute.html"><a href="contribute.html"><i class="fa fa-check"></i><b>9</b> 著者貢献</a></li>
<li class="chapter" data-level="10" data-path="cite.html"><a href="cite.html"><i class="fa fa-check"></i><b>10</b> この本の引用</a></li>
<li class="chapter" data-level="11" data-path="translations.html"><a href="translations.html"><i class="fa fa-check"></i><b>11</b> 翻訳</a></li>
<li class="chapter" data-level="12" data-path="謝辞.html"><a href="謝辞.html"><i class="fa fa-check"></i><b>12</b> 謝辞</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a><ul>
<li class="chapter" data-level="" data-path="r-packages-used-for-examples.html"><a href="r-packages-used-for-examples.html"><i class="fa fa-check"></i>R Packages Used for Examples</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Interpretable Machine Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="rulefit" class="section level2">
<h2><span class="header-section-number">4.6</span> RuleFit</h2>
<!--The RuleFit algorithm by Friedman and Popescu (2008)[^Friedman2008] learns sparse linear models that include automatically detected interaction effects in the form of decision rules.-->
<p>Friedman と Popescu (2008)<a href="#fn24" class="footnoteRef" id="fnref24"><sup>24</sup></a> による RuleFit アルゴリズムは、相互作用効果を決定規則の形で自動的に検出したスパース線形モデルの学習に使われます。</p>
<!--The linear regression model does not account for interactions between features.
Would it not be convenient to have a model that is as simple and interpretable as linear models, but also integrates feature interactions?
RuleFit fills this gap.
RuleFit learns a sparse linear model with the original features and also a number of new features that are decision rules.
These new features capture interactions between the original features.
RuleFit automatically generates these features from decision trees.
Each path through a tree can be transformed into a decision rule by combining the split decisions into a rule.
The node predictions are discarded and only the splits are used in the decision rules:
-->
<p>線形回帰モデルは特徴間の相互作用を考慮していません。 線形モデルのようにシンプルで解釈しやすいモデルでありながら、特徴間の相互作用を統合したモデルがあれば便利ではないでしょうか？ 実は、RuleFit はギャップを埋めることができます。 RuleFit は、元の特徴量と決定規則である多数の新しい特徴量を用いて、スパース線形モデルを学習します。 これらの新しい特徴量は、元の特徴量間の相互作用を説明します。 RuleFit は、決定木からこれらの特徴量を自動的に生成します。 分割された決定を結合し、規則にすることで、木を通る各パスを決定規則に変換できます。 ノードによる予測を破棄し、分割のみを決定規則に使用します。</p>
<!--fig.cap="4 rules can be generated from a tree with 3 terminal nodes."-->
<div class="figure"><span id="fig:rulefit-split"></span>
<img src="images/rulefit.jpg" alt="3つの終端ノードを持つ木から4つの規則を生成することができます。" width="500" />
<p class="caption">
FIGURE 4.21: 3つの終端ノードを持つ木から4つの規則を生成することができます。
</p>
</div>
<!--
Where do those decision trees come from?
The trees are trained to predict the outcome of interest.
This ensures that the splits are meaningful for the prediction task.
Any algorithm that generates a lot of trees can be used for RuleFit, for example a random forest.
Each tree is decomposed into decision rules that are used as additional features in a sparse linear regression model (Lasso).
-->
<p>これらの決定木はどこから来ているのでしょうか？ 決定木は興味のある結果を予測するために学習されます。 これは、予測問題に対して分割が意味を持つことを保証しています。 ランダムフォレストのように、多数の木を生成するアルゴリズムを RuleFit に使うことができます。 それぞれの木は、スパース線形回帰モデル (Lasso) で使用される追加の特徴量である決定規則に分解されます。</p>
<!--
The RuleFit paper uses the Boston housing data to illustrate this:
The goal is to predict the median house value of a Boston neighborhood.
One of the rules generated by RuleFit is:
IF `number of rooms > 6.64`  AND  `concentration of nitric oxide <0.67` THEN 1 ELSE 0.
-->
<p>RuleFit が提案された論文では、ボストンの住宅データを使って説明しています。 目標は、ボストンの住宅の中央値を予測することです。 RuleFit で生成された規則の1つは、<code>部屋の数 &gt; 6.64</code> かつ <code>一酸化炭素濃度 &lt; 0.67</code> であるならば 1、そうでないならば 0 としています。</p>
<!--
RuleFit also comes with a feature importance measure that helps to identify linear terms and rules that are important for the predictions.
Feature importance is calculated from the weights of the regression model.
The importance measure can be aggregated for the original features (which are used in their "raw" form and possibly in many decision rules).
-->
<p>RuleFit は、予測に重要な線形項や規則を特定するための重要度の指標としても機能します。 特徴量の重要度は回帰モデルの回帰係数から計算されます。 重要度の指標は元の特徴量（&quot;生&quot;の形で使われ、多くの決定規則で使われる可能性があります）を集約したものになります。</p>
<!--
RuleFit also introduces partial dependence plots to show the average change in prediction by changing a feature.
The partial dependence plot is a model-agnostic method that can be used with any model, and is explained in the [book chapter on partial dependence plots](#pdp).
-->
<p>RuleFit は、特徴量を変更することで、予測値の平均的な変化を示す partial dependence plot を導入します。 partial dependence plot は、どんなモデルにも使うことのできるモデル診断の手法であり、<a href="pdp.html#pdp">partial dependence plots</a>の章で解説されています。</p>
<!--### Interpretation and Example-->
<div id="解釈と例" class="section level3">
<h3><span class="header-section-number">4.6.1</span> 解釈と例</h3>
<!--
Since RuleFit estimates a linear model in the end, the interpretation is the same as for "normal" [linear models](#limo).
The only difference is that the model has new features derived from decision rules.
Decision rules are binary features:
A value of 1 means that all conditions of the rule are met, otherwise the value is 0.
For linear terms in RuleFit, the interpretation is the same as in linear regression models:
If the feature increases by one unit, the predicted outcome changes by the corresponding feature weight.
-->
<p>RuleFit は最終的には線形モデルを推定するので、解釈は&quot;普通の&quot;<a href="limo.html#limo">線形モデル</a>と同じになります。 違いは、決定規則からなる新しい特徴量を有していることです。 値が 1 であることは全ての条件を満たしていることを示し、そうでない場合は値は 0 になります。 RuleFit における線形項の解釈は、線形回帰モデルと同様になります。ある特徴量が 1 増加すると、予測結果は特徴量の重みに応じて変化します。</p>
<!--
In this example, we use RuleFit to predict the number of [rented bicycles](#bike-data) on a given day.
The table shows five of the rules that were generated by RuleFit, along with their Lasso weights and importances.
The calculation is explained later in the chapter.
-->
<p>この例では、RuleFit をある日付の<a href="bike-data.html#bike-data">レンタル自転車数</a>を予測するために使用しています。 この表は、RuleFit によって生成された5つの規則と、それらの Lasso による重みと重要性を示しています。 計算については、この章の後半で説明します。</p>
<table>
<thead>
<tr class="header">
<th align="left">Description</th>
<th align="right">Weight</th>
<th align="right">Importance</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">days_since_2011 &gt; 111 &amp; weathersit in (&quot;GOOD&quot;, &quot;MISTY&quot;)</td>
<td align="right">793</td>
<td align="right">303</td>
</tr>
<tr class="even">
<td align="left">37.25 &lt;= hum &lt;= 90</td>
<td align="right">-20</td>
<td align="right">272</td>
</tr>
<tr class="odd">
<td align="left">temp &gt; 13 &amp; days_since_2011 &gt; 554</td>
<td align="right">676</td>
<td align="right">239</td>
</tr>
<tr class="even">
<td align="left">4 &lt;= windspeed &lt;= 24</td>
<td align="right">-41</td>
<td align="right">202</td>
</tr>
<tr class="odd">
<td align="left">days_since_2011 &gt; 428 &amp; temp &gt; 5</td>
<td align="right">366</td>
<td align="right">179</td>
</tr>
</tbody>
</table>
<!--
The most important rule was: "days_since_2011 > 111 & weathersit in ("GOOD", "MISTY")" and the corresponding weight is 793.
The interpretation is:
If days_since_2011 > 111 & weathersit in ("GOOD", "MISTY"), then the predicted number of bikes increases by 793, when all other feature values remain fixed.
In total, 278 such rules were created from the original 8 features.
Quite a lot!
But thanks to Lasso, only 58 of the 278 have a weight different from 0.
-->
<p>最も重要な規則は &quot;days_since_2011 &gt; 111 &amp; weathersit in (&quot;GOOD&quot;, &quot;MISTY&quot;)&quot; であり、対応する重みは 793 です。 このような規則は、元の 8 つの特徴量から合計 278 個が作成されました。 かなりの数です! しかし Lasso のおかげで、278 のうち 58 だけが 0 ではない重みを持っていることがわかります。</p>
<!--
Computing the global feature importances reveals that temperature and time trend are the most important features:
-->
<p>大域的な特徴量の重要性を計算すると、気温と時間の傾向が最も重要な特徴量であることがわかります。</p>
<!--fig.cap = 'Feature importance measures for a RuleFit model predicting bike counts. The most important features for the predictions were temperature and time trend.'-->
<div class="figure"><span id="fig:rulefit-importance"></span>
<img src="images/rulefit-importance-1.png" alt="自転車の数を予測する RuleFit モデルの特徴量重要度。予測のために最も重要な特徴量は気温と時間傾向でした。" width="1050" />
<p class="caption">
FIGURE 4.22: 自転車の数を予測する RuleFit モデルの特徴量重要度。予測のために最も重要な特徴量は気温と時間傾向でした。
</p>
</div>
<!--
The feature importance measurement includes the importance of the raw feature term and all the decision rules in which the feature appears.
-->
<p>特徴量重要度の尺度は、生の特徴量の重要度と、その特徴量が現れるすべての決定規則を含みます。</p>
<!--**Interpretation template**-->
<p><strong>解釈のテンプレート</strong></p>
<!--
The interpretation is analogous to linear models:
The predicted outcome changes by $\beta_j$ if feature $x_j$ changes by one unit, provided all other features remain unchanged.
The weight interpretation of a decision rule is a special case:
If all conditions of a decision rule $r_k$ apply, the predicted outcome changes by $\alpha_k$ (the learned weight of rule $r_k$ in the linear model).
-->
<p>解釈は、線形モデルと類似しています。 他の特徴量が固定されている場合、特徴量 <span class="math inline">\(x_j\)</span> が 1 変化すると、予測結果は <span class="math inline">\(\beta_j\)</span> だけ変化します。 決定規則に関する重みの解釈は特殊です。 決定規則 <span class="math inline">\(r_k\)</span> のすべての条件を満たすならば、予測結果は <span class="math inline">\(\alpha_k\)</span> (線形モデルで学習された規則 <span class="math inline">\(r_k\)</span> の重み) だけ変化します。</p>
<!--
For classification (using logistic regression instead of linear regression):
If all conditions of the decision rule $r_k$ apply, the odds for event vs. no-event changes by a factor of $\alpha_k$.
-->
<p>分類問題では（線形回帰ではなくロジスティック回帰を用いた場合）、 決定規則 <span class="math inline">\(r_k\)</span> の全ての条件を満たすなら、その事象が発生するかしないかのオッズが <span class="math inline">\(\alpha_k\)</span> 倍変化します。</p>
<!--### Theory-->
</div>
<div id="理論-1" class="section level3">
<h3><span class="header-section-number">4.6.2</span> 理論</h3>
<!--
Let us dive deeper into the technical details of the RuleFit algorithm.
RuleFit consists of two components:
The first component creates "rules" from decision trees and the second component fits a linear model with the original features and the new rules as input (hence the name "RuleFit").
-->
<p>RuleFit アルゴリズムの技術的な詳細について深く見ていくことにしましょう。 RuleFit は2つのコンポーネントで構成されています。 最初のコンポーネントは、決定木から&quot;規則&quot;を作成し、2番目のコンポーネントでは、元の特徴量と作成した規則を入力とする線形モデルを学習します（これが &quot;RuleFit&quot; という名前の由来です）。</p>
<!--**Step 1: Rule generation**-->
<p><strong>Step 1: 規則の生成</strong></p>
<!--
What does a rule look like?
The rules generated by the algorithm have a simple form.
For example:
IF `x2 < 3` AND `x5 < 7` THEN 1 ELSE 0.
The rules are constructed by decomposing decision trees:
Any path to a node in a tree can be converted to a decision rule.
The trees used for the rules are fitted to predict the target outcome.
Therefore the splits and resulting rules are optimized to predict the outcome you are interested in.
You simply chain the binary decisions that lead to a certain node with "AND", and voilà, you have a rule.
It is desirable to generate a lot of diverse and meaningful rules.
Gradient boosting is used to fit an ensemble of decision trees by regressing or classifying y with your original features X.
Each resulting tree is converted into multiple rules.
Not only boosted trees, but any tree ensemble algorithm can be used to generate the trees for RuleFit.
A tree ensemble can be described with this general formula:
-->
<p>規則とはどのようなものでしょうか？ アルゴリズムによって生成された規則は単純な形式になります。 例えば、IF <code>x2 &lt; 3</code> AND <code>x5 &lt; 7</code> THEN 1 ELSE 0 といったものです。 規則は、決定木を分解することで構築されます。 決定木上の任意のパスは、決定規則に変換できます。 規則のための木は、出力を予測するために利用されます。 したがって、分割や得られる規則は興味のある結果を得るために最適化されています。 特定のノードに至る二分決定を &quot;AND&quot; で連結させるだけで規則ができます。 多様かつ意味のある規則を多く生成することが望まれます。 勾配ブースティングでは、y を元の特徴量 X を使って回帰あるいは分類をすることで、決定木のアンサンブルを学習させます。 そして作成された各々の木は、複数の規則に変換されます。 ブースティングに限らず、任意の木のアンサンブルアルゴリズムに対して、RuleFit の木を生成できます。 木のアンサンブルは、次の一般的な式で記述できます。</p>
<p><span class="math display">\[f(x)=a_0+\sum_{m=1}^M{}a_m{}f_m(X)\]</span></p>
<!--
M is the number of trees and $f_m(x)$ is the prediction function of the m-th tree.
The $a$'s are the weights.
Bagged ensembles, random forest, AdaBoost and MART produce tree ensembles  and can be used for RuleFit.
-->
<p>M は木の数であり、<span class="math inline">\(f_m(x)\)</span> は m 番目の木の予測関数です。 <span class="math inline">\(a\)</span> は重みです。 Bagged ensembles、ランダムフォレスト、AdaBoost、そして MART は木のアンサンブルを生成し、RuleFit で使用されます。</p>
<!--
We create the rules from all trees of the ensemble.
Each rule $r_m$ takes the form of:
-->
<p>アンサンブルの全ての木から規則を作成します。 各規則 <span class="math inline">\(r_m\)</span> は次の形式で表されます。</p>
<p><span class="math display">\[r_m(x)=\prod_{j\in\text{T}_m}I(x_j\in{}s_{jm})\]</span></p>
<!--
where $\text{T}_{m}$ is the set of features used in the m-th tree, I is the indicator function that is 1 when feature $x_j$ is in the specified subset of values s for the j-th feature (as specified by the tree splits) and 0 otherwise.
For numerical features, $s_{jm}$ is an interval in the value range of the feature.
The interval looks like one of the two cases:
-->
<p>ここで、<span class="math inline">\(\text{T}_{m}\)</span> は、m 番目の木で利用される特徴量の集合です。 I は、特徴量 <span class="math inline">\(x_j\)</span> が j 番目の特徴量（木の分割で指定されたもの）に対する部分集合 s に含まれる場合に 1、それ以外の場合に 0 となる指示関数です。 量的特徴量の場合、<span class="math inline">\(s_{jm}\)</span> は特徴量の値の区間となります。 区間は次の2つの場合のいずれかのようになります。</p>
<p><span class="math display">\[x_{s_{jm},\text{lower}}&lt;x_j\]</span></p>
<p><span class="math display">\[x_j&lt;x_{s_{jm},upper}\]</span></p>
<!--
Further splits in that feature possibly lead to more complicated intervals.
For categorical features the subset s contains some specific categories of the feature.
-->
<p>特徴量を更に分割すると、より複雑な区間になる可能性があります。 カテゴリカル特徴量の場合、部分集合は特徴量の特定のカテゴリが含まれることになります。</p>
<!--A made up example for the bike rental dataset:-->
<p>自転車レンタルのデータセットの例を見てみましょう。</p>
<p><span class="math display">\[r_{17}(x)=I(x_{\text{temp}}&lt;15)\cdot{}I(x_{\text{weather}}\in\{\text{good},\text{cloudy}\})\cdot{}I(10\leq{}x_{\text{windspeed}}&lt;20)\]</span></p>
<!--
This rule returns 1 if all three conditions are met, otherwise 0.
RuleFit extracts all possible rules from a tree, not only from the leaf nodes.
So another rule that would be created is:
-->
<p>この規則は、3つの条件全てが満たされた場合に 1、それ以外は 0 を返します。 RuleFit は、葉だけではなく、木の全てのノードから規則を抽出します。 したがって、作成されるであろう規則は次のようになります。</p>
<p><span class="math display">\[r_{18}(x)=I(x_{\text{temp}}&lt;15)\cdot{}I(x_{\text{weather}}\in\{\text{good},\text{cloudy}\}\]</span></p>
<!--
Altogether, the number of rules created from an ensemble of M trees with $t_m$ terminal nodes each is:
-->
<p>全体として、<span class="math inline">\(t_m\)</span> 個の葉をもつ M 個の木のアンサンブルから作成される規則の数は次式で与えられます。</p>
<p><span class="math display">\[K=\sum_{m=1}^M2(t_m-1)\]</span></p>
<!--
A trick introduced by the RuleFit authors is to learn trees with random depth so that many diverse rules with different lengths are generated.
Note that we discard the predicted value in each node and only keep the conditions that lead us to a node and then we create a rule from it.
The weighting of the decision rules is done in step 2 of RuleFit.
-->
<p>RuleFit の著者によって導入されたトリックは、ランダムな深さの木を学習することで、長さの異なる多種多様な規則を生成するというものです。 各ノードにおける予測値は破棄して、そのノードに至る条件のみを保持し、そこから規則を作るということに注意してください。 決定規則の重みづけは、RuleFit の第2ステップで行われます。</p>
<!--
Another way to see step 1:
RuleFit generates a new set of features from your original features.
These features are binary and can represent quite complex interactions of your original features.
The rules are chosen to maximize the prediction task.
The rules are automatically generated from the covariates matrix X.
You can simply see the rules as new features based on your original features.
-->
<p>ステップ1はこのように見ることもできます。 RuleFit は、元の特徴量から新しい特徴量の集合を生成します。 これらの特徴量は、二値であり、元の特徴量の極めて複雑な相互作用を表現できます。 規則は予測タスクで最良の結果が得られるように選択されます。 規則は、共変量行列Xから自動的に生成されます。 規則は元の特徴量に基づく新たな特徴量としてみなすことができます。</p>
<!--**Step 2: Sparse linear model**-->
<p><strong>Step 2: スパース線形モデル</strong></p>
<!--
You get MANY rules in step 1.
Since the first step can be seen as only a feature transformation, you are still not done with fitting a model.
Also, you want to reduce the number of rules.
In addition to the rules, all your "raw" features from your original dataset will  also be used in the sparse linear model.
Every rule and every original feature becomes a feature in the linear model and gets a weight estimate.
The original raw features are added because trees fail at representing simple linear relationships between y and x.
Before we train a sparse linear model, we winsorize the original features so that they are more robust against outliers:
-->
<p>ステップ1で、多くの規則を得ることができます。 この最初のステップは、単なる特徴量の変換にすぎないため、モデルへの適合はまだ終わっていません。 また、規則の数を減らしたいとも思うでしょう。 これらの規則に加えて、元のデータセットの全ての&quot;生&quot;の特徴量も、スパース線形モデルで利用することになります。 全ての規則と元の特徴量が線形モデルの特徴量となり、重みが推定値されます。 元の生の特徴量を追加するのは、木は y と x の間の単純な線形関係を表現するのに失敗するためです。 スパース線形モデルを学習する前に、元の特徴量の外れ値をクリッピング (winsorizing) し、外れ値に対してより頑健になるようにします。</p>
<p><span class="math display">\[l_j^*(x_j)=min(\delta_j^+,max(\delta_j^-,x_j))\]</span></p>
<!--
where $\delta_j^-$ and $\delta_j^+$ are the $\delta$ quantiles of the data distribution of feature $x_j$.
A choice of 0.05 for $\delta$ means that any value of feature $x_j$ that is in the 5% lowest or 5% highest values will be set to the quantiles at 5% or 95% respectively.
As a rule of thumb, you can choose $\delta$ = 0.025.
In addition, the linear terms have to be normalized so that they have the same prior importance as a typical decision rule:
-->
<p>ここで、<span class="math inline">\(\delta_j^-\)</span> と <span class="math inline">\(\delta_j^+\)</span> は、特徴量 <span class="math inline">\(x_j\)</span> のデータ分布の <span class="math inline">\(\delta\)</span> 分位数です。 <span class="math inline">\(\delta\)</span> に0.05を選択すると、上位 5％ または下位 5％ の特徴量 <span class="math inline">\(x_j\)</span> の値が、それぞれ 5％ または 95％ の分位数に設定されます。 経験則として、<span class="math inline">\(\delta\)</span> = 0.025 を選択できます。 さらに、線形項は、通常の決定規則と事前の重要性が同一となるように正規化する必要があります。</p>
<p><span class="math display">\[l_j(x_j)=0.4\cdot{}l^*_j(x_j)/std(l^*_j(x_j))\]</span></p>
<!--
The $0.4$ is the average standard deviation of rules with a uniform support distribution of $s_k\sim{}U(0,1)$.
-->
<p><span class="math inline">\(0.4\)</span> は、<span class="math inline">\(s_k\sim{}U(0,1)\)</span> の一様なサポート分布を持つ規則の標準偏差の平均です。</p>
<!--
We combine both types of features to generate a new feature matrix and train a sparse linear model with Lasso, with the following structure:
-->
<p>両方のタイプの特徴量を組み合わせて、新たな特徴量行列を作成し、次の形式で Lasso を利用してスパース線形モデルを学習します。</p>
<p><span class="math display">\[\hat{f}(x)=\hat{\beta}_0+\sum_{k=1}^K\hat{\alpha}_k{}r_k(x)+\sum_{j=1}^p\hat{\beta}_j{}l_j(x_j)\]</span></p>
<!--
where $\hat{\alpha}$ is the estimated weight vector for the rule features and $\hat{\beta}$ the weight vector for the original features.
Since RuleFit uses Lasso, the loss function gets the additional constraint that forces some of the weights to get a zero estimate:
-->
<p>ここで、<span class="math inline">\(\hat{\alpha}\)</span> は、規則の特徴量に対して推定された重みベクトルであり、<span class="math inline">\(\hat{\beta}\)</span> は、元の特徴量に対する重みベクトルです。 RuleFit は Lasso を利用するため、損失関数は、一部の重みを 0 にするための制約が必要になります。</p>
<p><span class="math display">\[(\{\hat{\alpha}\}_1^K,\{\hat{\beta}\}_0^p)=argmin_{\{\hat{\alpha}\}_1^K,\{\hat{\beta}\}_0^p}\sum_{i=1}^n{}L(y^{(i)},f(x^{(i)}))+\lambda\cdot\left(\sum_{k=1}^K|\alpha_k|+\sum_{j=1}^p|b_j|\right)\]</span></p>
<!--
The result is a linear model that has linear effects for all of the original features and for the rules.
The interpretation is the same as for linear models, the only difference is that some features are now binary rules.
-->
<p>この結果は、元の全ての特徴量と規則に対して線形な効果をもつ線形モデルです。 解釈は、線形モデルの場合と同様ですが、唯一の違いは、一部の特徴量が二値の規則となっている点です。</p>
<!--**Step 3 (optional): Feature importance**-->
<p><strong>Step3（optional）: 特徴量重要度</strong></p>
<!--
For the linear terms of the original features, the feature importance is measured with the standardized predictor:
-->
<p>元の特徴量の線形項については、標準化された予測器を利用して特徴量重要度を測定します。</p>
<p><span class="math display">\[I_j=|\hat{\beta}_j|\cdot{}std(l_j(x_j))\]</span></p>
<!--
where $\beta_j$ is the weight from the Lasso model and $std(l_j(x_j))$ is the standard deviation of the linear term over the data.
-->
<p>ここで、<span class="math inline">\(\beta_j\)</span> は、Lasso モデルから得られた重みであり、<span class="math inline">\(std(l_j(x_j))\)</span> はデータ全体の線形項の標準偏差です。</p>
<!--For the decision rule terms, the importance is calculated with the following formula:-->
<p>決定規則の項の場合、重要度は次式で計算されます。</p>
<p><span class="math display">\[I_k=|\hat{\alpha}_k|\cdot\sqrt{s_k(1-s_k)}\]</span></p>
<!--
where $\hat{\alpha}_k$ is the associated Lasso weight of the decision rule and $s_k$ is the support of the feature in the data, which is the percentage of data points to which the decision rule applies (where $r_k(x)=1$):
-->
<p>ここで、<span class="math inline">\(\hat{\alpha}_k\)</span> は、決定規則の関連するLassoの重みであり、<span class="math inline">\(s_k\)</span> は、データにおける特徴量のサポートであり、決定規則が適用されるデータの割合です（ここで、<span class="math inline">\(r_k(x)=1\)</span> ）。</p>
<p><span class="math display">\[s_k=\frac{1}{n}\sum_{i=1}^n{}r_k(x^{(i)})\]</span></p>
<!--
A feature occurs as a linear term and possibly also within many decision rules.
How do we measure the total importance of a feature?
The importance $J_j(x)$ of a feature can be measured for each individual prediction:
-->
<p>特徴量は線形項として現れるだけでなく、場合によっては多くの決定規則の内部にも現れます。 どのように特徴量の重要度を測るべきでしょうか？ 特徴量の重要度 <span class="math inline">\(J_j(x)\)</span> は、個々の予測ごとに測定できます。</p>
<p><span class="math display">\[J_j(x)=I_j(x)+\sum_{x_j\in{}r_k}I_k(x)/m_k\]</span></p>
<!--
where $I_l$ is the importance of the linear term and $I_k$ the importance of the decision rules in which $x_j$ appears, and $m_k$ is the number of features constituting the rule $r_k$.
Adding the feature importance from all instances gives us the global feature importance:
-->
<p>ここで、<span class="math inline">\(I_l\)</span> は線形項の重要度、<span class="math inline">\(I_k\)</span> は <span class="math inline">\(x_j\)</span> が現れる決定規則の重要度、<span class="math inline">\(m_k\)</span> は規則 <span class="math inline">\(r_k\)</span> を構成する特徴量の数です。 全ての事例から特徴量の重要度を足し合わせることで、大域的な重要度を得ることができます。</p>
<p><span class="math display">\[J_j(X)=\sum_{i=1}^n{}J_j(x^{(i)})\]</span></p>
<!--
It is possible to select a subset of instances and calculate the feature importance for this group.
-->
<p>事例の部分集合を選択して、そのグループの特徴量重要度を計算できます。</p>
<!--### Advantages-->
</div>
<div id="長所-4" class="section level3">
<h3><span class="header-section-number">4.6.3</span> 長所</h3>
<!--
RuleFit automatically adds **feature interactions** to linear models.
Therefore, it solves the problem of linear models that you have to add interaction terms manually and it helps a bit with the issue of modeling nonlinear relationships.
-->
<p>RuleFitは<strong>特徴量間の相互作用</strong>を線形モデルに自動で追加します。 したがって、相互作用項を手動で追加する必要のある線形モデルの問題を解決し、非線形関係をモデリングする問題にも少し役立ちます。</p>
<!--
RuleFit can handle both classification and regression tasks.
-->
<p>RuleFit は分類問題と回帰問題の両方を扱えます。</p>
<!--
The rules created are easy to interpret, because they are binary decision rules.
Either the rule applies to an instance or not.
Good interpretability is only guaranteed if the number of conditions within a rule is not too large.
A rule with 1 to 3 conditions seems reasonable to me.
This means a maximum depth of 3 for the trees in the tree ensemble.
-->
<p>作成される決定規則は二値であるため、規則が観測データに適用されるかどうかを調べることで簡単に解釈できます。 優れた解釈可能性は、決定規則内の条件の数が多すぎない場合にのみ保証されます。 個人的には、1〜3 個の条件の決定規則が合理的だと思います。 つまり、アンサンブルの木の最大の深さは 3 が良いということです。</p>
<!--
Even if there are many rules in the model, they do not apply to every instance.
For an individual instance only a handful of rules apply (= have a non-zero weights).
This improves local interpretability.
-->
<p>たとえモデルに多くの決定規則がある場合でも、それらがすべての観測データに適用されるわけではありません。 個々の観測データにはほんのひと握りの決定規則のみ（= 非ゼロの重みを持つ）が適用されます。 これにより、個々のデータに対する解釈可能性が向上します。</p>
<!--
RuleFit proposes a bunch of useful diagnostic tools. 
These tools are model-agnostic, so you can find them in the model-agnostic section of the book: [feature importance](#feature-importance),  [partial dependence plots](#pdp) and [feature interactions](#interaction).
-->
<p>RuleFitは便利な診断ツールを多数提供しています。 これらのツールはモデルに依存しないため、この本のモデル非依存 (model-agnostic) のセクションで紹介されています：<a href="feature-importance.html#feature-importance">特徴量重要度</a>、<a href="pdp.html#pdp">partial dependence plots</a>、<a href="interaction.html#interaction">特徴量の相互作用</a>。</p>
<!--### Disadvantages-->
</div>
<div id="短所-4" class="section level3">
<h3><span class="header-section-number">4.6.4</span> 短所</h3>
<!--
Sometimes RuleFit creates many rules that get a non-zero weight in the Lasso model.
The interpretability degrades with increasing number of features in the model.
A promising solution is to force feature effects to be monotonic, meaning that an increase of a feature has to lead to an increase of the prediction.
-->
<p>RuleFit は、Lasso モデルにおいて非ゼロな重みを得るたくさんの規則を作り出すことがあります。 解釈性は特徴量の数が増えるにつれ低下します。 有望な解決策としては特徴量の影響を単調にすることです。 つまり、特徴量が増加すると、予測結果も増加する必要があるということです。</p>
<!--
An anecdotal drawback: The papers claim a good performance of RuleFit -- often close to the predictive performance of random forests! -- but in the few cases where I tried it personally, the performance was disappointing.
Just try it out for your problem and see how it performs.
-->
<p>論文では度々 RuleFit の性能が、ランダムフォレストの予測性能に匹敵するほど良いと主張しています。 しかしながら、私が個人的に試したいくつかの場合において、がっかりするような性能でした。 まず、適用してみてどのような性能が出るかを確認しましょう。</p>
<!--
The end product of the RuleFit procedure is a linear model with additional fancy features (the decision rules).
But since it is a linear model, the weight interpretation is still unintuitive.
It comes with the same "footnote" as a usual linear regression model:
"... given all features are fixed."
It gets a bit more tricky when you have overlapping rules.
For example, one decision rule (feature) for the bicycle prediction could be: "temp > 10" and another rule could be "temp > 15 & weather='GOOD'".
If the weather is good and the temperature is above 15 degrees, the temperature is automatically greater then 10.
In the cases where the second rule applies, the first rule applies as well.
The interpretation of the estimated weight for the second rule is:
"Assuming all other features remain fixed, the predicted number of bikes increases by $\beta_2$ when the weather is good and temperature above 15 degrees.".
But, now it becomes really clear that the 'all other feature fixed' is problematic, because if rule 2 applies, also rule 1 applies and the interpretation is nonsensical.
-->
<p>RuleFit の手順をふんで得られる最終生成物は、追加の特徴（決定規則）を持つ線形モデルです。 しかし、線形モデルであるからこそ、重みの解釈が直感的ではありません。 通常の線形回帰モデルと同様に、&quot;...他の全ての特徴量が固定されている場合に限る。&quot;という&quot;脚注&quot;がついています。 また、規則が重複していると少し厄介になります。 例えば、自転車の数を予測するための1つの決定規則（特徴量）として &quot;temp &gt; 10&quot; と &quot;temp &gt; 15 &amp; weather='GOOD'&quot; があるとします。 天気が良く、気温が15度以上であれば、自動的に気温が10度以上になります。 2つ目の規則が満たされているときに、1つ目の規則も満たされています。 2つ目の規則における推測された重みの解釈は&quot;他の特徴量が固定され、天気が良く、気温が15度以上のとき、予測される自転車の数は <span class="math inline">\(\beta_2\)</span> 増加する。&quot;となります。 しかしここで、&quot;他の特徴量が固定された場合&quot;というのが問題になってきます。 なぜなら、2つ目の規則が適合しているとき、1つ目の規則にも適合し、解釈が意味の無いものになってしまうからです。</p>
<!--### Software and Alternative-->
</div>
<div id="ソフトウェアと代替手法-1" class="section level3">
<h3><span class="header-section-number">4.6.5</span> ソフトウェアと代替手法</h3>
<!--
The RuleFit algorithm is implemented in R by Fokkema and Christoffersen (2017)[^Fokkema] and you can find a [Python version on Github](https://github.com/christophM/rulefit).
-->
<p>RuleFit アルゴリズムは R では Fokkema と Christoffersen (2017)<a href="#fn25" class="footnoteRef" id="fnref25"><sup>25</sup></a> によって実装されています。 Python 実装は <a href="https://github.com/christophM/rulefit">Github</a> 上にもあります。</p>
<!--
A very similar framework is [skope-rules](https://github.com/scikit-learn-contrib/skope-rules), a Python module that also extracts rules from ensembles.
It differs in the way it learns the final rules:
First, skope-rules remove low-performing rules, based on recall and precision thresholds.
Then, duplicate and similar rules are removed by performing a selection based on the diversity of logical terms (variable + larger/smaller operator) and performance (F1-score) of the rules.
This final step does not rely on using Lasso, but considers only the out-of-bag F1-score and the logical terms which form the rules.
-->
<p>非常によく似たフレームワークは <a href="https://github.com/scikit-learn-contrib/skope-rules">skope-rules</a> という Python のモジュールでアンサンブルから規則を抽出します。 これは最終的な規則を学習する方法が違います。 まず、skope-rules はパフォーマンスのよくない規則を、recall（再現性）とprecision（適合率）に基づいて除去します。 そして、重複あるいは似ている規則が、論理項（変数 + 大なり／小なり）の多様性や F1-score に基づいて除去します。 最後に Lasso を用いる代わりに、out-of-bag の F1-score や規則を構成する論理項を用います。</p>

<!--{pagebreak}-->
<!--
## Other Interpretable Models {#other-interpretable}
-->
</div>
</div>
<div class="footnotes">
<hr />
<ol start="24">
<li id="fn24"><p>Friedman, Jerome H, and Bogdan E Popescu. &quot;Predictive learning via rule ensembles.&quot; The Annals of Applied Statistics. JSTOR, 916–54. (2008).<a href="rulefit.html#fnref24">↩</a></p></li>
<li id="fn25"><p>Fokkema, Marjolein, and Benjamin Christoffersen. &quot;Pre: Prediction rule ensembles&quot;. <a href="https://CRAN.R-project.org/package=pre" class="uri">https://CRAN.R-project.org/package=pre</a> (2017).<a href="rulefit.html#fnref25">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="rules.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="other-interpretable.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/christophM/interpretable-ml-book/edit/master/04.7-interpretable-rulefit.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
