```{r, message = FALSE, warning = FALSE, echo = FALSE}
devtools::load_all()
set.seed(42)
```

<!--{pagebreak}-->


<!--
## Linear Regression {#limo}
-->
## 線形回帰 {#limo}

<!-- A linear regression model predicts the target as a weighted sum of the feature inputs.
The linearity of the learned relationship makes the interpretation easy.
Linear regression models have long been used by statisticians, computer scientists and other people who tackle quantitative problems. -->
線形回帰モデルは予測値を特徴量の重み付き和として表します。
学習された関係の線形性が解釈を簡単にしてくれます。
線形回帰モデルは長い間、定量的な問題に取り組む統計学者や計算機科学者たちによって使用されてきました。

<!-- Linear models can be used to model the dependence of a regression target y on some features x.
The learned relationships are linear and can be written for a single instance i as follows: -->
線形モデルでは、特徴量 x が目的変数 y にどれくらい依存するかをモデリングできます。
学習された関係は線形で、1つのデータ i に対して次のように表すことができます。

$$y=\beta_{0}+\beta_{1}x_{1}+\ldots+\beta_{p}x_{p}+\epsilon$$

<!-- The predicted outcome of an instance is a weighted sum of its p features.
The betas ($\beta_{j}$) represent the learned feature weights or coefficients.
The first weight in the sum ($\beta_0$) is called the intercept and is not multiplied with a feature.
The epsilon ($\epsilon$) is the error we still make, i.e. the difference between the prediction and the actual outcome.
These errors are assumed to follow a Gaussian distribution, which means that we make errors in both negative and positive directions and make many small errors and few large errors. -->
予測結果は p 個の特徴量の重み付き和です。
$\beta_{j}$ は学習された特徴の重み、つまり係数を表します。
1つ目の重み ($\beta_0$) は切片と呼ばれ、特徴量と乗算はしません。
$\epsilon$ は予測と実際の結果との差、つまり誤差です。
これらの誤差値はガウス分布に従うと仮定されます。誤差が正の方向にも負の方向にも存在して、小さい誤差は多く、大きい誤差は少ないと仮定するという意味です。

<!-- Various methods can be used to estimate the optimal weight.
The ordinary least squares method is usually used to find the weights that minimize the squared differences between the actual and the estimated outcomes: -->
最適な重みを推定するために様々な方法が用いられます。
実際の結果と推定結果との差の二乗和を最小化するような重みを求めるために、最小二乗法がよく用いられます。

$$\hat{\boldsymbol{\beta}}=\arg\!\min_{\beta_0,\ldots,\beta_p}\sum_{i=1}^n\left(y^{(i)}-\left(\beta_0+\sum_{j=1}^p\beta_jx^{(i)}_{j}\right)\right)^{2}$$

<!-- We will not discuss in detail how the optimal weights can be found, but if you are interested, you can read chapter 3.2 of the book "The Elements of Statistical Learning" (Friedman, Hastie and Tibshirani 2009)[^Hastie] or one of the other online resources on linear regression models. -->
具体的に最適な重みがどう求められるかはここでは論じませんが、興味があれば、"The Elements of Statistical Learning" (Friedman, Hastie and Tibshirani 2009)[^Hastie] の 3.2 章や他の線形回帰に関するオンライン資料を参考にしてください。

<!-- The biggest advantage of linear regression models is linearity:
It makes the estimation procedure simple and, most importantly, these linear equations have an easy to understand interpretation on a modular level (i.e. the weights).
This is one of the main reasons why the linear model and all similar models are so widespread in academic fields such as medicine, sociology, psychology, and many other quantitative research fields.
For example, in the medical field,  it is not only important to predict the clinical outcome of a patient, but also to quantify the influence of the drug and at the same time take sex, age, and other features into account in an interpretable way. -->
線形回帰の最大の利点は線形性です。
線形性により予測手順が簡単になります。最も重要なことは、これらの一次方程式がモジュールレベル（つまり重み）で理解しやすくなります。
これが、線形モデルおよび同様のモデルが、医学、社会学、心理学の学術分野および他の多くの定量的研究分野で広く普及している主な理由の 1つです。
例えば、医療分野では、患者の臨床成果の予測だけでなく、薬の影響を定量化し、同時に性別、年齢、その他の特徴を解釈可能な方法で考慮することが重要です。

<!--
Estimated weights come with confidence intervals.
A confidence interval is a range for the weight estimate that covers the "true" weight with a certain confidence.
For example, a 95% confidence interval for a weight of 2 could range from 1 to 3.
The interpretation of this interval would be:
If we repeated the estimation 100 times with newly sampled data, the confidence interval would include the true weight in 95 out of 100 cases, given that the linear regression model is the correct model for the data. -->
推定される重みには信頼区間があります。
信頼区間とは、ある信頼度で「真」の重みを含むような重み推定の範囲です。
例えば、重み2に対する信頼度 95％ の信頼区間は1から3の範囲になる可能性があります。
この信頼区間は、「回帰モデルが与えられるデータに正しいという仮定のもとで、新しくサンプリングされたデータに対して推定を 100 回繰り返したときに、この信頼区間は 100 回中 95 回真の重みを含む」と解釈できます。

<!-- Whether the model is the "correct" model depends on whether the relationships in the data meet certain assumptions, which are linearity, normality, homoscedasticity, independence, fixed features, and absence of multicollinearity. -->
モデルが「正しい」かはデータ中の関係が、線形性、正規性、等分散性、独立性、固定された特徴量、また、多重共線性の欠如 というこれらの仮定を満たしているかどうかによります。

<!-- **Linearity** 
The linear regression model forces the prediction to be a linear combination of features, which is both its greatest strength and its greatest limitation.
Linearity leads to interpretable models.
Linear effects are easy to quantify and describe.
They are additive, so it is easy to separate the effects.
If you suspect feature interactions or a nonlinear association of a feature with the target value, you can add interaction terms or use regression splines.
-->
**線形性**
線形回帰モデルは予測が特徴量の線形結合になることを要請しますが、これは線形回帰の最大の強みでもあり、最大の制約でもあります。
線形性は解釈可能なモデルに繋がります。
線形効果は定量化しやすく、説明しやすいです。
線形的に足されるため、効果を分離することも簡単です。
特徴量の相互作用や、特徴量と目的値の間の非線形性が疑われる場合は、相互作用項を追加するか、スプライン回帰を使うことができます。

<!-- **Normality**  
It is assumed that the target outcome given the features follows a normal distribution.
If this assumption is violated, the estimated confidence intervals of the feature weights are invalid. -->
**正規性**
与えられた特徴量に対する目的値は正規分布に従うと仮定されます。
もしこの仮定が破れると、特徴量に対する重みの推定信頼区間は意味を持たなくなります。

<!-- **Homoscedasticity** (constant variance)  
The variance of the error terms is assumed to be constant over the entire feature space.
Suppose you want to predict the value of a house given the living area in square meters.
You estimate a linear model that assumes that, regardless of the size of the house, the error around the predicted response has the same variance.
This assumption is often violated in reality.
In the house example, it is plausible that the variance of error terms around the predicted price is higher for larger houses, since prices are higher and there is more room for price fluctuations.
Suppose the average error (difference between predicted and actual price) in your linear regression model is 50,000 Euros.
If you assume homoscedasticity, you assume that the average error of 50,000 is the same for houses that cost 1 million and for houses that cost only 40,000.
This is unreasonable because it would mean that we can expect negative house prices. -->
**等分散性**
誤差項の分散は、特徴空間全体において一定であると想定されます。
平方メートルで与えられる居住面積に基づいて住宅の価格を予測するとします。
家の大きさに関わらず、予測値における誤差の分散が一定であると想定する線形モデルを作ります。
この仮定は、実際には現実に反します。
住宅の例でいうと、家の価値が高いほど価格変動の余地が大きく、価格の予測値における誤差項の分散が大きくなるかもしれません。
線形回帰モデルの平均誤差（予測価格と実際の価格との差）が 50,000 ユーロであるとしましょう。
等分散性を想定すると、平均 50,000 の誤差が 100 万ユーロの家と 4 万ユーロの家とに対して同じであるとすることになります。
これは家の価格がマイナスになりうるということになり、合理的ではありません。

<!-- **Independence**  
It is assumed that each instance is independent of any other instance.
If you perform repeated measurements, such as multiple blood tests per patient, the data points are not independent.
For dependent data you need special linear regression models, such as mixed effect models or GEEs.
If you use the "normal" linear regression model, you might draw wrong conclusions from the model. -->
**独立性**
各データは互いに独立であることが想定されます。
患者ごとに血液検査を複数回行うなど、繰り返して測定する場合、データ点は互いに独立になりません。
従属なデータに対しては混合効果モデルや GEE などの特殊な線形回帰モデルが必要です。
このような場合に、「標準的な」線形回帰モデルを使ってしまうと、モデルから誤った結論を導き出してしまう恐れがあります。

<!-- **Fixed features**  
The input features are considered "fixed".
Fixed means that they are treated as "given constants" and not as statistical variables.
This implies that they are free of measurement errors.
This is a rather unrealistic assumption.
Without that assumption, however, you would have to fit very complex measurement error models that account for the measurement errors of your input features.
And usually you do not want to do that. -->
**固定された特徴量**
入力特徴量は「固定されている」と見なします。
固定されているとは、入力特徴量が統計的な変数でなく、「与えられた定数」として扱われるということです。
これは測定誤差がないことを意味します。
これは、かなり非現実的な仮定です。
しかし、この仮定なしでは、入力特徴の測定誤差を説明するために非常に複雑な測定誤差モデルを使う必要があります。
通常、そんなことはしたくないでしょう。

<!--
**Absence of multicollinearity**  
You do not want strongly correlated features, because this messes up the estimation of the weights.
In a situation where two features are strongly correlated, it becomes problematic to estimate the weights because the feature effects are additive and it becomes indeterminable to which of the correlated features to attribute the effects.
-->
**多重共線性の欠如**
強い相関関係にある特徴量は、重みの推定を台無しにするので望ましくありません。
2つの特徴量が強く相関している場合、特徴量の効果が相加的であり、相関する特徴量のどちらが影響を与えているのか不定となるため、重みを推定するときに確定できません。

<!--
### Interpretation
-->
### 解釈

<!--
The interpretation of a weight in the linear regression model depends on the type of the corresponding feature.
-->
線形回帰モデルの重みの解釈は、対応する特徴量のタイプに依存します。

<!--
- Numerical feature: Increasing the numerical feature by one unit changes the estimated outcome by its weight. 
An example of a numerical feature is the size of a house.
- Binary feature: A feature that takes one of two possible values for each instance.
An example is the feature "House comes with a garden".
One of the values counts as the reference category (in some programming languages encoded with 0), such as "No garden".
Changing the feature from the reference category to the other category changes the estimated outcome by the feature's weight.
- Categorical feature with multiple categories:
A feature with a fixed number of possible values.
An example is the feature "floor type", with possible categories "carpet", "laminate" and "parquet".
A solution to deal with many categories is the one-hot-encoding, meaning that each category has its own binary column.
For a categorical feature with L categories, you only need L-1 columns, because the L-th column would have redundant information (e.g. when columns 1 to L-1 all have value 0 for one instance, we know that the categorical feature of this instance takes on category L).
The interpretation for each category is then the same as the interpretation for binary features.
Some languages, such as R, allow you to encode categorical features in various ways, as [described later in this chapter](#cat-code).
- Intercept $\beta_0$: 
The intercept is the feature weight for the "constant feature", which is always 1 for all instances.
Most software packages automatically add this "1"-feature to estimate the intercept.
The interpretation is:
For an instance with all numerical feature values at zero and the categorical feature values at the reference categories, the model prediction is the intercept weight.
The interpretation of the intercept is usually not relevant because instances with all features values at zero often make no sense.
The interpretation is only meaningful when the features have been standardised (mean of zero, standard deviation of one).
Then the intercept reflects the predicted outcome of an instance where all features are at their mean value.
The interpretation of the features in the linear regression model can be automated by using following text templates.
-->
- 量的特徴量 (Numerical feature): 量的特徴量の値が1単位分増えると、推定結果が重み分変化します。量的特徴量の一例としては、家の大きさが挙げられます。
- バイナリ特徴量 (Binary feature): それぞれのインスタンスごとに、2つの値のどちらか一方のみを取る特徴量のことです。
そのような特徴量の一例として、庭付きの家であるかどうかが挙げられます。値のうちの1つは"庭付きでない"のような参照カテゴリ（プログラミング言語によっては 0 と符号化されます）として数えられます。ある特徴量の参照カテゴリが別のカテゴリに変化すると、推定結果は特徴量の重み分変化します。
- 複数のカテゴリを含んだカテゴリカル特徴量: 取りうる値の数が固定された特徴量のことです。一例として、"カーペット"、"ラミネート"、"寄木細工"というカテゴリを取りうる”床の種類”という特徴量が挙げられます。
多くのカテゴリを扱う解決策の1つとして、one-hot エンコーディングというものがあります。one-hot エンコーディングとは、それぞれのカテゴリごとに 0, 1 の値を取るカラムを設定するエンコーディングの手法です。
L 個のカテゴリがあるカテゴリカル特徴量に対しては、L-1 個のカラムがあれば十分です。なぜならば、L 番目のカラムは冗長な情報だからです（例えば、ある1つのインスタンスに対して 1 番目から L-1 番目のカラムが全て 0 であるとき、そのインスタンスのカテゴリは L 番目のものであることが分かるからです）。
各カテゴリに対する解釈はバイナリ特徴量に対する解釈と同じです。
R のような言語では、[この章で後ほど説明するような](#cat-code)様々な方法でカテゴリカル特徴量をエンコードすることが出来ます。
- 切片$\beta_0$:
切片は、全てのインスタンスについて常に1をとる”定数特徴量”に対する重みであるとみなすことが出来ます。
大抵のソフトウェアパッケージは切片を推定するために定数特徴量を自動的に追加します。
これに対する解釈は以下の通りです。
全ての量的特徴量が0であり、カテゴリカルデータの値が参照カテゴリであるインスタンスの場合には、モデルの予測値は切片の重みになります。
そのような、全ての特徴量の値が0であるようなインスタンスは多くの場合意味をなさないため、切片の解釈は通常関係がありません。
切片の解釈は、全ての特徴量が平均0、標準偏差1に標準化されているときにのみ意味があるものとなります。
そのような場合、切片は全ての特徴量が平均値であるようなインスタンスの予測結果を反映しています。
線形回帰モデルの特徴量の解釈は以下のテンプレートを用いて自動化できます。

<!--
**Interpretation of a Numerical Feature**
-->
**量的特徴量の解釈**

<!--
An increase of feature $x_{k}$ by one unit increases the prediction for y by $\beta_k$ units when all other feature values remain fixed.
-->
特徴量 $x_{k}$ が 1 だけ増えて、その他の特徴量は固定されている場合、予測結果 y は $\beta_k$ だけ増えます。

<!--
**Interpretation of a Categorical Feature**
-->
**カテゴリカル特徴量の解釈**

<!--
Changing feature $x_{k}$ from the reference category to the other category increases the prediction for y by $\beta_{k}$ when all other features remain fixed.
-->
特徴量 $x_{k}$ が参照カテゴリから他のカテゴリに変化し、その他の特徴量は固定されている場合、予測結果 y は $\beta_{k}$ だけ増えます。

<!--
Another important measurement for interpreting linear models is the R-squared measurement.
R-squared tells you how much of the total variance of your target outcome is explained by the model.
The higher R-squared, the better your model explains the data.
The formula for calculating R-squared is: 
-->
もう1つの重要な指標は、決定係数 $R^2$ です。
$R^2$ はモデルによって、どの程度、目的値の全てのばらつきが説明されているかを教えてくれます。
$R^2$ が高くなればなるほど、そのモデルはデータを説明していることになります。
$R^2$ の計算方法は以下の通りです。

$$R^2=1-SSE/SST$$

<!--
SSE is the squared sum of the error terms:
-->
SSE は、二乗和誤差 (Squared Sum of the Error)です。

$$SSE=\sum_{i=1}^n(y^{(i)}-\hat{y}^{(i)})^2$$ 

<!--
SST is the squared sum of the data variance:
-->
SST は、データの分散の二乗和です。

$$SST=\sum_{i=1}^n(y^{(i)}-\bar{y})^2$$

<!--
The SSE tells you how much variance remains after fitting the linear model, which is measured by the squared differences between the predicted and actual target values.
SST is the total variance of the target outcome.
R-squared tells you how much of your variance can be explained by the linear model.
R-squared ranges between 0 for models where the model does not explain the data at all and 1 for models that explain all of the variance in your data.
-->
SSE は線形回帰で学習した後、どの程度のばらつきが残っているかを教えてくれます。これは、予測と実際の目的値の間の二乗誤差を計測することで求めます。
SST はデータ自体の目的値の全体の分散です。
$R^2$ は、線形モデルによって、どの程度ばらつきが説明できているのかを教えてくれます。
$R^2$ は 0 のとき、モデルはデータを全く説明できていないのに対して、1 のときは、データのばらつきを完全に説明できていることを意味します。

<!--
There is a catch, because R-squared increases with the number of features in the model, even if they do not contain any information about the target value at all.
Therefore, it is better to use the adjusted R-squared, which accounts for the number of features used in the model.
Its calculation is:
-->
実は、たとえ目的値に対する情報を全く含んでいない特徴量を付け加えたとしても、モデルの特徴量の数を増やすと $R^2$ の値を増加させることができます。
そのため、モデルで使われている特徴量の数を反映した、自由度調整済みの決定係数 $R^2$ を使用することが推奨されます。
その計算方法は次のようになります。

$$\bar{R}^2=1-(1-R^2)\frac{n-1}{n-p-1}$$

<!--
where p is the number of features and n the number of instances.
-->
ただし、 p は特徴量の数で、n はインスタンスの数です。

<!--
It is not meaningful to interpret a model with very low (adjusted) R-squared, because such a model basically does not explain much of the variance.
Any interpretation of the weights would not be meaningful.
-->
(自由度調整済み)決定係数 $R^2$ がとても低いモデルに対しては、そもそもモデルがデータをうまく説明できていないため、そのモデルの解釈をしても意味がありません。
どのように重みの解釈しても、意味がないでしょう。

<!--
**Feature Importance**
-->
**特徴量重要度 (Feature Importance)**

<!--
The importance of a feature in a linear regression model can be measured by the absolute value of its t-statistic.
The t-statistic is the estimated weight scaled with its standard error.
-->
線形回帰モデルの特徴量の重要度は t 統計量の絶対値で計測できます。
t 統計量は標準誤差でスケーリングされた推定重みです。

$$t_{\hat{\beta}_j}=\frac{\hat{\beta}_j}{SE(\hat{\beta}_j)}$$

<!--
Let us examine what this formula tells us:
The importance of a feature increases with increasing weight.
This makes sense.
The more variance the estimated weight has (= the less certain we are about the correct value), the less important the feature is.
This also makes sense.
-->
この式は何を意味しているのでしょうか。
特徴量の重要度は、重みが増えると増加します。
これは理にかなっています。
推定された重みに、よりばらつきがあると (正しい値に関しての確証が小さくなる)、その特徴量に対する重要度は小さくなります。
これも理にかなっています。

<!--
###  Example
-->
### 例

<!-- In this example, we use the linear regression model to predict the [number of rented bikes](#bike-data) on a particular day, given weather and calendar information. -->
この例では、線形回帰モデルを使って、天気や日付情報が与えられたときの[自転車レンタル台数](#bike-data) の予測をします。

<!-- For the interpretation, we examine the estimated regression weights.
The features consist of numerical and categorical features.
For each feature, the table shows the estimated weight, the standard error of the estimate (SE), and the absolute value of the t-statistic (|t|).-->
解釈のために、回帰の重みについて調べます。
特徴量は、量的特徴量、カテゴリカル特徴量のどちらもがあります。
各特徴に対して、推定された重み、推定値の標準誤差 (SE)、及び t 統計量の絶対値(|t|)を表に示します。


```{r linear_model}

data(bike)
X = bike[bike.features.of.interest]
y = bike[,'cnt']
dat = cbind(X, y)

mod = lm(y ~ ., data = dat, x = TRUE)
lm_summary = summary(mod)$coefficients

lm_summary_print = lm_summary
lm_summary_print[,'t value'] = abs(lm_summary_print[,'t value'])
rownames(lm_summary_print) = pretty_rownames(rownames(lm_summary_print))

kable(lm_summary_print[,c('Estimate', 'Std. Error', 't value')], digits = 1, col.names = c('Weight', 'SE', "|t|"))
```

<!-- Interpretation of a numerical feature (temperature):
An increase of the temperature by 1 degree Celsius increases the predicted number of bicycles by `r sprintf('%.1f', lm_summary_print['temp', 'Estimate'])`, when all other features remain fixed.-->
量的特徴量の解釈について (temperature):
摂氏が1度上昇したとき、他の特徴量は全て同じとすると、自転車のレンタル台数の予測は `r sprintf('%.1f', lm_summary_print['temp', 'Estimate'])` だけ増加します。

<!-- Interpretation of a categorical feature ("weathersit"):
The estimated number of bicycles is `r sprintf('%.1f', lm_summary_print['weathersitRAIN/SNOW/STORM', 'Estimate'])` lower when it is raining, snowing or stormy, compared to good weather -- again assuming that all other features do not change.
When the weather is misty, the predicted number of bicycles is `r sprintf('%.1f', lm_summary_print['weathersitMISTY', 'Estimate'])` lower compared to good weather, given all other features remain the same.-->
カテゴリカル特徴量の解釈について (weathersit):
自転車のレンタル数の予測値は、天気の良い日と比べて、雨や雪や嵐のとき `r sprintf('%.1f',lm_summary_print['weathersitRAIN/SNOW/STORM', 'Estimate'])` だけ変化します。ただし、このときも他の特徴量は変化しないことを想定しています。
晴れた日に比べて、霧が濃い時は、自転車のレンタル台数の予測値は `r sprintf('%.1f', lm_summary_print['weathersitMISTY', 'Estimate'])` だけ変化します。

<!--
All the interpretations always come with the footnote that "all other features remain the same".
This is because of the nature of linear regression models.
The predicted target is a linear combination of the weighted features.
The estimated linear equation is a hyperplane in the feature/target space (a simple line in the case of a single feature).
The weights specify the slope (gradient) of the hyperplane in each direction.
The good side is that the additivity isolates the interpretation of an individual feature effect from all other features.
That is possible because all the feature effects (= weight times feature value) in the equation are combined with a plus. 
On the bad side of things, the interpretation ignores the joint distribution of the features.
Increasing one feature, but not changing another, can lead to unrealistic or at least unlikely data points.
For example increasing the number of rooms might be unrealistic without also increasing the size of a house.
-->
全ての解釈では必ず、"他の特徴量は変化させない"という脚注がついています。
これは、線形回帰モデルの性質です。
予測値は、重み付けされた特徴量の線形和です。
推定された線形方程式は特徴量/予測値空間の超平面です (単一の特徴量の場合単純な直線になります)。
重みは各方向の超平面の傾き(勾配)を指定しています。
良い点としては、加法性によって個々の特徴量の効果を他の全ての特徴量から分離できることです。
これが可能なのは、全ての特徴量の効果(重みと特徴量の積)が和によって組み合わされているからなのです。
悪い点としては、この解釈は特徴量の同時分布を無視していることです。
ある特徴量を増やすが、他の特徴量は変化させないとき、非現実的か好ましくないデータが得られるかもしれません。
例えば、家の大きさを広くすることなく、部屋の数を増加させることは現実的ではないでしょう。

<!-- ### Visual Interpretation -->
### 可視化による解釈

<!--
Various visualizations make the linear regression model easy and quick to grasp for humans.! -->
種々の可視化手法を用いることで、線形回帰モデルを簡単かつ素早く把握できます。

<!-- #### Weight Plot -->
#### 重みプロット (Weight Plot)

<!--
The information of the weight table (weight and variance estimates) can be visualized in a weight plot.
The following plot shows the results from the previous linear regression model. 
-->
重みの情報（重みと分散の推定値）は重みプロットを用いることで簡単に可視化できます。
以下のプロットでは、先ほどの線形回帰の結果を可視化しています。

<!--
fig.cap="Weights are displayed as points and the 95% confidence intervals as lines."
-->
```{r linear-weights-plot, fig.cap="重みが点として、95%信頼区間が線として表示されている。"}
coef_plot(mod) + scale_y_discrete("")
```

<!--The weight plot shows that rainy/snowy/stormy weather has a strong negative effect on the predicted number of bikes.
The weight of the working day feature is close to zero and zero is included in the 95% interval, which means that the effect is not statistically significant.
Some confidence intervals are very short and the estimates are close to zero, yet the feature effects were statistically significant.
Temperature is one such candidate.
The problem with the weight plot is that the features are measured on different scales.
While for the weather the estimated weight reflects the difference between good and rainy/stormy/snowy weather, for temperature it only reflects an increase of 1 degree Celsius.
You can make the estimated weights more comparable by scaling the features (zero mean and standard deviation of one) before fitting the linear model.-->
重みプロットは、自転車のレンタル台数を予測するにあたって、雨、雪、嵐といった天気は大きな負の影響を及ぼすことを示しています。
稼働日数の重みはほぼ 0 になっており、かつ 95% 信頼区間に 0 は含まれています。従って、この特徴量は統計的に重要でないことが示されます。
気温についても同様のことが言える可能性があります。
重みプロットにおける問題は、各特徴量が異なるスケールで計測されているということです。
天気の推定された重みは晴れ、雨、嵐、雪のそれぞれの天気の違いを反映していますが、気温は 1 度の変化による影響を反映しています。
特徴をスケーリングすることで、線形回帰モデルを学習させる前に、推定された重みを比較できるようになります。（例えば標準化など）

<!-- #### Effect Plot -->
#### 影響力プロット (Effect Plot)

<!--
The weights of the linear regression model can be more meaningfully analyzed when they are multiplied by the actual feature values.
The weights depend on the scale of the features and will be different if you have a feature that measures e.g. a person's height and you switch from meter to centimeter.
The weight will change, but the actual effects in your data will not.
It is also important to know the distribution of your feature in the data, because if you have a very low variance, it means that almost all instances have similar contribution from this feature.
The effect plot can help you understand how much the combination of weight and feature contributes to the predictions in your data.
Start by calculating the effects, which is the weight per feature times the feature value of an instance:
-->
線形回帰モデルの重みは特徴量と掛け合わせることで、より意味のある分析になり得ます。重みは特徴量のスケールに依存し、例えば身長の単位をメートルからセンチメートルに変更すればまた異なる重みになります。
ただ、重みは変化したとしても、重みが実際にデータに及ぼす影響は変化しません。
また特徴量の分布を知ることは重要です。なぜなら、ある特徴量の分散が非常に低い場合、ほとんどのインスタンスがその特徴量から同じような影響を受けることを意味するからです。
影響力プロットは、重みと特徴の組み合わせがどれほど予測に貢献しているかを理解する助けになります。
影響力を計算するのは、各特徴ごとの重みと、特徴量をインスタンスごとに掛け合わせます。


$$\text{effect}_{j}^{(i)}=w_{j}x_{j}^{(i)}$$

<!--
The effects can be visualized with boxplots.
A box in a boxplot contains the effect range for half of your data (25% to 75% effect quantiles).
The vertical line in the box is the median effect, i.e. 50% of the instances have a lower and the other half a higher effect on the prediction.
The horizontal lines extend to $\pm1.5\text{IQR}/\sqrt{n}$, with IQR being the inter quartile range (75% quantile minus 25% quantile).
The dots are outliers.
The categorical feature effects can be summarized in a single boxplot, compared to the weight plot, where each category has its own row.
-->
影響力は箱ひげ図を用いて可視化できます。
箱ひげ図内の各箱は、全データのうち半分のデータに対する影響力の範囲を表します（第1四分位点から第3四分位点まで）。箱内の縦線は中央値を表します。つまり、予測時に半数のインスタンスはこれよりも低い影響力をもち、もう半数は高い影響力を持つということです。
横線は $\pm1.5\text{IQR}/\sqrt{n}$ まで引かれていて、IQR は四分位範囲（inter quartile range）（第3四分位数から第1四分数を引いたもの）のことです。

<!-- fig.cap="The feature effect plot shows the distribution of effects (= feature value times feature weight) across the data per feature." -->
```{r linear-effects, fig.cap="特徴量の影響力プロットは、特徴量ごとのデータに対する影響 (特徴量と重みの積) の分布を示している。"}
effect_plot(mod, dat) + scale_x_discrete("")
```

<!--
The largest contributions to the expected number of rented bicycles comes from the temperature feature and the days feature, which captures the trend of bike rentals over time.
The temperature has a broad range of how much it contributes to the prediction.
The day trend feature goes from zero to large positive contributions, because the first day in the dataset (01.01.2011) has a very small trend effect and the estimated weight for this feature is positive (`r sprintf('%.2f', lm_summary_print['days_since_2011', 'Estimate'])`).
This means that the effect increases with each day and is highest for the last day in the dataset (31.12.2012).
Note that for effects with a negative weight, the instances with a positive effect are those that have a negative feature value.
For example, days with a high negative effect of windspeed are the ones with high wind speeds.
-->
図から、自転車のレンタル台数の予測に大きく寄与している特徴量は、気温と日数の特徴であることがわかります。この特徴量は、自転車レンタルのトレンドを捉えていると言えます。
気温が予測にどれだけ寄与しているかの範囲はかなり広くなっています。日数の特徴量は 0 から大きな正の値に渡って寄与していることがわかります。これは、データセット内での初日（2011/1/1）ではトレンドの影響は非常に小さく、かつ推測された重みは正の値 (`r sprintf('%.2f', lm_summary_print['days_since_2011', 'Estimate'])`) だったからです。これが意味するのは、影響力は毎日上昇し続け、データセット内の最終日（2013/12/31）に最大値を迎えるということです。
重みが負の値のとき、あるインスタンスが正の影響力を持っていたとすると、特徴量が負であるということに注意してください。
例えば、風速が大きく負の影響力を持っているような日は、風速がかなり強い日であるということです。

<!--
### Explain Individual Predictions
-->
### 個々の予測に対する説明

```{r linear-effects-single-preparation}
i = 6
effects = get_effects(mod, dat)
predictions = predict(mod)

effects_i = tidyr::gather(effects[i, ])
predictions_mean = mean(predictions)
# For proper indexing, names have to be removed
names(predictions) = NULL
pred_i = predictions[i]
```

<!--
How much has each feature of an instance contributed to the prediction?
This can be answered by computing the effects for this instance.
An interpretation of instance-specific effects only makes sense in comparison to the distribution of the effect for each feature.
We want to explain the prediction of the linear model for the `r i`-th instance from the bicycle dataset.
The instance has the following feature values.
-->
あるインスタンスの各特徴量はどれだけ予測に貢献したのでしょうか。
この疑問への回答はインスタンスに対する効果を計算することで得られます。
インスタンスに固有の効果の解釈は各特徴量に関する効果の分布を比較することでのみ意味を持ちます。 自転車データセットの`r i`番目のインスタンスに対する線形モデルの予測について説明します。 このインスタンスは以下の特徴量をもっています。

```{r linear-effects-single-table}
df = data.frame(feature = colnames(bike), value = t(bike[i,]))
colnames(df) = c("feature", "value")
kable(df, col.names = c("Feature", "Value"), row.names = FALSE)
```

<!--
To obtain the feature effects of this instance, we have to multiply its feature values by the corresponding weights from the linear regression model.
For the value "`r df["workingday", "value"]`" of feature "`r df["workingday", "feature"]`", the effect is, `r round(lm_summary_print[paste(df["workingday", "feature"], df["workingday", "value"], sep = ""), "Estimate"], 1)`.
For a temperature of `r round(as.numeric(as.character(df["temp", "value"])), 1)` degrees Celsius, the effect is `r round(as.numeric(as.character(df["temp", "value"])) * lm_summary_print[as.character(df["temp", "feature"]), "Estimate"], 1)`.
We add these individual effects as crosses to the effect plot, which shows us the distribution of the effects in the data.
This allows us to compare the individual effects with the distribution of effects in the data. 
-->
このインスタンスの特徴量の効果を知るため、その特徴量と、それに対応する線形回帰モデルの重みの積を求めなくてはなりません。
特徴量 "`r df["workingday", "feature"]`" の値 "`r df["workingday", "value"]`" に対する効果は `r round(lm_summary_print[paste(df["workingday", "feature"], df["workingday", "value"], sep = ""), "Estimate"], 1)` となります。
気温 `r round(as.numeric(as.character(df["temp", "value"])), 1)` 度の効果は `r round(as.numeric(as.character(df["temp", "value"])) * lm_summary_print[as.character(df["temp", "feature"]), "Estimate"], 1)` です。
これらの個々の効果をデータ全体への効果の分布を示す図にX印としてプロットします。
これにより個々の効果とデータ全体の効果が比較できます。

<!--
fig.cap="The effect plot for one instance shows the effect distribution and highlights the effects of the instance of interest.
-->
```{r linear-effects-single, fig.cap="1つのインスタンスに対する影響力プロットは、影響力の分布を示し、興味のあるインスタンスの効果のハイライトする。"}
i = 6
effects = get_effects(mod, dat)
predictions = predict(mod)

effects_i = tidyr::gather(effects[i, ])
predictions_mean = mean(predictions)
# For proper indexing, names have to be removed
names(predictions) = NULL
pred_i = predictions[i]

effect_plot(mod, dat) +
  geom_point(aes(x=key, y=value), color = 'red', data = effects_i, shape = 4, size=4) +
  scale_x_discrete("") +
  ggtitle(sprintf('Predicted value for instance: %.0f\nAverage predicted value: %.0f\nActual value: %.0f', pred_i, predictions_mean, y[i]))
```

<!--
If we average the predictions for the training data instances, we get an average of `r round(predictions_mean, 0)`。
In comparison, the prediction of the `r i`-th instance is small, since only `r round(pred_i, 0)` bicycle rents are predicted。
The effect plot reveals the reason why。
The boxplots show the distributions of the effects for all instances of the dataset, the crosses show the effects for the `r i`-th instance.
The `r i`-th instance has a low temperature effect because on this day the temperature was `r round(X[i, 'temp'],0)` degrees, which is low compared to most other days (and remember that the weight of the temperature feature is positive)。
Also, the effect of the trend feature "days_since_2011" is small compared to the other data instances because this instance is from early 2011 (`r  X[i, 'days_since_2011']` days) and the trend feature also has a positive weight。
-->
訓練データのインスタンスに対する予測を平均すると、`r round(predictions_mean, 0)` の平均となります.
それと比較すると、`r i` 番目のインスタンスによる自転車の台数予測は `r round(pred_i, 0)` しかないことから、小さいといえます。
影響力プロットはその理由を明らかにしています。
箱ひげ図はデータセットにおける全インスタンスの効果分布を表し、X印は `r i` 番目のインスタンスの効果を示しています。
`r i` 番目のインスタンスは、この日の気温が 2 度であり、その他のほとんどの日と比べて気温が低いことから、気温による影響は小さいと言えます ( 気温に対する重みは正であることに注意)。
また、トレンド特徴量である "days_since_2011" の効果も、このインスタンスが2011初頭(`r  X[i, 'days_since_2011']` days) のものであることと、トレンドに対する重みが正であることから、影響力は小さいと言えます。


<!-- ### Encoding of Categorical Features {#cat-code} -->
### カテゴリカル特徴量のエンコーディング

<!-- There are several ways to encode a categorical feature, and the choice influences the interpretation of the weights.-->
カテゴリカル特徴量をエンコーディングする方法は複数あり、選択した方法によって重みの解釈に影響が生じます。

<!-- The standard in linear regression models is treatment coding, which is sufficient in most cases.
Using different encodings boils down to creating different (design) matrices from a single column with the categorical feature.-->
線形回帰モデルにおいて標準となるのは treatment coding で、これは多くの場合で十分に機能します。
異なるエンコーディングの手法は1つのカテゴリカル特徴量から異なる計画行列を作り出すことに相当します。

<!-- This section presents three different encodings, but there are many more.
The example used has six instances and a categorical feature with three categories.
For the first two instances, the feature takes category A;
for instances three and four, category B;
and for the last two instances, category C.-->
この節では3つの異なるエンコーディングの方法について紹介しますが、他にも多くの手法が知られています。
使用する例では、6つのインスタンスと3つのカテゴリを持つカテゴリカル特徴量を持ちます。
最初の2つのインスタンスでは、特徴量はカテゴリAを取ります。
3番目、4番目のインスタンスでは、カテゴリーBを取り、最後2つのインスタンスはカテゴリーCを取ります。

<!-- **Treatment coding** -->
**Treatment coding**

<!-- In treatment coding, the weight per category is the estimated difference in the prediction between the corresponding category and the reference category.
The intercept of the linear model is the mean of the reference category (when all other features remain the same). -->
treatment coding では、カテゴリごとの重みは、対応するカテゴリと参照カテゴリ間の予測の差の推定値とします。
線形モデルの切片は参照カテゴリの平均値です。(他の全ての特徴量が変わらない場合)

<!-- The first column of the design matrix is the intercept, which is always 1.
Column two indicates whether instance i is in category B, column three indicates whether it is in category C. -->
計画行列の最初の列は切片で常に 1 となります。
2 列目はインスタンス i がカテゴリ B かどうかを示しており、3 列目はインスタンスがカテゴリ C かどうかを示しています。


<!-- There is no need for a column for category A, because then the linear equation would be overspecified and no unique solution for the weights can be found.
It is sufficient to know that an instance is neither in category B or C. -->
線形方程式が過剰になり、重みに対して一意な解を見つけることができなくなるため、カテゴリ A に対する列は必要ありません。
インスタンスがカテゴリ B にも C にも属していないことがわかれば十分なのです。

<!-- Feature matrix: -->
Feature matrix:

$$\begin{pmatrix}1&0&0\\1&0&0\\1&1&0\\1&1&0\\1&0&1\\1&0&1\\\end{pmatrix}$$

<!-- **Effect coding** -->
**Effect coding**

<!--
The weight per category is the estimated y-difference from the corresponding category to the overall mean (given all other features are zero or the reference category).
The first column is used to estimate the intercept.
-->
カテゴリごとの重みは、対応するカテゴリと全体の平均 (他のすべての特徴がゼロまたは参照カテゴリである場合) の推定された y の差です。
最初の列は切片を推定するために用いられます。

<!--
The weight $\beta_{0}$ associated with the intercept represents the overall mean and $\beta_{1}$, the weight for column two, is the difference between the overall mean and category B.
The total effect of category B is $\beta_{0}+\beta_{1}$.
-->
切片に関連づけられた重み $\beta_{0}$ は、全体の平均を表し、2 列目に対する重み $\beta_{1}$ は、
全体の平均とカテゴリ B との間の差となります。
したがって、カテゴリ B の全体の効果は $\beta_{0}+\beta_{1}$ となります。

<!--
The interpretation for category C is equivalent.
For the reference category A, $-(\beta_{1}+\beta_{2})$ is the difference to the overall mean and $\beta_{0}-(\beta_{1}+\beta_{2})$ the overall effect.
-->
カテゴリ C に対する解釈も同様です。
参照カテゴリ A に対しては、$-(\beta_{1}+\beta_{2})$が全体の平均を表し、$\beta_{0}-(\beta_{1}+\beta_{2})$が全体の影響となります。

<!-- Feature matrix: -->
Feature matrix:

$$\begin{pmatrix}1&-1&-1\\1&-1&-1\\1&1&0\\1&1&0\\1&0&1\\1&0&1\\\end{pmatrix}$$

<!-- **Dummy coding** -->
**Dummy coding**

<!--
The $\beta$ per category is the estimated mean value of y for each category (given all other feature values are zero or the reference category).
Note that the intercept has been omitted here so that a unique solution can be found for the linear model weights.
-->
カテゴリごとの $\beta$ は各カテゴリに対する推定された y の平均値です。（その他の特徴量はゼロもしくは、参照カテゴリ）
ただし、切片は、線形モデルの重みに対して一意な解が見つけられるように、省略されていることに注意してください。

<!-- Feature matrix: --> 
Feature matrix:

 $$\begin{pmatrix}1&0&0\\1&0&0\\0&1&0\\0&1&0\\0&0&1\\0&0&1\\\end{pmatrix}$$

<!-- If you want to dive a little deeper into the different encodings of categorical features, checkout [this overview webpage](http://stats.idre.ucla.edu/r/library/r-library-contrast-coding-systems-for-categorical-variables/) and
[this blog post](http://heidiseibold.github.io/page7/). -->
カテゴリカル特徴量に対するエンコーディング手法にさらに興味がある場合は次のサイトをご覧ください。
[概要サイト](http://stats.idre.ucla.edu/r/library/r-library-contrast-coding-systems-for-categorical-variables/)、
[ブログ](http://heidiseibold.github.io/page7/).

<!--
### Do Linear Models Create Good Explanations?
-->
### 線形モデルは良い説明を与えるか?

<!--
Judging by the attributes that constitute a good explanation, as presented [in the Human-Friendly Explanations chapter](#good-explanation), linear models do not create the best explanations.
They are contrastive, but the reference instance is a data point where all numerical features are zero and the categorical features are at their reference categories.
This is usually an artificial, meaningless instance that is unlikely to occur in your data or reality.
-->
[人間に優しい説明](#good-explanation)の章で説明した、良い説明とは何かという観点で見ると、線形モデルは最良の説明を与えるというわけではありません。
これらは対照的ですが、参照インスタンスは全ての量的特徴量が0でありカテゴリカル特徴量は参照カテゴリであるようなデータ点となります。
これは大抵、現実的には起こりそうもない人工的で意味のないインスタンスです。

<!--
There is an exception:
If all numerical features are mean centered (feature minus mean of feature) and all categorical features are effect coded, the reference instance is the data point where all the features take on the mean feature value.
This might also be a non-existent data point, but it might at least be more likely or more meaningful.
-->
例外もあります：
すべての量的特徴量が平均化（特徴量から特徴量の平均値を引いた値）されており、すべてのカテゴリ特徴量がエンコードされている場合、参照インスタンスは、すべての特徴量が平均値を取るデータ点となります。
これは実在しないデータ点かもしれませんが、少なくとも可能性が高く意味があるものかもしれません。

<!--
In this case, the weights times the feature values (feature effects) explain the contribution to the predicted outcome contrastive to the "mean-instance".
Another aspect of a good explanation is selectivity, which can be achieved in linear models by using less features or by training sparse linear models.
But by default, linear models do not create selective explanations.
-->
この場合、特徴量に重みを掛けたもの（feature effects）は、"平均的なインスタンス"と比較したときの予測結果への貢献度を説明しています。
良い説明のもう1つの側面は選択性であり、線形モデルにはおいては、より少ない特徴量を使うこと、もしくはスパースな線形モデルを用いることが挙げられます。
線形モデル自体では選択性を持つ説明は達成できないことに注意してください。

<!--
Linear models create truthful explanations, as long as the linear equation is an appropriate model for the relationship between features and outcome.
The more non-linearities and interactions there are, the less accurate the linear model will be and the less truthful the explanations become.
Linearity makes the explanations more general and simpler.
The linear nature of the model, I believe, is the main factor why people use linear models for explaining relationships.
-->
線形モデルは、線形方程式が特徴量と出力結果の関係を表す適切なモデルであるかぎり、正しい説明を与えます。
非線形性や交互作用が多いほど、線形モデルは正確ではなくなり、説明も真実味の欠如が起こります。
線形性はモデルに対する説明を、より一般的に単純にします。
人々が関係性を説明するために線形モデルを使う主な理由は、モデルの線形の性質によるものだと考えられます。


<!--
###  Sparse Linear Models {#sparse-linear}
-->
###  スパースな線形モデル {#sparse-linear}

<!--
The examples of the linear models that I have chosen all look nice and neat, do they not?
But in reality you might not have just a handful of features, but hundreds or thousands.
And your linear regression models?
Interpretability goes downhill.
You might even find yourself in a situation where there are more features than instances, and you cannot fit a standard linear model at all.
The good news is that there are ways to introduce sparsity (= few features) into linear models.
-->
私が選んだ線形モデルの例は、どれも上手くいきましたよね？
しかし実際のデータでは、ほんの一握りの特徴量ではなく、何百、何千の特徴量を持ってるかもしれません。
そのときに、線形回帰モデルはどうなるでしょうか。
解釈性は下がります。
インスタンスよりも特徴量が多く、標準的な線形モデルでは学習ができないという状況に陥ってしまうこともあるかもしれません。
このような時は、線形モデルにスパース性（＝少数の特徴量）を導入することで解決できます。

<!--
#### Lasso {#lasso}
-->
#### Lasso {#lasso}

<!--
Lasso is an automatic and convenient way to introduce sparsity into the linear regression model.
Lasso stands for "least absolute shrinkage and selection operator" and, when applied in a linear regression model, performs feature selection and regularization of the selected feature weights.
Let us consider the minimization problem that the weights optimize:
-->
Lasso は、線形回帰モデルにスパース性を導入するための便利な方法です。
Lasso は「least absolute shrinkage and selection operator」の略で、線形回帰モデルに適用すると、特徴量の選択と選択された特徴量の重みの正則化を行います。
以下の重みを最適化する最小化問題を考えてみましょう。

$$min_{\boldsymbol{\beta}}\left(\frac{1}{n}\sum_{i=1}^n(y^{(i)}-x_i^T\boldsymbol{\beta})^2\right)$$

<!--
Lasso adds a term to this optimization problem.
-->
Lasso は、この最適化問題に新しく項を付け加えます。

$$min_{\boldsymbol{\beta}}\left(\frac{1}{n}\sum_{i=1}^n(y^{(i)}-x_{i}^T\boldsymbol{\beta})^2+\lambda||\boldsymbol{\beta}||_1\right)$$

<!--
The term $||\boldsymbol{\beta}||_1$, the L1-norm of the feature vector,  leads to a penalization of large weights.
Since the L1-norm is used, many of the weights receive an estimate of 0 and the others are shrunk.
The parameter lambda ($\lambda$) controls the strength of the regularizing effect and is usually tuned by cross-validation.
Especially when lambda is large, many weights become 0.
The feature weights can be visualized as a function of the penalty term lambda.
Each feature weight is represented by a curve in the following figure.
-->
$||\boldsymbol{\beta}||_1$ の項は、重みに対する L1 ノルムであり、大きな重みに対するペナルティの役割があります。
L1 ノルムを使用しているため、多くの重みは 0 となり、他の重みは小さくなります。
パラメータ $\lambda$ は正則化効果の強さを制御し、通常はクロスバリデーションによって調整されます。
特に $\lambda$ が大きいと、多くの重みが 0 になります。
特徴量の重みは、ペナルティ項 $\lambda$ の関数として可視化できます。
各特徴量の重みは、次の図のように曲線で表すことができます。

<!--
fig.cap="With increasing penalty of the weights, fewer and fewer features receive a non-zero weight estimate. These curves are also called regularization paths. The number above the plot is the number of non-zero weights."
-->
```{r lasso-path, fig.cap="重みに対するペナルティが大きくなるにつれて、非ゼロの重みを持つ特徴量が少なくなっていきます。これらの曲線は解パス図とも呼ばれます。プロットの上の数字は、非ゼロの重みの数です。"}
library("glmnet")
X.d = model.matrix(y ~ . -1, data = X)
l.mod = glmnet(X.d, y)
plot(l.mod,  xvar = "lambda", ylab="Weights")
```

<!--
What value should we choose for lambda?
If you see the penalization term as a tuning parameter, then you can find the lambda that minimizes the model error with cross-validation.
You can also consider lambda as a parameter to control the interpretability of the model.
The larger the penalization, the fewer features are present in the model (because their weights are zero) and the better the model can be interpreted.
-->
$\lambda$ にはどのような値を選ぶべきでしょうか？
ペナルティ項をチューニングできるパラメータとして捉えれば、クロスバリデーションでモデル誤差を最小化する $\lambda$ を求めることができます。
$\lambda$ をモデルの解釈性を制御するパラメータとして考えることもできます。
ペナルティが大きければ大きいほど、モデルに存在する特徴量が少なくなり（重みがゼロになるので）、モデルの解釈性が良くなります。

<!--
**Example with Lasso**
-->
**Lassoを使用した例**

<!--
We will predict bicycle rentals using Lasso. 
We set the number of features we want to have in the model beforehand.
Let us first set the number to 2 features:
-->
Lasso を使ってレンタル自転車の数を予測してみましょう。
モデルに持たせたい特徴量の数を事前に設定しておきます。
まずは特徴量の数を 2 に設定してみましょう。

```{r lasso_effects}
extract.glmnet.effects = function(betas, best.index) {
  data.frame(beta = betas[, best.index])
}
n.features = apply(l.mod$beta, 2, function(x){sum(x!=0)})
kable(extract.glmnet.effects(l.mod$beta, max(which(n.features == 2))), col.names = "Weight", digits = 2)
```

<!--
The first two features with non-zero weights in the Lasso path are temperature ("temp") and the time trend ("days_since_2011").
-->
Lasso により重みが 0 にならなかったのは2つの特徴量は、温度（"temp"）と時間トレンド（"days_since_2011"）です。

<!--
Now, let us select 5 features:
-->
では、次に、5つの特徴量を選択してみましょう。

```{r lasso_effects2}
kable(extract.glmnet.effects(l.mod$beta, max(which(n.features == 5))), col.names = "Weight", digits = 2)
```

<!--
Note that the weights for "temp" and "days_since_2011" differ from the model with two features.
The reason for this is that by decreasing lambda even features that are already "in" the model are penalized less and may get a larger absolute weight.
The interpretation of the Lasso weights corresponds to the interpretation of the weights in the linear regression model.
You only need to pay attention to whether the features are standardized or not, because this affects the weights.
In this example, the features were standardized by the software, but the weights were automatically transformed back for us to match the original feature scales.
-->
"temp" と "days_since_2011" の重みが、先に示した2つの特徴量を持つモデルとは異なることに注意してください。
この理由は、$\lambda$ を減少させることで、2つの特徴量を持つモデルで選択された特徴量であっても、ペナルティが少なくなり、より大きな重みが得られる可能性があるからです。
Lasso の重みの解釈は、線形回帰モデルの重みの解釈に対応しています。
重みに影響するため、特徴量が標準化されているかどうかに注意を払う必要があります。
この例では、特徴量はソフトウェアによって標準化されていますが、重みは元の特徴量の尺度と一致するように自動的に変換されています。

<!--
**Other methods for sparsity in linear models**
-->
**線形モデルのスパース性のための他の方法**

<!--
A wide spectrum of methods can be used to reduce the number of features in a linear model.
-->
線形モデルの特徴量の数を減らすために、さまざまな手法があります。

<!--
Pre-processing methods:

- Manually selected features:
You can always use expert knowledge to select or discard some features.
The big drawback is that it cannot be automated and you need to have access to someone who understands the data.
- Univariate selection:
An example is the correlation coefficient.
You only consider features that exceed a certain threshold of correlation between the feature and the target.
The disadvantage is that it only considers the features individually.
Some features might not show a correlation until the linear model has accounted for some other features.
Those ones you will miss with univariate selection methods.
-->
**前処理に関する方法**

- 手動による特徴量選択:
専門家の知識・ドメイン知識を使うことで、特徴量の選択ができます。
自動化できないのが大きな欠点で、データを理解している人と協力する必要があります。
- 単変量選択:
例としては、相関係数があります。
特徴量と目的変数の相関関係が一定の閾値を超えた特徴量のみを選択します。
デメリットは、特徴量を単体でしか考えていないことです。
いくつかの特徴量は、線形モデルが他の特徴量を説明するまで相関を示さないかもしれません。
こういった場合、単変量選択法では見逃してしまいます。

<!--
Step-wise methods:

- Forward selection:
Fit the linear model with one feature.
Do this with each feature.
Select the model that works best (e.g. highest R-squared).
Now again, for the remaining features, fit different versions of your model by adding each feature to your current best model.
Select the one that performs best.
Continue until some criterion is reached, such as the maximum number of features in the model.
- Backward selection:
Similar to forward selection.
But instead of adding features, start with the model that contains all features and try out which feature you have to remove to get the highest performance increase.
Repeat this until some stopping criterion is reached.

I recommend using Lasso, because it can be automated, considers all features simultaneously, and can be controlled via lambda.
It also works for the [logistic regression model](#logistic) for classification.
-->
**段階的な方法**

- Forward selection:
1つの特徴量で線形モデルをフィットします。
これを特徴量ごとに行います。
最も性能の良いモデルを選択してください (例: 決定係数 $R^2$ が最大)。
ここでもう一度、残りの特徴量について、現在の最良のモデルに新たな特徴量を1つ追加することで、異なるバージョンのモデルを学習させます。
そして、最も性能の良いモデルを選びます。
この作業をモデル内の特徴量の最大数など、ある基準に達するまで続けましょう。
- Backward selection:
この手法は Forward selection と似ています。
しかし、特徴量を追加するのではなく、すべての特徴量を含むモデルから始めて、どの特徴を削除すれば最高の性能向上が得られるかを試してみましょう。
これを、ある停止基準に達するまで繰り返します。

Lasso を使うことをお勧めする理由は、すべての特徴量を同時に考慮し、$\lambda$ を変更することで制御できるからです。
また、Lassoは、分類のための[ロジスティック回帰モデル](#logistic)でも使用できます。

<!--
### Advantages
-->
### 長所

<!--
The modeling of the predictions as a **weighted sum** makes it transparent how predictions are produced.
And with Lasso we can ensure that the number of features used remains small.

Many people use linear regression models.
This means that in many places it is **accepted** for predictive modeling and doing inference.
There is a **high level of collective experience and expertise**, including teaching materials on linear regression models and   software implementations.
Linear regression can be found in R, Python, Java, Julia, Scala, Javascript, ...

Mathematically, it is straightforward to estimate the weights and you have a **guarantee to find optimal weights** (given all assumptions of the linear regression model are met by the data).

Together with the weights you get confidence intervals, tests, and solid statistical theory. 
There are also many extensions of the linear regression model (see [chapter on GLM, GAM and more](#extend-lm)).
-->
予測値を**重み付き和**としてモデル化することで、予測値がどのように生成されるかの透明性を高くできます。
そして、Lasso を使用することで、使用される特徴量の数を減らすことができます。

多くの人が線形回帰モデルを使います。
これは、多くの場所で、予測モデルや推論実行のために線形モデルが受け入れられていることを意味します。
線形回帰に関して、**高度な経験をもつ専門家**がいたり、教材やソフトウェアなども豊富にあります。
線形回帰はR、Python、Java、Julia、Scala、Javascript、その他多数で使用できます。

数学的には、重みを推定するのは簡単で、最適な重みを見つけることができることが保証されています（線形回帰モデルのすべての仮定がデータによって満たされている場合）。

重みと一緒に、信頼区間、検定、強固な統計理論を得ることができます。
線形回帰モデルの拡張もたくさん知られています（[GLM,GAMなどの章](#extend-lm)を参照）。

<!--
### Disadvantages
-->
### 短所

<!--
Linear regression models can only represent linear relationships, i.e. a weighted sum of the input features.
Each **nonlinearity or interaction has to be hand-crafted** and explicitly given to the model as an input feature.

Linear models are also often **not that good regarding predictive performance**, because the relationships that can be learned are so restricted and usually oversimplify how complex reality is.
-->
線形回帰モデルは、線形関係しか表現できません。
非線形性や交互作用を考慮するには、手作業で新たに特徴量を作成する必要があります。

線形モデルは、学習できる関係が非常に制限されており、実際には複雑な関係を単純化しすぎているため、**予測性能に関してもあまり良くない**ことが多いです。

<!--
The interpretation of a weight **can be unintuitive** because it depends on all other features.
A feature with high positive correlation with the outcome y and another feature might get a negative weight in the linear model, because, given the other correlated feature, it is negatively correlated with y in the high-dimensional space.
Completely correlated features make it even impossible to find a unique solution for the linear equation.
-->
重みの解釈は、他のすべての特徴量に依存しているため、**直感的ではない**場合があります。
出力 y と他の特徴量に対して強い正の相関のある特徴量は、線形モデルにおいては、負の重みとなる可能性があります。なぜなら、他にも相関のある特徴量がある場合、高次元空間において y と負の相関があるためです。
完全に相関のある特徴量がある場合は、線形方程式の一意の解を見つけることが不可能になります。

<!--
An example:
You have a model to predict the value of a house and have features like number of rooms and size of the house.
House size and number of rooms are highly correlated: the bigger a house is, the more rooms it has.
If you take both features into a linear model, it might happen, that the size of the house is the better predictor and gets a large positive weight.
The number of rooms might end up getting a negative weight, because, given that a house has the same size, increasing the number of rooms could make it less valuable or the linear equation becomes less stable, when the correlation is too strong.
-->
例: 
家の価値を予測するモデルがあり、部屋数や広さなどの特徴量があります。
家の大きさと部屋の数は非常に強い相関関係があります。つまり、家が大きければ大きいほど、部屋数が多くなります。
線形モデルに両方の特徴量を使用した場合、家のサイズがより良い予測指標となり、大きな正の重みを取得することが起こるかもしれません。
そうすると、同じ広さの家でも、部屋数を増やすと価値が下がったり、相関関係が強すぎると線形方程式が安定しなくなったりするので、部屋の数に対する重みは負になるかもしれません。