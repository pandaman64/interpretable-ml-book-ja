```{r, message = FALSE, warning = FALSE, echo = FALSE}


devtools::load_all()
set.seed(42)
```

<!--{pagebreak}-->

<!--## Shapley Values {#shapley}-->
## シャープレイ値 (Shapley Values) {#shapley}

<!--
A prediction can be explained by assuming that each feature value of the instance is a "player" in a game where the prediction is the payout.
Shapley values -- a method from coalitional game theory -- tells us how to fairly distribute the "payout" among the features.
-->
予測は、インスタンスの特徴量の値が "プレイヤー"で、予測が報酬であるようなゲームを想定して説明できます。
シャープレイ値（協力ゲーム理論の手法）は、特徴量の間で "報酬" を公平に分配する方法を教えてくれます。

<!--### General Idea-->
### 一般的なアイデア

<!--
Assume the following scenario:
-->
次のシナリオを想定してみましょう。

<!--
You have trained a machine learning model to predict apartment prices.
For a certain apartment it predicts €300,000 and you need to explain this prediction.
The apartment has a size of 50 m^2^, is located on the 2nd floor, has a park nearby and cats are banned:
-->
あなたは、アパートの価格を予測するための機械学習モデルを学習しました。
あるアパートは €300,000 と予測されており、この予測を説明する必要があります。
そのアパートの広さは 50m^2^ で、二階にあり、近くに駐車場があり、猫を飼うことは禁止されています。

<!--
fig.cap = "The predicted price for a 50 m^2^ 2nd floor apartment with a nearby park and cat ban is €300,000. Our goal is to explain how each of these feature values contributed to the prediction."
-->
```{r shapley-instance, fig.cap = "駐車場が近く猫が禁止されている 50m^2^ の二階のアパートの予測価格は €300,000。目的は、これらの特徴量がそれぞれどのように予測値に寄与したのかを説明すること。", out.width=500}
knitr::include_graphics("images/shapley-instance.png")
```

<!--
The average prediction for all apartments is €310,000.
How much has each feature value contributed to the prediction compared to the average prediction?
-->
全アパートの平均予測値は €310,000 です。
平均予測値と比較して、それぞれの特徴量の値は予測にどの程度寄与しているでしょうか。

<!--
The answer is simple for linear regression models.
The effect of each feature is the weight of the feature times the feature value.
This only works because of the linearity of the model.
For more complex models, we need a different solution.
For example, [LIME](#lime) suggests local models to estimate effects.
Another solution comes from cooperative game theory:
The Shapley value, coined by Shapley (1953)[^shapley1953], is a method for assigning payouts to players depending on their contribution to the total payout.
Players cooperate in a coalition and receive a certain profit from this cooperation.
-->
線形回帰モデルの場合、答えは簡単です。
各特徴量の効果は、特徴量の重みと値を掛け合わせたものだからです。
ただしこれは、モデルが線形なので上手くいっているだけです。
より複雑なモデルに対しては、異なる解決策が必要です。
例えば、[LIME](#lime) は、効果を見積もるために局所的なモデルを提案します。
もう1つの解決策は、協力ゲーム理論に由来します。
シャープレイ値は、Shapley (1953)[^shapley1953]によって提案された方法であり、総報酬額に対する貢献度に応じて参加者に支払額を割り当てる方法です。
参加者は、連合とよばれるグループ内で互いに協力し、この協力を通じて一定の利益を受け取ります。

<!--
Players?
Game?
Payout?
What is the connection to machine learning predictions and interpretability?
The "game" is the prediction task for a single instance of the dataset.
The "gain" is the actual prediction for this instance minus the average prediction for all instances.
The "players" are the feature values of the instance that collaborate to receive the gain (= predict a certain value).
In our apartment example, the feature values `park-nearby`, `cat-banned`, `area-50` and `floor-2nd` worked together to achieve the prediction of €300,000.
Our goal is to explain the difference between the actual prediction (€300,000) and the average prediction (€310,000): a difference of -€10,000.
-->
参加者、ゲーム、報酬、これらは、機械学習の予測や解釈可能性に対して、一体何の関係があるのでしょうか。
"ゲーム" とは、データセットの中の1つのインスタンスに対する予測タスクを意味します。
"報酬" は、このインスタンスの実際の予測から全てのインスタンスの予測平均を引いたものです。
"プレイヤー" とは、報酬を受け取るために協力するインスタンスの各特徴量の値です。
アパートの例では、`park-nearby`、`cat-banned`、`area-50` そして `floor-2nd` という特徴量の値が互いに連携することで €300,000 の予測を達成したということになります。
ここで我々の目標は、実際の予測 (€300,000) と平均予測 (€310,000) の違い、つまり -€10,000 の違いを説明することです。

<!--
The answer could be:
The `park-nearby` contributed €30,000; `size-50` contributed €10,000; `floor-2nd` contributed €0; `cat-banned` contributed -€50,000.
The contributions add up to -€10,000, the final prediction minus the average predicted apartment price.
-->

この答えとしては次のようなものが考えられます。
`park-nearby` は €30,000 の寄与、`size-50` は €10,000 の寄与、`floor-2nd` は €0 の寄与、`cat-banned` は -€50,000 の寄与。
寄与の合計は -€10,000 になり、最終予測から平均予測のアパート価格を差し引いたものになります。

<!--**How do we calculate the Shapley value for one feature?**-->
**１つの特徴量のシャープレイ値をどのように計算するのでしょうか**。 

<!--
The Shapley value is the average marginal contribution of a feature value across all possible coalitions.
All clear now?
-->
シャープレイ値は、考えうる全ての組み合わせ（連合）にわたる平均周辺寄与 (average marginal contribution) です。
これで理解できるしょうか。

<!--
In the following figure we evaluate the contribution of the `cat-banned` feature value when it is added to a coalition of `park-nearby` and `size-50`.
We simulate that only `park-nearby`, `cat-banned` and `size-50` are in a coalition by randomly drawing another apartment from the data and using its value for the floor feature.
The value `floor-2nd` was replaced by the randomly drawn `floor-1st`.
Then we predict the price of the apartment with this combination (€310,000).
In a second step, we remove `cat-banned` from the coalition by replacing it with a random value of the cat allowed/banned feature from the randomly drawn apartment.
In the example it was `cat-allowed`, but it could have been `cat-banned` again.
We predict the apartment price for the coalition of `park-nearby` and `size-50` (€320,000).
The contribution of `cat-banned` was €310,000 - €320,000 = -€10.000.
This estimate depends on the values of the randomly drawn apartment that served as a "donor" for the cat and floor feature values.
We will get better estimates if we repeat this sampling step and average the contributions.
-->
次の図では、`cat-banned` を `park-nearby` と `size-50` の連合に追加するときの寄与を評価します。
データから別のアパートをランダム抽出してその値を基準にすることで、`park-nearby`、`cat-banned` および `size-50` のみが連合をなしている場合をシミュレートします。
`floor-2nd` がランダム抽出された `floor-1st` に置き換えられたとします。
そしてこの組み合わせでアパート価格を予測した結果は €310,000 です。
次のステップでは、`cat-banned` を連合から取り除いて、それをランダム抽出したアパートから、猫の禁止/許可という値と置き換えます。
この例では `cat-allowed` ですが、`cat-banned` が再び選ばれるかもしれません。
`park-nearby` と `size-50` の連合のアパート価格を予測すると€320,000です。
その結果、`cat-banned` の寄与は €310,000 - €320,000 = -€10.000 となりました。
この見積は、ランダム抽出されたアパート (猫と階の特徴量の値を提供した "ドナー" ) の価格に依存しています。
そのため、サンプリングを繰り返して、寄与の平均を計算することで、よりよい推定を行ます。

<!--
fig.cap = "One sample repetition to estimate the contribution of `cat-banned` to the prediction when added to the coalition of `park-nearby` and `area-50`."
-->

```{r shapley-instance-intervened, fig.cap = "`park-nearby` と `area-50` の連合に `cat-banned` を追加する場合の予測に対する寄与を推定するための反復", out.width=500}
knitr::include_graphics("images/shapley-instance-intervention.png")
```

<!--
We repeat this computation for all possible coalitions.
The Shapley value is the average of all the marginal contributions to all possible coalitions.
The computation time increases exponentially with the number of features.
One solution to keep the computation time manageable is to compute contributions for only a few samples of the possible coalitions.
-->
全ての可能な連合に対して、この計算を繰り返します。
シャープレイ値は考えうる全ての連合に対する全ての周辺寄与の平均です。
計算時間は、特徴量の数とともに指数関数的に増加します。
計算時間を現実的にするための1つの解決策は、連合の少数のサンプルから寄与を計算することです。

<!--
The following figure shows all coalitions of feature values that are needed to determine the Shapley value for `cat-banned`.
The first row shows the coalition without any feature values.
The second, third and fourth rows show different coalitions with increasing coalition size, separated by "|".
All in all, the following coalitions are possible:
-->
次の図は、`cat-banned` のシャープレイ値を決定するために必要となる特徴量の全ての連合を示しています。
最初の行は、特徴量の値がない連合です。
2行目、3行目、4行目は、"|" で区切られた、サイズが増加した様々な連合です。
全体としては、以下の連合が可能です。

- `No feature values`
- `park-nearby`
- `size-50`
- `floor-2nd`
- `park-nearby`+`size-50`
- `park-nearby`+`floor-2nd`
- `size-50`+`floor-2nd`
- `park-nearby`+`size-50`+`floor-2nd`.

<!--
For each of these coalitions we compute the predicted apartment price with and without the feature value `cat-banned` and take the difference to get the marginal contribution.
The Shapley value is the (weighted) average of marginal contributions.
We replace the feature values of features that are not in a coalition with random feature values from the apartment dataset to get a prediction from the machine learning model.
-->
これらの連合のそれぞれについて、`cat-banned` という特徴量の値がある場合とない場合のアパート価格を予測して、その差を計算することで周辺寄与を求めます。
シャープレイ値は、周辺寄与の（加重）平均です。
連合に入っていない特徴量の値を、アパートデータセットから取得したランダムな特徴量の値と置き換えて、機械学習モデルで予測値を計算します。

<!--
fig.cap = "All 8 coalitions needed for computing the exact Shapley value of the `cat-banned` feature value."
-->

```{r shapley-coalitions, fig.cap = "`cat-banned` の正確なシャープレイ値を計算するのに必要な8つの連合すべて。", out.width=500}
knitr::include_graphics("images/shapley-coalitions.png")
```

<!--
If we estimate the Shapley values for all feature values, we get the complete distribution of the prediction (minus the average) among the feature values.
-->
全ての特徴量値に対してシャープレイ値を推定すると、特徴量間の予測の完全な分布（平均を差し引いたもの）が得られます。

<!--### Examples and Interpretation-->
### 例と解釈

<!--
The interpretation of the Shapley value for feature value j is:
The value of the j-th feature contributed $\phi_j$ to the prediction of this particular instance compared to the average prediction for the dataset.
--->

特徴量 j のシャープレイ値の解釈は次のようになります。
j 番目の特徴量の値は、データセットの平均予測と比較して、この特定のインスタンスの予測に $\phi_j$ 寄与しました。

<!--
The Shapley value works for both classification (if we are dealing with probabilities) and regression.
-->
シャープレイ値は分類（確率を扱う場合）と回帰の両方に有効です。

<!--
We use the Shapley value to analyze the predictions of a random forest model predicting [cervical cancer](#cervical):
-->
シャープレイ値を使ってランダムフォレストモデルを使用した[子宮頸がん](#cervical)の予測を分析します。

```{r shapley-cervical-prepare}
data("cervical")
library("caret")
library("iml")


ntree = 30
cervical.x = cervical[names(cervical) != 'Biopsy']

model <- caret::train(cervical.x,
               cervical$Biopsy,
               method = 'rf', ntree=ntree, maximise = FALSE)
predictor = Predictor$new(model, class = "Cancer", data = cervical.x, type = "prob")

instance_indices = 326
x.interest = cervical.x[instance_indices,]

avg.prediction = mean(predict(model, type = 'prob')[,'Cancer'])
actual.prediction = predict(model, newdata = x.interest, type = 'prob')['Cancer']
diff.prediction = actual.prediction - avg.prediction
```

<!--
fig.cap = sprintf("Shapley values for a woman in the cervical cancer dataset. With a prediction of %.2f, this woman's cancer probability is %.2f above the average prediction of %.2f. The number of diagnosed STDs increased the probability the most. The sum of contributions yields the difference between actual and average prediction (%.2f)."
-->

```{r shapley-cervical-plot, fig.cap = sprintf("子宮頚がんデータセットのシャープレイ値。%.2fの予測では、この女性の癌の確率が平均予測%.2fより、%.2fだけ高いことを示している。STDsと診断された回数が多いほど確率がもっとも増加する。寄与度の合計は実際の予測と平均予測の差 (%.2f) を示す。", actual.prediction,avg.prediction, diff.prediction, diff.prediction)}
shapley2 = Shapley$new(predictor, x.interest = x.interest, sample.size = 100)
plot(shapley2) +
  scale_y_continuous("Feature value contribution") +
  ggtitle(sprintf("Actual prediction: %.2f\nAverage prediction: %.2f\nDifference: %.2f", actual.prediction, avg.prediction, diff.prediction)) +
 scale_x_discrete("")
```

<!--
For the [bike rental dataset](#bike-data), we also train a random forest to predict the number of rented bikes for a day, given weather and calendar information.
The explanations created for the  random forest prediction of a particular day:
-->

[自転車レンタルデータセット](#bike-data)に対しても、与えられた天気やカレンダー情報から一日のレンタル数を予測するランダムフォレストを学習させます。
そして、特定の日のランダムフォレストの予測を説明します。

```{r shapley-bike-prepare}
data("bike")
ntree = 30
bike.train.x = bike[names(bike) != 'cnt']

model <- caret::train(bike.train.x,
               bike$cnt,
               method = 'rf', ntree=ntree, maximise = FALSE)
predictor = Predictor$new(model, data = bike.train.x)

instance_indices = c(295, 285)

avg.prediction = mean(predict(model))
actual.prediction = predict(model, newdata = bike.train.x[instance_indices[2],])
diff.prediction = actual.prediction - avg.prediction
x.interest = bike.train.x[instance_indices[2],]
```

<!--
fig.cap = sprintf("Shapley values for day %i. With a predicted %.0f rental bikes, this day is %.0f below the average prediction of %.0f. The weather situation and humidity had the largest negative contributions. The temperature on this day had a positive contribution. The sum of Shapley values yields the difference of actual and average prediction (%.0f).", instance_indices[2], actual.prediction, diff.prediction, avg.prediction, diff.prediction)
-->

```{r shapley-bike-plot, fig.cap = sprintf("%i日目のシャープレイ値。レンタル自転車の予測は %.0f となり、この日は平均予測 %.0f より %.0f だけ低い。天候と湿度が最も大きい負の寄与となり、気温は正の寄与があった。シャープレイ値の合計は実際の予測値と予測平均の差 (%.0f) になっている。", instance_indices[2], actual.prediction, avg.prediction, diff.prediction, diff.prediction)}
shapley2 = Shapley$new(predictor, x.interest = x.interest)
plot(shapley2) +  scale_y_continuous("Feature value contribution") +
  ggtitle(sprintf("Actual prediction: %.0f\nAverage prediction: %.0f\nDifference: %.0f", actual.prediction, avg.prediction, diff.prediction))  +
 scale_x_discrete("")
```

<!--
Be careful to interpret the Shapley value correctly:
The Shapley value is the average contribution of a feature value to the prediction in different coalitions.
The Shapley value is NOT the difference in prediction when we would remove the feature from the model.
-->
シャープレイ値を正しく解釈するために、注意すべきことがあります。
シャープレイ値は異なる連合の中で、特徴量の値が予測に与える平均寄与であり、モデルから特徴量を削除したときの予測の差ではありません。

<!--### The Shapley Value in Detail-->
### シャープレイ値の詳細

<!--
This section goes deeper into the definition and computation of the Shapley value for the curious reader.
Skip this section and go directly to "Advantages and Disadvantages" if you are not interested in the technical details.
-->
この章では好奇心旺盛な読者のためにシャープレイ値の定義と計算について深掘りします。
もし、技術的な詳細に興味がなければこの章をスキップして、"長所と短所" に飛んでください。

<!--
We are interested in how each feature affects the prediction of a data point.
In a linear model it is easy to calculate the individual effects.
Here is what a linear model prediction looks like for one data instance:
-->
各特徴量がデータ点の予測にどれだけ影響があるかに興味があります。
線形モデルでは各々の効果を計算するのは簡単です。
1つのデータインスタンスに対する線形モデルの予測は次のようになります。

$$\hat{f}(x)=\beta_0+\beta_{1}x_{1}+\ldots+\beta_{p}x_{p}$$

<!--
where x is the instance for which we want to compute the contributions.
Each $x_j$ is a feature value, with j = 1,...,p.
The $\beta_j$ is the weight corresponding to feature j.
-->

x を寄与を計算したいインスタンスとします。
$x_j$ (j = 1, ..., p) はそれぞれ特徴量 j の値とします。
$\beta_j$ は特徴量 j に対応する重みです。

<!--
The contribution $\phi_j$ of the j-th feature on the prediction $\hat{f}(x)$ is:
-->
j 番目特徴量の予測 $\hat{f}(x)$ に対する寄与 $\phi_j$ は次のようになります。

$$\phi_j(\hat{f})=\beta_{j}x_j-E(\beta_{j}X_{j})=\beta_{j}x_j-\beta_{j}E(X_{j})$$

<!--
where $E(\beta_jX_{j})$ is the mean effect estimate for feature j.
The contribution is the difference between the feature effect minus the average effect.
Nice!
Now we know how much each feature contributed to the prediction.
If we sum all the feature contributions for one instance, the result is the following:
-->

ただし、$E(\beta_jX_{j})$ は特徴量 j の平均効果です。
寄与度は特徴量の効果から平均効果を引いた値です。
すばらしい！
これで各特徴量がどれだけ予測に影響を及ぼしたかが分かります。
1つのインスタンスに対するすべての特徴量の寄与度を合計すると、結果は以下のようになります。

$$\begin{align*}\sum_{j=1}^{p}\phi_j(\hat{f})=&\sum_{j=1}^p(\beta_{j}x_j-E(\beta_{j}X_{j}))\\=&(\beta_0+\sum_{j=1}^p\beta_{j}x_j)-(\beta_0+\sum_{j=1}^{p}E(\beta_{j}X_{j}))\\=&\hat{f}(x)-E(\hat{f}(X))\end{align*}$$

<!--
This is the predicted value for the data point x minus the average predicted value.
Feature contributions can be negative.
-->
これはデータ点 x の予測値から平均予測値を引いたものです。
特徴量の寄与度は負になることもあります。

<!--
Can we do the same for any type of model?
It would be great to have this as a model-agnostic tool.
Since we usually do not have similar weights in other model types, we need a different solution.
-->
どんなモデルに対しても同じようにいくでしょうか？
モデルに依存しないツールとしてこれがあるといいですね。
他のタイプのモデルは同様の重みを持たないことから、異なる解決策が必要です。

<!--
Help comes from unexpected places: cooperative game theory.
The Shapley value is a solution for computing feature contributions for single predictions for any machine learning model.
-->
助けは意外なところからやってきます。
それこそが、協力ゲーム理論に由来するシャープレイ値です。
シャープレイ値は任意の機械学習モデルの単一の予測に対する特徴量の寄与度を計算する方法です。

<!--#### The Shapley Value-->
#### シャープレイ値

<!--
The Shapley value is defined via a value function val of players in S.
-->
シャープレイ値は S の中のプレイヤーの値の関数 val と定義されます。

<!--
The Shapley value of a feature value is its contribution to the payout, weighted and summed over all possible feature value combinations:
-->

特徴量の値のシャープレイ値は報酬への寄与度であり、特徴量の値の全ての組み合わせに対して、加重和をとったのものです。

$$\phi_j(val)=\sum_{S\subseteq\{x_{1},\ldots,x_{p}\}\setminus\{x_j\}}\frac{|S|!\left(p-|S|-1\right)!}{p!}\left(val\left(S\cup\{x_j\}\right)-val(S)\right)$$

<!--
where S is a subset of the features used in the model, x is the vector of feature values of the instance to be explained and p the number of features.
$val_x(S)$ is the prediction for feature values in set S that are marginalized over features that are not included in set S:
-->
ただし、S はモデルで使用されている特徴量の部分集合、x は説明されるインスタンスの特徴量ベクトル、p は特徴量の数を表します。
$val_x(S)$ は集合 S に含まれていない特徴量で周辺化された集合 S の特徴量の値に対する予測です。

$$val_{x}(S)=\int\hat{f}(x_{1},\ldots,x_{p})d\mathbb{P}_{x\notin{}S}-E_X(\hat{f}(X))$$

<!--
You actually perform multiple integrations for each feature that is not contained S.
A concrete example:
The machine learning model works with 4 features x1, x2, x3 and x4 and we evaluate the prediction for the coalition S consisting of feature values x1 and x3:
-->
実際には、S に含まれていないそれぞれの特徴量に対して多重積分します。
具体例として、4つの特徴量 x1、x2、x3、x4 に対する機械学習モデルがあったときに、特徴量 x1 と x3 からなる連合 S の予測値を評価します。

$$val_{x}(S)=val_{x}(\{x_{1},x_{3}\})=\int_{\mathbb{R}}\int_{\mathbb{R}}\hat{f}(x_{1},X_{2},x_{3},X_{4})d\mathbb{P}_{X_2X_4}-E_X(\hat{f}(X))$$

<!--
This looks similar to the feature contributions in the linear model!
-->
これは線形モデルの特徴量の寄与度に似ています。

<!--
Do not get confused by the many uses of the word "value":
The feature value is the numerical or categorical value of a feature and instance;
the Shapley value is the feature contribution to the prediction;
the value function is the payout function for coalitions of players (feature values).
-->
"値" という言葉の多用に惑わされないでください。
特徴量の値は、インスタンスや特徴の数値(numerical or categorical)です。
シャープレイ値は予測に対する特徴量の寄与度です。
値関数 (value function) は、プレイヤー（特徴量の値）の連合に対する報酬関数です。

<!--
The Shapley value is the only attribution method that satisfies the properties **Efficiency**, **Symmetry**, **Dummy** and **Additivity**,  which together can be considered a definition of a fair payout.
-->
シャープレイ値は**効率性**、**対称性**、**ダミー**、**加法性**を満たす唯一の方法で、これらを組み合わせることにより、公平な報酬の定義を考えることができます。

<!--
**Efficiency**
The feature contributions must add up to the difference of prediction for x and the average.
-->
**効率性**: 特徴量の寄与度の合計は x の予測と平均との差である必要があります。

$$\sum\nolimits_{j=1}^p\phi_j=\hat{f}(x)-E_X(\hat{f}(X))$$

<!--
**Symmetry**
The contributions of two feature values j and k should be the same if they contribute equally to all possible coalitions.
If

$$val(S\cup\{x_j\})=val(S\cup\{x_k\})$$

for all

$$S\subseteq\{x_{1},\ldots,x_{p}\}\setminus\{x_j,x_k\}$$

then

$$\phi_j=\phi_{k}$$
-->

**対称性**: 2つの特徴量の値 j と k の寄与度は、全ての連合に等しく寄与する場合、同じである必要があります。  
すなわち、全ての $$S\subseteq\{x_{1},\ldots,x_{p}\}\setminus\{x_j,x_k\}$$ について、$$val(S\cup\{x_j\})=val(S\cup\{x_k\})$$ ならば、 $$\phi_j=\phi_{k}$$ が成立します。

<!--
**Dummy**
A feature j that does not change the predicted value -- regardless of which coalition of feature values it is added to -- should have a Shapley value of 0.
If

$$val(S\cup\{x_j\})=val(S)$$

for all

$$S\subseteq\{x_{1},\ldots,x_{p}\}$$

then

$$\phi_j=0$$
-->

**ダミー**: 予測値に影響のない特徴量 j は、追加される連合に関係なく、シャープレイ値が 0 になる必要があります。
すなわち、すべての $$S\subseteq\{x_{1},\ldots,x_{p}\}$$ について $$val(S\cup\{x_j\})=val(S)$$ ならば、$$\phi_j=0$$ が成立します。

<!--
**Additivity**
For a game with combined payouts val+val^+^ the respective Shapley values are as follows:
-->
**加法性**: 結合した報酬 val+val^+^ をもつゲームに対して、それぞれのシャープレイ値は次のようになります。

$$\phi_j+\phi_j^{+}$$

<!--
Suppose you trained a random forest, which means that the prediction is an average of many decision trees.
The Additivity property guarantees that for a feature value, you can calculate the Shapley value for each tree individually, average them, and get the Shapley value for the feature value for the random forest.
-->
ランダムフォレストを学習したとすると、予測は多くの決定木の平均となります。
加法性は各々の決定木のシャープレイ値を計算し、平均を取るとランダムフォレスト全体のシャープレイ値を得られることを保証します。

<!--#### Intuition-->
#### 直感

<!--
An intuitive way to understand the Shapley value is the following illustration:
The feature values enter a room in random order.
All feature values in the room participate in the game (= contribute to the prediction).
The Shapley value of a feature value is the average change in the prediction that the coalition already in the room receives when the feature value joins them.
-->
シャープレイ値は次のように直感的に理解できます。
特徴量がランダムな順序で部屋に入ります。
部屋にいる全ての特徴量はゲームに参加します（= 予測に寄与します）。
ある特徴量のシャープレイ値とは、既に部屋にいた特徴量が受ける予測と、その特徴量が加わった時の予測の変化の平均です。

<!--#### Estimating the Shapley Value-->
#### シャープレイ値の推定

<!--
All possible coalitions (sets) of feature values have to be evaluated with and without the j-th feature to calculate the exact Shapley value.
For more than a few features, the exact solution to this problem becomes problematic as the number of possible coalitions exponentially increases as more features are added.
Strumbelj et al. (2014)[^strumbelj2014] propose an approximation with Monte-Carlo sampling:
-->
シャープレイ値を正確に計算するには、特徴量のすべての可能な連合（集合）を $j$ 番目の特徴量がある場合とない場合とで評価する必要があります。
可能な連合の数は特徴量が追加されると指数的に増加するため、この問題の正確な解を得るのは困難になります。
Strumbelj et al. (2014)[^strumbelj2014] はモンテカルロサンプリングによる近似を提案しています。

$$\hat{\phi}_{j}=\frac{1}{M}\sum_{m=1}^M\left(\hat{f}(x^{m}_{+j})-\hat{f}(x^{m}_{-j})\right)$$

<!--
where $\hat{f}(x^{m}_{+j})$ is the prediction for x, but with a random number of feature values replaced by feature values from a random data point z, except for the respective value of feature j.
The x-vector $x^{m}_{-j}$ is almost identical to $x^{m}_{+j}$, but the value $x_j^{m}$ is also taken from the sampled z.
Each of these M new instances is a kind of "Frankenstein Monster" assembled from two instances.
-->
ここで、$\hat{f}(x^{m}_{+j})$ は x に対する予測ですが、$j$ 番目の特徴量の各値を除いて、ランダムな数の特徴量がランダムなデータ点 $z$ からの特徴量に置き換えられています。
ベクトル $x^{m}_{-j}$ はほとんど $x^{m}_{+j}$ と同じですが、$x_j^{m}$ もサンプリングされた $z$ から得られます。
これらの $M$ 個の新しいインスタンスはそれぞれ、2つのインスタンスから組み立てられた一種の「フランケンシュタインの怪物」です。

<!--*Approximate Shapley estimation for single feature value**:-->
**1つの特徴量値に対するシャープレイ値の概算**

<!--
- Output: Shapley value for the value of the j-th feature
- Required: Number of iterations M, instance of interest x, feature index j, data matrix X, and machine learning model f
  - For all m = 1,...,M:
    - Draw random instance z from the data matrix X
    - Choose a random permutation o of the feature values
    - Order instance x: $x_o=(x_{(1)},\ldots,x_{(j)},\ldots,x_{(p)})$
    - Order instance z: $z_o=(z_{(1)},\ldots,z_{(j)},\ldots,z_{(p)})$
    - Construct two new instances
        - With feature j: $x_{+j}=(x_{(1)},\ldots,x_{(j-1)},x_{(j)},z_{(j+1)},\ldots,z_{(p)})$
        - Without feature j: $x_{-j}=(x_{(1)},\ldots,x_{(j-1)},z_{(j)},z_{(j+1)},\ldots,z_{(p)})$
    - Compute marginal contribution: $\phi_j^{m}=\hat{f}(x_{+j})-\hat{f}(x_{-j})$
- Compute Shapley value as the average: $\phi_j(x)=\frac{1}{M}\sum_{m=1}^M\phi_j^{m}$
-->

- 出力: j 番目の特徴量の値に対するシャープレイ値
- 入力: 反復回数 M、関心のあるインスタンス x、特徴量のインデックス j、データ行列 X、および機械学習モデル f
  - すべての m = 1,...,M について
    - データ行列 X からランダムなインスタンス z を抽出する
    - 特徴量のランダムな並べ替え o を選択する
    - o に従ってインスタンス x を並べる: $x_o=(x_{(1)},\ldots,x_{(j)},\ldots,x_{(p)})$
    - o に従ってインスタンス z を並べる: $z_o=(z_{(1)},\ldots,z_{(j)},\ldots,z_{(p)})$
    - 2つの新しいインスタンスを構築する
        - j 番目の特徴量がある場合: $x_{+j}=(x_{(1)},\ldots,x_{(j-1)},x_{(j)},z_{(j+1)},\ldots,z_{(p)})$
        - j 番目の特徴量がない場合: $x_{-j}=(x_{(1)},\ldots,x_{(j-1)},z_{(j)},z_{(j+1)},\ldots,z_{(p)})$
    - 周辺寄与を算出する: $\phi_j^{m}=\hat{f}(x_{+j})-\hat{f}(x_{-j})$
- 平均値としてシャープレイ値を計算する: $\phi_j(x)=\frac{1}{M}\sum_{m=1}^M\phi_j^{m}$

<!--
First, select an instance of interest x, a feature j and the number of iterations M.
For each iteration, a random instance z is selected from the data and a random order of the features is generated.
Two new instances are created by combining values from the instance of interest x and the sample z.
The instance $x_{+j}$ is the instance of interest, but all values in the order before feature j are replaced by feature values from the sample z.
The instance $x_{-j}$ is the same as $x_{+j}$, but in addition has feature j replaced by the value for feature j from the sample z.
The difference in the prediction from the black box is computed:
-->
まず、関心のあるインスタンス x、特徴量 j、反復回数 M を選択します。
繰り返しごとに、データからランダムなインスタンス z を選択し、特徴量のランダムな順序を生成します。
関心のあるインスタンス x とサンプル z の値を組み合わせて、2つの新しいインスタンスが生成されます。
インスタンス $x_{+j}$ が関心のあるインスタンスですが、特徴量 j より前の順序の値はすべてサンプル z からの特徴量で置き換えられます。
インスタンス $x_{-j}$ は $x_{+j}$ と同じですが、さらに特徴量 j の値がサンプル z からの特徴量 j の値で置き換えられています。
そして次のようにブラックボックスからの予測値の差が計算されます。

$$\phi_j^{m}=\hat{f}(x^m_{+j})-\hat{f}(x^m_{-j})$$

<!--
All these differences are averaged and result in:
-->
これらの差をすべて平均すると、シャープレイ値が得られます。

$$\phi_j(x)=\frac{1}{M}\sum_{m=1}^M\phi_j^{m}$$

<!--
Averaging implicitly weighs samples by the probability distribution of X.
-->
平均することによって、暗黙のうちにXの確率分布によってサンプルに重みづけされます。

<!--
The procedure has to be repeated for each of the features to get all Shapley values.
-->
すべての特徴量についてのシャープレイ値を得るため、この手順を各特徴量に対して繰り返す必要があります。

<!--### Advantages-->
### 長所

<!--
The difference between the prediction and the average prediction is **fairly distributed** among the feature values of the instance -- the Efficiency property of Shapley values.
This property distinguishes the Shapley value from other methods such as [LIME](#lime).
LIME does not guarantee that the prediction is fairly distributed among the features.
The Shapley value might be the only method to deliver a full explanation.
In situations where the law requires explainability -- like EU's "right to explanations" -- the Shapley value might be the only legally compliant method, because it is based on a solid theory and distributes the effects fairly.
I am not a lawyer, so this reflects only my intuition about the requirements.
-->
予測と予測の平均の差分は、シャープレイ値の効率性という性質から、インスタンスの特徴量間で**公平に分散**されます。
この性質により、シャープレイ値は [LIME](#lime) などの他の手法と区別されます。
LIME は予測が特徴量間で公平に分散されることを保証しません。
EU の「説明を求める権利」のように、法が説明を求める状況下において、シャープレイ値は確かな理論に基づき、予測を公平に分配しているため、法に準拠した唯一の手法であるかもしれません。
ただし、筆者は法律家ではないので、これは法の要請に対する著者の直感でしかありません。

<!--
The Shapley value allows **contrastive explanations**.
Instead of comparing a prediction to the average prediction of the entire dataset, you could compare it to a subset or even to a single data point.
This contrastiveness is also something that local models like LIME do not have.
-->
シャープレイ値は**対照的な説明**を可能にします。
予測をデータセット全体の予測の平均と比較する代わりに、データの部分集合や単一のデータとも比較できます。
この性質もまた、LIME のような局所モデルにはないものです。

<!--
The Shapley value is the only explanation method with a **solid theory**.
The axioms -- efficiency, symmetry, dummy, additivity -- give the explanation a reasonable foundation.
Methods like LIME assume linear behavior of the machine learning model locally, but there is no theory as to why this should work.
-->
シャープレイ値は**確かな理論**に基づいた唯一の説明手法です。
効率性、対称性、ダミー、加法性の原理により説明に合理的な根拠が与えられます。
LIME のような手法は機械学習モデルが局所的には線形に振る舞うことを仮定しますが、これがうまくいく理由についての理論はないのです。

<!--
It is mind-blowing to **explain a prediction as a game** played by the feature values.
-->
特徴量によってプレイされる**ゲームとして予測を説明する**というのは驚くべきことです。

<!--### Disadvantages-->
### 短所

<!--
The Shapley value requires **a lot of computing time**.
In 99.9% of real-world problems, only the approximate solution is feasible.
An exact computation of the Shapley value is computationally expensive because there are 2^k^ possible coalitions of the feature values and the "absence" of a feature has to be simulated by drawing random instances, which increases the variance for the estimate of the Shapley values estimation.
The exponential number of the coalitions is dealt with by sampling coalitions and limiting the number of iterations M.
Decreasing M reduces computation time, but increases the variance of the Shapley value.
There is no good rule of thumb for the number of iterations M.
M should be large enough to accurately estimate the Shapley values, but small enough to complete the computation in a reasonable time.
It should be possible to choose M based on Chernoff bounds, but I have not seen any paper on doing this for Shapley values for machine learning predictions.
-->
シャープレイ値は、**多くの計算時間** を必要とします。
実世界の問題の99.9%では、近似解しか求めることができません。
シャープレイ値の厳密な計算は、特徴量の 2^k^ の可能な連合があり、特徴量の "不在" はランダムなインスタンスを描くことでシミュレートしなければならず、シャープレイ値の推定値の分散が大きくなるため、計算量が多くなります。
連合の指数関数的な数は、連合をサンプリングし、反復回数 M を制限することで対処します。
M を減らすと計算時間は短縮されますが、シャープレイ値の分散が大きくなります。
反復回数 M には良い経験則はありません。
Mは、シャープレイ値を正確に推定するのに十分な大きさでなければなりませんが、妥当な時間で計算を完了するのに十分な小ささである必要もあります。
チェルノフ・バウンズに基づいて M を選択できますが、機械学習予測のためのシャープレイ値についてこれを行った論文を見たことはありません。

<!--
The Shapley value **can be misinterpreted**.
The Shapley value of a feature value is not the difference of the predicted value after removing the feature from the model training.
The interpretation of the Shapley value is:
Given the current set of feature values, the contribution of a feature value to the difference between the actual prediction and the mean prediction is the estimated Shapley value.
-->
シャープレイ値は**誤解される可能性**があります。
特徴量のシャープレイ値は、モデル学習から特徴量を除去した後の予測値の差ではありません。
シャープレイ値は、「現在の特徴量の値の集合が与えられたときの、実際の予測値と平均予測値の差に対する特徴量の寄与度」であると解釈してください。

<!--
The Shapley value is the wrong explanation method if you seek sparse explanations (explanations that contain few features).
Explanations created with the Shapley value method **always use all the features**.
Humans prefer selective explanations, such as those produced by LIME.
LIME might be the better choice for explanations lay-persons have to deal with.
Another solution is [SHAP](https://github.com/slundberg/shap) introduced by Lundberg and Lee (2016)[^lundberg2016], which is based on the Shapley value, but can also provide explanations with few features.
-->
スパースな説明（特徴量をほとんど含まない説明）を求める場合、シャープレイ値は不適切な説明方法です。
シャープレイ値は**常にすべての特徴を使う**説明を作成します。
人はLIMEのような選択的な説明を好みます。
一般の人が扱わなければならない説明には、LIMEの方が適しているかもしれません。
もう1つの解決策として、Lundberg and Lee (2016)[^lundberg2016]が紹介している[SHAP](https://github.com/slundberg/shap)があります。
これはシャープレイ値をベースにしていますが、特徴量の少ない説明も可能です。

<!--
The Shapley value returns a simple value per feature, but **no prediction model** like LIME.
This means it cannot be used to make statements about changes in prediction for changes in the input, such as:
"If I were to earn €300 more a year, my credit score would increase by 5 points."
-->
シャープレイ値は特徴量あたりの単純な値を返しますが、LIMEのような**予測モデルはありません**。
つまり、「年収が300ユーロ増えると、私のクレジットスコアは5ポイント上昇します。」というような入力の変化に対する予測の変化についての記述には使えません。

<!--
Another disadvantage is that **you need access to the data** if you want to calculate the Shapley value for a new data instance.
It is not sufficient to access the prediction function because you need the data to replace parts of the instance of interest with values from randomly drawn instances of the data.
This can only be avoided if you can create data instances that look like real data instances but are not actual instances from the training data.
-->
もう1つの欠点は、新しいインスタンスのシャープレイ値を計算したい場合、**データへのアクセスが必要になる**ことです。
対象のインスタンスの一部をランダムに選ばれたインスタンスからの値に置き換えるデータが必要なため、予測関数にアクセスできるだけでは十分ではありません。
これは、実際のインスタンスのように見えるがそうではないデータを、学習データから作成できる場合にのみ回避できます。

<!--
Like many other permutation-based interpretation methods, the Shapley value method suffers from **inclusion of unrealistic data instances** when features are correlated.
To simulate that a feature value is missing from a coalition, we marginalize the feature.
This is achieved by sampling values from the feature's marginal distribution.
This is fine as long as the features are independent.
When features are dependent, then we might sample feature values that do not make sense for this instance.
But we would use those to compute the feature's Shapley value.
One solution might be to permute correlated features together and get one mutual Shapley value for them.
Another adaptation is conditional sampling: Features are sampled conditional on the features that are already in the team.
While conditional sampling fixes the issue of unrealistic data points, a new issue is introduced:
The resulting values are no longer the Shapley values to our game, since they violate the symmetry axiom, as found out by Sundararajan et. al (2019)[^cond1] and further discussed by Janzing et. al (2020)[^cond2].
-->
他の多くの並べ替えに基づく解釈法と同様に、シャープレイ値による手法も、特徴量に相関がある場合、**非現実的なデータインスタンス**が含まれているという問題があります。
連合から特徴値が欠落していることをシミュレートするために、特徴量を周辺化します。
これは、特徴量の周辺分布から値をサンプリングすることで達成されます。
特徴量が独立であれば問題ありませんが、特徴量が依存関係にある場合、このインスタンスに対して意味をなさない特徴量の値をサンプリングするかもしれません。
しかし、特徴量のシャープレイ値を計算するためにそれらの値を利用します。
これは、相関のある特徴量を一緒に変化させて、それらの特徴量のために1つの相互シャープレイ値を得ることで解決できるかもしれません。
もう1つの解決策は，条件付きサンプリングです。
特徴量は、すでに連合にある特徴量に条件をつけてサンプリングされます。
条件付きサンプリングにより、非現実的なデータ点の問題は修正できますが、新たな問題があります。
Sundararajan et.al. (2019)[^cond1]によって発見され、さらにJanzing et.al. (2020)[^cond2]によって議論されているように、結果として得られる値は対称性の公理に反しているので、我々のゲームにとってのシャープレイ値ではありません。

<!--### Software and Alternatives-->
### ソフトウェアと代替手法

<!--
Shapley values are implemented in both the `iml` and [fastshap](https://github.com/bgreenwell/fastshap) packages for R.
-->

R では、シャープレイ値は `iml` と [fastshap](https://github.com/bgreenwell/fastshap) のパッケージで実装されています。

<!--
SHAP, an alternative estimation method for Shapley values, is presented in the [next chapter](#shap).
-->
シャープレイ値の代替推定手法である SHAP は[次の章](#shap)で紹介されています。

<!--
Another approach is called breakDown, which is implemented in the `breakDown` R package[^breakdown].
BreakDown also shows the contributions of each feature to the prediction, but computes them step by step.
Let us reuse the game analogy:
We start with an empty team, add the feature value that would contribute the most to the prediction and iterate until all feature values are added.
How much each feature value contributes depends on the respective feature values that are already in the "team", which is the big drawback of the breakDown method.
It is faster than the Shapley value method, and for models without interactions, the results are the same.
-->

他には、breakDown と呼ばれる手法があり、Rのパッケージ `breakDown`[^breakdown] で実装されています。
breakDown もまた、各特徴量の予測への寄与を示しますが、breakDown ではそれを段階的に計算します。
ゲームの例えを再び用いましょう。
空のチームからスタートし、予測に最も寄与している特徴量を追加し、これを全ての特徴量が追加されるまで繰り返します。
各特徴量がどのくらい予測に貢献するのかは、すでに「チーム」に属しているそれぞれの特徴量に依存しています。
これが breakDown メソッドの大きな欠点です。
breakDown はシャープレイ値法よりも高速であり、相互作用のないモデルの場合には同じ結果を与えます。

[^shapley1953]: Shapley, Lloyd S. "A value for n-person games." Contributions to the Theory of Games 2.28 (1953): 307-317.

[^strumbelj2014]: Štrumbelj, Erik, and Igor Kononenko. "Explaining prediction models and individual predictions with feature contributions." Knowledge and information systems 41.3 (2014): 647-665.

[^breakdown]: Staniak, Mateusz, and Przemyslaw Biecek. "Explanations of model predictions with live and breakDown packages." arXiv preprint arXiv:1804.01955 (2018).

[^lundberg2016]: Lundberg, Scott M., and Su-In Lee. "A unified approach to interpreting model predictions." Advances in Neural Information Processing Systems. 2017.


[^cond1]: Sundararajan, Mukund, and Amir Najmi. "The many Shapley values for model explanation." arXiv preprint arXiv:1908.08474 (2019).

[^cond2]: Janzing, Dominik, Lenon Minorics, and Patrick Blöbaum. "Feature relevance quantification in explainable AI: A causal problem." International Conference on Artificial Intelligence and Statistics. PMLR, 2020.
