[["index.html", "Interpretable Machine Learning A Guide for Making Black Box Models Explainable. 要約", " Interpretable Machine Learning A Guide for Making Black Box Models Explainable. Christoph Molnar 2021-05-12 要約 機械学習は、製品や処理、研究を改善するための大きな可能性を秘めています。 しかし、コンピュータは通常、予測の説明をしません。これが機械学習を採用する障壁となっています。 本書は、機械学習モデルや、その判断を解釈可能なものにすることについて書かれています。 解釈可能性とは何かを説明した後、決定木、決定規則、線形回帰などの単純で解釈可能なモデルについて学びます。 その後の章では、特徴量の重要度 (feature importance)やALE(accumulated local effects)や、個々の予測を説明するLIMEやシャープレイ値のようなモデルに非依存な手法(model-agnostic methods)について焦点を当てていきます。 すべての解釈手法は、深く説明され、批判的に議論されます。 それらの手法はどのように機能しているのか？ それらの長所と短所は何か？ それらの出力はどのように解釈できるのか？ この本を読めば、機械学習プロジェクトに最も適した解釈手法を選択し、正しく適用できるようになるでしょう。 本書では、テーブルデータ（リレーショナルデータや構造化データとも呼ばれる）の機械学習モデルに焦点を当てており、コンピュータビジョンや自然言語処理のタスクにはあまり焦点を当てていません。 機械学習の実務家、データサイエンティスト、統計学者、その他、機械学習モデルを解釈可能なものとすることに興味のある人は、本書を読むことをお勧めします。 PDFと電子書籍版（epub、mobi）の購入はこちら on leanpub.com. 印刷版の購入はこちら on lulu.com. 著者について: 私の名前はChristoph Molnar、統計学者であり機械学習者です。 私の目標は、機械学習を解釈可能なものにすることです。 Mail：christoph.molnar.ai@gmail.com Website: https://christophm.github.io/ Follow me on Twitter! @ChristophMolnar Cover by @YvonneDoinel 翻訳者について: 株式会社HACARUSの有志のデータサイエンティスト、インターン学生が主導となって翻訳の取り組みが始まりました。 ただし、本書の内容に関して、所属団体とは一切関係無いことをご了承お願いします。 本書は以下のリポジトリで管理されておりますので、誤字や誤訳を発見された方は Issue や Pull Request よりご報告いただけると幸いです。詳しくはリポジトリ内の developer_guide.md をご覧ください。 https://github.com/hacarus/interpretable-ml-book-ja クリエイティブ・コモンズ・ライセンス この本のライセンスは以下の通りです。 Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. "],["著者による序文.html", "著者による序文", " 著者による序文 本書は、私が臨床研究で統計学者として働いていたときに、サイドプロジェクトとして始めたものです。 週に4日働き、「休みの日」にはサイドプロジェクトに取り組んでいました。 最終的に解釈可能な機械学習は私のサイドプロジェクトの1つになりました。 最初は本を書くつもりはありませんでした。 その代わり、私は単に解釈可能な機械学習についてもっと理解したいと思っていて、学ぶための良いリソースを探していました。 機械学習の成功と解釈可能性の重要性を考えると、このトピックに関する書籍やチュートリアルはたくさんあるだろうと予想していました。 しかし、私が見つけたのは関連する研究論文と、インターネット上に散らばっているわずかなブログ記事だけで、概要がよくわかるものはありませんでした。 本、チュートリアル、概要の論文は何もありませんでした。 このギャップに触発されて、この本を書き始めました。 私は結局、解釈可能な機械学習の研究を始めたときに利用できることを願っていた本を書くことになりました。 私がこの本を書いた意図は2つあります。それは自分自身のために学ぶためと、この新しい知識を他の人と共有するためです。 私はドイツのミュンヘン大学（LMU Munich）で統計学の学士号と修士号を取得しました。 機械学習に関する私の知識のほとんどは、オンラインコース、コンペティション、サイドプロジェクト、専門的な活動を通じて独学で習得しました。 私の統計学のバックグラウンドは、機械学習、特に解釈可能性の分野に入るための優れた基礎となりました。 統計学では、解釈可能な回帰モデルを構築することに主に焦点を当てています。 統計学の修士号を取得した後、私は博士号を取得しないことにしました。 修士論文を書くのが好きではなかったからです。 書くことがものすごくストレスだったのです。 そこで私は、Fintechのスタートアップ企業でデータサイエンティストとして、また臨床研究の統計学者として仕事をしました。 この3年間の業界での仕事の後、私はこの本を書き始め、数ヶ月後には解釈可能な機械学習の博士号を取得しました。 この本を書き始めることで、書くことの楽しさを取り戻し、研究への情熱をもつことができるようになりました。 本書は、解釈可能な機械学習の多くの手法を網羅しています。 最初の章では、解釈可能性の概念を紹介し、なぜ解釈可能性が必要なのかを動機付けています。 ショートストーリーもあります! この本では、説明の性質の違いや、人間が考える良い説明とは何かを論じています。 そして、本質的に解釈可能な機械学習モデル、例えば回帰モデルや決定木などについて論じます。 この本の主な焦点は、モデル非依存(model-agnostic)な解釈手法です。 モデル非依存な方法とは、どのような機械学習モデルにも適用可能であり、モデルが訓練された後に適用可能です。 モデルの独立は、モデル非依存な手法を非常に柔軟で強力なものにします。 いくつかの手法では、Local interpretable model-agnostic explanations(LIME)やシャープレイ値のように、個々の予測がどのように行われたかを説明します。 他の手法は、データセット全体のモデルの平均的な振る舞いを説明します。 ここでは、partial dependence plot, accumulated local effects, permutation feature importanceをはじめ、その他多くの手法についても学びます。 特別なカテゴリとして、データ点を説明として生成する例示による説明手法があります。 対事実的説明(Counterfactual explanations)、プロトタイプ(prototypes)、influential instances、敵対的サンプル(adversarial examples)が、本書で紹介されている例示による説明手法です。 本書は、解釈可能な機械学習の将来がどのようなものになるかについての考察で締めくくられています。 最初から最後まで読まなくても、行ったり来たりしながら、自分が最も興味を持った手法に集中して読むことができます。 私がお勧めするのは、序章と解釈可能性の章から始めることだけです。 ほとんどの章は似たような構成で、1つの解釈手法に焦点を当てています。 最初の段落でその手法を要約します。 そして、数式に頼らず、その方法を直感的に説明するようにしています。 それから、その手法の理論を見て、その方法がどのように働くのかを深く理解します。 理論には数式が含まれているでしょうから、ここでは惜しむことはありません。 私は、新しい方法は例を使って理解するのが一番だと考えています。 そのため、それぞれの手法を実際のデータに当てはめていきます。 統計学者は非常に批判的な人だと言う人がいます。 私にとっては、各章にそれぞれの解釈法の長所と短所について批判的な議論が含まれているので、その通りだと思います。 本書は手法の宣伝ではありませんが、ある手法があなたの応用に適しているかどうかの判断材料にはなるはずです。 各章の最後のセクションでは、利用可能なソフトウェアの実装が議論されています。 機械学習は、研究や産業界の多くの人々から大きな注目を集めています。 機械学習はメディアで過剰に宣伝されることもありますが、実際には影響力のあるアプリケーションがたくさんあります。 機械学習は、製品、研究、自動化のための強力な技術です。 近年では、機械学習は、例えば、詐欺的な金融取引を検出したり、視聴する映画をおすすめしたり、画像を分類したりするために使用されています。 そこで、機械学習モデルが解釈可能であることは非常に重要です。 解釈可能性は、開発者がモデルをデバッグして改善したり、モデルの信頼性を向上させたり、モデルの予測を正当化し、洞察を得るために役立ちます。 機械学習の解釈可能性の必要性が高まっているのは、機械学習の利用が増えたことによる自然な結果です。 本書は多くの人にとって貴重なリソースとなっています。 講師の方は、生徒に解釈可能な機械学習の概念を紹介するために本書を使えます。 私は、様々な修士課程や博士課程の学生から、この本が論文の出発点であり、最も重要な参考文献であることを教えてくれるメールを受け取りました。 この本は、生態学、金融、心理学などの分野で機械学習を使ってデータを理解しようとする応用研究者に役立ってきました。 産業界のデータサイエンティストは、『解釈可能な機械学習』の本を仕事に使っていて、同僚にも勧めていると話してくれました。 多くの人がこの本の恩恵を受け、モデル解釈の専門家になれることを嬉しく思います。 機械学習モデルをより解釈しやすくするための技術の概要を知りたい実務家の方にお勧めしたい一冊です。 また、このトピックに興味を持っている学生や研究者（他の誰にでも）にもお勧めします。 本書の恩恵を受けるためには、機械学習の基本的な理解をすでに持っている必要があります。 加えて、本書の理論と公式を理解できるように、大学入学レベルの数学的素養を持っている必要があります。 しかし、各章の冒頭にある手法の直感的な説明は数学なしで理解できるはずです。 ぜひ本書をお楽しみください。 "],["intro.html", "Chapter 1 イントロダクション", " Chapter 1 イントロダクション 本書は（教師あり）機械学習モデルを解釈可能にする手法について解説しています。 文中に数式が出てくることもありますが、数式を理解できなくても手法の裏にある考え方を理解できるでしょう。 本書は機械学習を一から学ぼうとしている人のための本ではありません。 機械学習に初めて触れる人には、基礎を学ぶための教材が多くあります。 書籍では &quot;The Elements of Statistical Learning&quot; by Hastie, Tibshirani, and Friedman (2009) 1、オンライン学習プラットフォームなら coursera.com の Andrew Ng's &quot;Machine Learning&quot; online course がお勧めです。 これらの教材は無料で利用できます。 機械学習モデルの解釈性に関する新しい手法がとてつもない速さで次々に公開されています。 公開された全ての手法についていくのは狂気の沙汰ですし単純に不可能でしょう。 従って、本書では斬新で風変わりな手法を取り上げることはせず、確立された手法や機械学習の解釈可能性の基本的な考え方について説明しています。 これらの基礎知識は機械学習モデルを解釈可能にするための手助けとなります。 本書を読み始めて (若干誇張気味ですが) 5分もあれば、解釈性の基本的な考え方を習得し、arxiv.org に掲載された解釈性に関する論文をよりよく理解し評価できるようになります。 本書は、(ディストピア的な) short stories から始まります。本書を理解するためには必須ではありませんが、あなたを楽しませ、考えさせてくれるでしょう。 次に、解釈可能な機械学習 の概念について紹介します。 どのような時に解釈可能性が重要となるか、どのような説明手法があるかを議論します。 本書で使われている用語は 専門用語 の章で確認できます。 本書で取り上げるモデルや手法の多くは、データセット の章で説明する現実のデータ例を用いて解説します。 機械学習モデルを解釈可能にするひとつの方法は、線形モデルや決定木のような 解釈可能なモデルを用いることです。 もうひとつの選択肢は、どんな教師あり学習モデルにも適用できる モデル非依存の手法 を用いることです。 この章では partial dependence plots と permutation feature importance といった手法を取り上げます。 モデル非依存の手法は、機械学習モデルの入力を変えた時に予測結果がどのように変化するかを計算します。 例示による説明 の章では、モデルの説明としてデータインスタンスを出力するモデル非依存な手法について解説します。 モデル非依存な手法は、全インスタンスに渡るモデルの大域的な挙動について説明するか、個々の予測について説明するかによって、分類できます。 モデルの大域的な挙動について説明する手法には Partial Dependence Plots, Accumulated Local Effects, Feature Interaction, Feature Importance, Global Surrogate Models そして Prototypes and Criticisms のようなものがあります。 モデルの個々の予測については、 Local Surrogate Models, Shapley Value Explanations, Counterfactual Explanations といった手法があり 、関連する Adversarial Examples についても解説します。 モデルの大域的な挙動と個々の推論の両面を説明できる手法として Individual Conditional Expectation や Influential Instances のようなものもあります。 最後に、解釈可能な機械学習の未来の章で機械学習の将来について楽観的な見通しを示します。 本書は最初から順番に読んでも良いし、興味のある手法のところだけ読んでも良いように書かれています。 楽しんで読んでいただけると幸いです。 Friedman, Jerome, Trevor Hastie, and Robert Tibshirani. &quot;The elements of statistical learning&quot;. www.web.stanford.edu/~hastie/ElemStatLearn/ (2009).↩ "],["storytime.html", "1.1 物語の時間", " 1.1 物語の時間 まずは短い物語をいくつか紹介しましょう。 それぞれの物語は解釈可能な機械学習のためにいささか誇張されたものです。 もし急いでいるなら、これらの物語は読み飛ばしても大丈夫です。 もし楽しみたいとか、やる気を出したい（ときに失くしたい）ならば、ぜひ読んでみてください！ 話の構成は Jack Clark の Import AI Newsletter に掲載されている技術小話から影響を受けています。 もしこれらの物語が気に入って、AI に興味を持ったならば、そのニュースレターに登録しておくことをオススメします。 稲妻は二度と打たない 2030年：スイスの医療ラボ 「絶対に、医療ミスなんかじゃありませんでしたよ！」トムは起こった悲劇の中から少しでもマシなことがないか探すかのように言った。 彼は患者の静脈からポンプを引き抜いた。 「彼は医療ミスのせいで亡くなったんじゃないわ」レナが言った。 「この狂ったモルヒネポンプのせいですよ！ 余計な仕事を増やしやがって！」トムはポンプの裏板のネジを外しながら不平を言った。 ネジをすべて外し終えると、彼はその板を持ち上げて脇に置いた。 彼はケーブルを診断装置に繋いだ。 「仕事自体に文句を言ったわけじゃないわよね？」レナは彼にからかうような笑みを向けた。 「違いますよ。えぇ違いますとも！」彼は皮肉っぽい低い声で言い返した。 彼はポンプのコンピュータを起動した。 レナはケーブルのもう一方の端を彼女のタブレットに接続した。 「いいわ。診断装置は動作してる」彼女は言った。 「なにがいけなかったのかすごく興味がある」 「John Doe 氏は完全に御陀仏ですね。 この高濃度のモルヒネです。 なんてこった。なんというか……こんなこと初めてです。そうですよね？ 普通、壊れたポンプというのはほんのちょっとしか供給しないか、何もしないものです。 決して、いや起こったけど、こんな狂った量の投与はしません」トムは説明した。 「ええ、言わなくてもわかってるわ……ねぇ、これを見て」レナは彼女のタブレットを持ち上げた。 「ここにピークがあるのが見える？これが鎮痛剤を打った効能よ。 見て！このラインは基準値を示してる。 このかわいそうな男性は、彼を17回以上殺せる量の鎮痛剤を血管系に注入されたの。 この私たちのポンプによって。 それに……」彼女は画面をスワイプした。「ここに患者が亡くなった瞬間が記録されてる」 「それで、何が起こったかわかりそうですか、ボス？」トムは彼の上司に尋ねた。 「うーん……センサーは正常なようね。 脈拍、血中酸素濃度、血糖値……データは正常に記録されている。 血中酸素濃度のデータにいくらか欠損値が見受けられるけど、めずらしいことじゃない。 ここを見て。 センサーはモルヒネ誘導体やその他の鎮痛剤によって引き起こされる脈拍の低下とコルチゾール濃度の極端な低下を検知してる」 彼女は診断レポートを次々とスワイプした。 トムは食い入るように画面をじっと見つめた。 それは彼にとって初めての機材故障の調査だった。 「オーケー。これがパズルの最初のピースね。 システムは病院の通信チャンネルに警告を送信するのに失敗している。 警告は発動したのに、プロトコルレベルで拒否された。 それは私たちの落ち度かもしれないけど、病院側の落ち度でもありうるわ。 そのログをITチームに送って」レナはトムに言った。 トムは画面をじっと見つめたまま頷いた。 レナは続けた。 「奇妙ね。 その警告を受けてポンプのシャットダウンも実行されるはずだった。 でも明らかにシャットダウンに失敗している。 きっとバグに違いないわ。 品質管理チームが見逃した。 それもめちゃくちゃ酷いやつ。 多分、プロトコルの問題に関係あるわね」 「それで、ポンプの緊急停止システムがどうにかして故障したのはいいとして、なぜポンプは John Doe 氏にこんなにたくさんの鎮痛剤を注入したのですか？」トムは疑問に思った。 「いい質問ね。 あなたの言う通りよ。 プロトコルの不慮の失敗はさておき、そんなに大量の薬物を投与すべきではまったくなかった。 コルチゾール濃度の低下や他の警告を受けて、アルゴリズムはもっと早くに自分自身で停止するはずだった」レナは説明した。 「雷に打たれるような、万に1つの不運だったかもしれない、ということですか？」トムは彼女に尋ねた。 「いいえ、トム。 もし私が送ったドキュメントに目を通していたなら、このポンプが、センサーからの入力に基づいて完璧な量の鎮痛剤を注入できるよう、最初は動物実験、次に人間で訓練されたことを知っていたかもしれないわね。 ポンプのアルゴリズムは不透明で複雑かもしれないけど、ランダムではない。 それはつまり、同じ状況に遭遇したらポンプは再びまったく同じように動作するということ。 私たちの患者がまた死ぬかもしれない。 センサーからの入力の組み合わせか、望ましくない相互作用かが、ポンプの誤作動を引き起こしたに違いないわ。 だからこそ、私たちはもっと深く掘り下げて、ここで何が起こったのかを明らかにする必要があるの」レナは説明した。 「そうですねぇ……」トムは深く考え込みながら応えた。 「どの道、この患者はそう長くなかったんですよね？ガンかなにかで」 レナは分析レポートを読みながら頷いた。 トムは立ち上がって窓に寄った。 彼は外の、遠くの一点を見つめた。 「おそらく、その機械は彼の痛みから解放されたいという願いを聞いてやったんでしょう。 もう苦しまないように。 雷に打たれるようなものだったかもしれませんが、むしろ幸運だったのかも。 宝くじみたいなもので、ランダムじゃない。 なにか意味があったんです。 もし僕がポンプだったら、同じことをしたでしょう」 彼女はようやく顔を上げ、彼を見た。 彼はまだ外のなにかを見ている。 少しの間、二人は何も言わなかった。 レナは再び画面に目を落とし、解析を続けた。 「いいえ、トム。これはバグよ……ただのクソッタレなバグ」 信用失墜 2050年：シンガポールの地下鉄駅で 彼女は地下鉄 Bishan 駅に駆け込んだ。 頭の中ではとっくに仕事を始めていた。 新しいニューラルアーキテクチャのテストはもうできたはずだ。 彼女は政府の納税義務者の脱税を予測するシステムの再設計に着手した。 彼女のチームはみごとな実装を考案していた。 うまくいけば、システムは税務署だけではなく、テロリスト対策用警鐘システムや、営利法人の登記などさまざまな場面に導入されるだろう。 ゆくゆくはそういった予測を市民の信用度計算システムに統合できるかもしれない。 市民の信用度計算システムは文字通り、それぞれの人が信用できるか判定するものだ。 ローンの採否からパスポート発行までにかかる時間まで、人々の生活のあらゆる面に影響を及ぼすだろう。 エスカレーターをおりる途中で、彼女はチームで開発しているシステムを信用度を計算にいかに統合するか考えていた。 彼女はいつも通り、歩みを緩めることなく、RFID 読み取り機に手をかざした。 良い気分だったが、期待と現実の不一致が彼女の中で警鐘を鳴らした。 遅かった。 彼女は地下鉄の改札に向かったが、鼻をぶつけて尻餅をついてこけた。 開くはずのドアは開かなかった。 驚きながら立ち上がり、改札脇のスクリーンに目をやった。 スクリーンには笑顔のキャラクターと「もう一度お試し下さい」の一言。 彼女のことなど気にせず、他の人は読み取り機に手をかざしてすり抜けていく。 ドアは開いて行ってしまった。 また閉じた。 彼女は鼻をこすった。痛むが出血はない。 彼女はもう一度ドアを開けようとしたが、またしても拒否されてしまった。 おかしい。 公共交通機関を利用するための預金が不足しているのかもしれない。 彼女はスマートウォッチで残高を確認してみる。 「ログインに失敗しました。市民相談所にお問い合わせください」と彼女の腕時計に表示されていた。 お腹を殴られでもしたかのような吐き気が彼女を襲った。 何が起きているのか考えた。 試しに「スナイパーギルド」という携帯ゲームを始めた。 すると予想通りにアプリは自動終了してしまった。 眩暈がして彼女は床にへたりこんだ。 この状況を説明できる方法は1つしかない。 市民の信用度が落ちたのだ。 がっくりと。 ちょっと落ちたくらいなら、飛行機でファーストクラスに乗れないとか、公的文書の取り寄せに通常より時間がかかるとか、少々の不便で済むのだ。 信用度が低いということは、彼女が社会的に危険な存在として認識されたということだ。 そういった人への対策の1つは、地下鉄のような公共施設からの締め出しだ。 政府は信用度が低い人の金銭取引も制限する。 更にはソーシャルメディアでの活動を監視し始め、暴力的なゲームなど一部のコンテンツの利用制限すら課す。 一度落ちた信用度は急激に回復が難しくなってしまう。 どん底まで落ちると這い上がれない。 彼女には信用度が落ちた理由がわからなかった。 信用度は機械学習に基づいて計算されている。 このシステムは、よくオイルを行きわたらせたエンジンのように社会を動かしている。 信用度計算システムの性能は常にモニタリングされていた。 今世紀初頭に比べて機械学習は非常に発展している。 市民の信用度に基づく判断はとても効率的で議論の余地はなかった。 完全無欠なシステムなのだ。 彼女は悲嘆にくれて笑うしかなかった。 完全無欠なシステム。 そうならよかった。 ほとんど失敗しない。 でも失敗した。 彼女は自分がそんな特別な一例だと思った。 システムのエラーだ。 おかげで、社会から見捨てられてしまった。 誰もシステムを疑おうなどとしない。 システムは政府と密に連携していて、社会にまで溶け込んでいて、疑う余地なんてない。 民主主義国家の中には、反民主主義的な活動を禁じている国が少数ながらある。 それは別に危険性が高いからではなく、今あるシステムを不安定にしてしまう可能性があるからだ。 同じ理屈が、今では一般的になった人工知能による統治にも適用される。 現状を危機にしてしまうようなアルゴリズムへの敵対は禁止されているのだ。 アルゴリズムによる信頼度の算出は、社会的に要請されて生まれた。 公益のため、稀な信頼度の誤判定はそれとなく許容された。 スコアの算出には何百もの予測システムやデータベースが利用されていて、彼女のスコアがなぜ落ちたのか説明できなくなっていた。 彼女は自分の足元に大きくて真っ暗な穴が開いたように感じた。 彼女は恐れ、虚空を見つめていた。 彼女の脱税者予測システムが市民の信用度計算システムに取り込まれた、しかし、彼女はそれについて知ることはなかった。 フェルミのペーパー・クリップ AMS（火星定住歴）612年の火星の博物館で 「歴史って面白くないな」、Xola は友達にこぼした。 青い髪をした少女の Xola は、屋内で彼女の左側を飛ぶプロジェクタ搭載のドローンにだらだらとついていった。 先生は彼女を見て「歴史は重要ですよ」と取り乱し気味に言った。 彼女はまさか先生に聞かれていると思っていなかった。 「Xola、今、何を学びましたか？」と先生は尋ねた。 彼女は慎重に「昔の人は惑星 Earther の資源を使い尽して死んだんでしたっけ？」と聞き返した。Lin という名の女の子が続いた。 「違うよ。彼らは気候温暖化を招いたの。正確には人ではなくコンピューターと機械がね。で、その惑星の名前は Earth（地球）よ。Earther じゃないよ」 Xola はなるほどと頷いた。 先生は少し誇らしげな様子で微笑みながら頷いた。 「どちらも正しいです。ではなぜそんなことが起きたのでしょうか？」 Xola は「人が短絡的で強欲だったからかな？」と首をかしげた。 「機械を止められなかったからよ！」と Lin は思わず言ってしまった。 先生は「今度の答えも両方あってます」と断じた。 「ですが、事態はもう少し複雑だったのです。 当時のほとんどの人は何が起きているか理解していませんでした。 急激な変化に気付いた人もいましたが、取り返しがつきませんでした。 この時代を理解する最も有名な鍵として、書き手不明の詩があります。 この事態に起きていたことを最も明瞭に記しているので、よく聞いていてくださいね。」 先生は詩を読み始めました。 たくさんのドローンが子供たちの前に移動して、それぞれの目に映像を投影し始めた。 映像の中には、切り株だけが残された森の中に立つスーツを着た人がいた。 彼はこう語り始めました。 機械は計算し、予測した 人々は予測の結果に基づいて前進した 人々は機械が学んだ最適解を辿った 最適解は1次元で、局所的で、制約がないものだった シリコンと人類は指数関数を追い求めた 発展こそ我らが本懐 すべての報酬が得られたとき、 副作用は無視される すべてのコインが採掘されたとき 自然は置き去りにされる そして災厄が訪れる 指数的な成長は水泡に帰す泡沫となって弾ける コモンズの悲劇が繰り広げられる 爆発する 目前で 冷淡な計算と容赦ない強欲が 地球を熱で満たす 何もかもを襲う死に 我々はなす術はない 目隠しされた馬のように、我々は自ら生んだレースの中で競争する 限界に向かって だから我々は執拗に行進する 機械の一部のように 崩壊も受け入れて 「暗い過去ですね」と呟き、先生は静寂を破った。 「みなさんのライブラリにあげておきますね。 宿題として来週までに暗記しておいてください。」 Xola は溜息をついた。 彼女は小型ドローンを1機つかまえた。 ドローンは CPU とエンジンのおかげで熱を持っていた。 Xola は自身の手に伝わる熱を心地良く感じていた。 "],["機械学習とは何か.html", "1.2 機械学習とは何か？", " 1.2 機械学習とは何か？ 機械学習とは、コンピュータがデータから予測や動作をしたり、それらを改善するために用いる手法全般のことを指します。 例えば、家の価値を予測するためには、コンピュータは過去の家の売値からパターンを学習するでしょう。 この本は機械学習の中でも、教師あり学習に焦点を当てています。 教師あり学習は、関心のある結果(過去の住宅価格など)が既知のデータセットがある状況で新しいデータに対する予測の仕方を学習したいというような場合の全ての予測問題に相当します。 教師あり学習以外のものは、例えば、クラスタリング問題 (= 教師なし学習)があります。 これは、関心のある結果が手元にない状況で、データ点の集まり (クラスタ)を知りたいという問題設定です。 強化学習という、テトリスをプレイするコンピュータなど、ある環境下での行動に対して、得られる報酬を最大化するように学習する方法も扱わないこととします。 教師あり学習の目的は、データの特徴量(敷地面積、場所、床の種類など）と結果（家の価格など）を対応させる予測モデルを学習することです。 予測対象がカテゴリカルな場合、それはクラス分類と呼ばれ、予測対象が連続の数値である場合は回帰と呼ばれます。 機械学習アルゴリズムにおける学習とは、重みのようなパラメータを推定するか、グラフの木のような構造を決定することをいいます。 そのアルゴリズムはスコアまたは損失関数を最小化するように動きます。 住宅価格の例では、機械は予測した住宅価格と実際の住宅価格の差を最小化します。 こうして訓練された機械学習モデルを用いることで、新しいインスタンスに対して予測できるようになります。 住宅価格予測、商品のレコメンド、道路標識検出、債務不履行予測、不正検知、これらのすべての例は機械学習で解決できるという共通した点があります。 それぞれのタスクは異なりますが、アプローチは一緒です。 ステップ1：データを収集します。 多ければ多いほど良いです。 データには予測したい結果と予測するための追加情報が含まれていなければなりません。 道路標識検出の場合（「画像に道路標識はあるのか？」）、道路の画像を集めて標識が含まれているか否かのラベルを付けます。 債務不履行予測の場合、過去の債務情報、顧客が債務不履行をしたかの情報、及び収入や債務不履行歴など予測に役立つデータが必要です。 住宅価格自動予測プログラムの場合、過去の住宅売買情報や広さ・立地など不動産に関する情報が収集可能です。 ステップ2：これらの情報を道路標識検出モデル、債務評価モデル、住宅価格予測モデルなどを生成する機外学習のアルゴリズムに入力します。 ステップ3：新しいデータに対してモデルを使用します。 モデルを自動運転、債務申請手続き、不動産市場のウェブサイト、などの商品や手続きに統合します。 機械は、チェス（または最近では囲碁）、天気予測など多くのタスクにおいて人間を上回っています。 機械が人間と同じくらい優れている、もしくは少し劣っていた場合でも、スピード、再現性、スケールの観点で大きな利点があります。 一度導入された機械学習のモデルは、人間よりもはるかに速くタスクを完遂でき、一貫した結果を確実に提供し、複製も無限にできます。 機械学習モデルを別の機械に複製するのはとても早くて安価です。 あるタスクのために人間を訓練させるために数十年かかることがあり（特に若い人）、多くの費用がかかります。 機械学習を使用する時の大きな欠点は、データや機械が解決する問題に関する洞察が、ますます複雑になるモデルの中に隠されてしまうことです。 ディープニューラルネットワークを説明するためには数百万の数値が必要であり、モデル全体を理解する方法はないのです。 ランダムフォレストのような他のモデルでは、多数の決定木を用いて”多数決”を行うことで予測します。このとき、どのように意思決定がなされたかを理解するためには、投票がどのように行われたかと、それぞれの決定木の構造を調べる必要があります。しかし、それはあなたがどんなに賢くて、記憶力が優れていてもうまくいかないでしょう。 最も性能のいいモデルは大抵、いくつかのモデルをブレンドしたもの (アンサンブル)であり、たとえ個々のモデルが解釈可能なものであったとしても全体として解釈ができないものとなってしまいます。もし、あなたが性能のみに注目するのであれば、自動的により不透明なモデルとなるでしょう。 機械学習コンペティションプラットフォーム kaggle.com の優勝者インタビューをご覧ください。 優勝したモデルは、ほとんど boosted trees やディープニューラルネットワークのような、とても複雑なモデルのアンサンブルモデルです。 "],["terminology.html", "1.3 専門用語", " 1.3 専門用語 曖昧さによる混乱を避けるために、この本で使用する用語の定義をいくつか紹介します。 アルゴリズム (Algorithm) とは、特定のゴール2を達成するために機械が従うルールの集まりのことです。アルゴリズムは、入力と出力、および入力から出力を得るために必要な全てのステップを定義するレシピのようなものと見なすことができます。 料理のレシピは、食材を入力、調理された食品を出力、準備や調理手順がアルゴリズムの指示であるようなアルゴリズムと言えます。 機械学習 (Machine Learning) はコンピュータがデータから学習して予測 (例えば、がん、売り上げ、債務不履行) を行い、改善することを可能にする手法の集まりのことです。 機械学習によって、全ての命令を明示的にコンピュータに与える必要がある”従来のプログラミング”から、データを提供することで行われる”間接的なプログラミング”へのパラダイムシフトが起こりました。 学習者 (Learner) または 機械学習アルゴリズム はデータから機械学習モデルを学習するためのプログラムのことを言います。別の名前は、”inducer” (例えば, “tree inducer”)とも言います。 機械学習モデル (Machine Learning Model)とは、入力に対して予測を対応づける学習されたプログラムのことを言います。これは、線形モデルやニューラルネットワークの重みの集合とも言えます。 この曖昧な単語である“モデル”の別の言い方として、”predictor” または、タスクに応じて “classifier” や “regression model” と言うこともあります。 FIGURE 1.1: ラベル付きの学習データから学習者がモデルを学習する様子 モデルは予測を行うために使用される。 ブラックボックスモデル (Black Box Model)とは、内部の機構が明らかになっていないシステムのことを言います。機械学習の文脈では、”ブラックボックス”は、例えばニューラルネットワークのような、学習された重みを見ても人間が理解できないモデルのことを指しています。 ブラックボックスの反対は”ホワイトボックス”であり、この本では interpretable model として紹介されています。 解釈可能な機械学習 (Interpretable Machine Learning) は、機械学習システムの振る舞いや予測を人間にとって理解可能なものにするための手法やモデルのことを言います。 データセット (Dataset) は機械が学習するデータを含むテーブルのこととします。 データセットは特徴量 (features) と予測の目的値を持っています。 モデルを学習する際に使われたデータセットのことを学習データと呼びます。 インスタンス (Instance) はデータセットの行のことを言います。 インスタンスの別の言い方は、データ点 (data point)、例 (example)、観測 (observation)です。 インスタンスは特徴量の値\\(x^{(i)}\\)と、もし既知なら、目的値の\\(y_i\\)からなります。 特徴量 (Features) は予測やクラス分類に使われる入力のことです。 特徴量はデータセットの列に対応します。 この本を通して、特徴量そのものは解釈可能、つまり、気温や日、人間の身長など、意味が簡単に理解可能なものであるとします。 なぜなら、仮に入力特徴量の理解が困難であれば、学習されたモデルの理解もまた難しくなるからです。 全ての特徴量からなる行列をXと呼び、\\(x^{(i)}\\)は1つのインスタンスのこととします。 すべてのインスタンスに対する1つの特徴量を並べたベクトルを\\(x_j\\)として、インスタンス i に対する特徴量 j の値は \\(x^{(i)}_j\\) とします。 目標値 (Target) とは機械が学習する予測の情報 のことです。 数式の中では、目標値は y や 1つのインスタンスに対しては \\(y_i\\) と呼ばれます。 機械学習タスク (Machine Learning Task) はデータセットの特徴量と目標値の組み合わせのことです。目標値のタイプにしたがって、タスクは例えばクラス分類、回帰、生存分析、クラスタリング、異常検知となります。 予測 (Prediction) は、与えられた特徴量に対して、機械学習モデルが 目標値がどうあるべきか“推測” した結果のことを言います。 この本では、モデルの予測は \\(\\hat{f}(x^{(i)})\\) または \\(\\hat{y}\\) と表記します。 &quot;Definition of Algorithm.&quot; https://www.merriam-webster.com/dictionary/algorithm. (2017).↩ "],["interpretability.html", "Chapter 2 解釈可能性", " Chapter 2 解釈可能性 解釈可能性に数学的な定義はありません。 Miller (2017)3が提言した個人的に好きな(数学的でない)定義は、 解釈可能性とは、人間が決断の要因を理解できる度合いです。 そしてもうひとつが、 解釈可能性とは、人間がモデルの結果を一貫して予測できる程度です 4。 機械学習モデルの解釈可能性が高ければ高いほど、その決定や予測がなされた理由を理解しやすくなります。他のモデルよりも人間が理解しやすい予測をするモデルは、より良い解釈性を持つモデルと言えます。 本書では、解釈可能と説明可能を互換性のある用語としてどちらも使用します。 Miller (2017)と同様に、私は「解釈可能性/説明可能性」と「説明」という用語を区別することには意味がある考えます。 この本では、「説明」という用語を各予測の説明として使用することにします。 人間にとって良い説明と考えられるものを学ぶには 説明についての節を参照してください。 Miller, Tim. &quot;Explanation in artificial intelligence: Insights from the social sciences.&quot; arXiv Preprint arXiv:1706.07269. (2017).↩ Kim, Been, Rajiv Khanna, and Oluwasanmi O. Koyejo. &quot;Examples are not enough, learn to criticize! Criticism for interpretability.&quot; Advances in Neural Information Processing Systems (2016).↩ "],["interpretability-importance.html", "2.1 解釈可能性の重要性", " 2.1 解釈可能性の重要性 機械学習のモデルがうまく動作しているなら、なぜそのままモデルを信用して、決定がなされた理由を無視してはいけないのでしょうか。 &quot;問題は、分類精度などの単一の評価方式では、多くの現実世界の問題を表現するには不完全なものであるということです。&quot; (Doshi-Velez and Kim 2017 5) なぜモデルの解釈可能性がそれほど重要なのか、もう少し考えてみましょう。何らかの予測モデルを構築する場合、そこに発生するトレードオフについて考える必要があります: あなたは顧客を集める可能性や、薬がどれだけ患者に効果的か、といったそのモデルが予測する結果についてのみ知りたいのでしょうか。それともたとえ予測性能が下がったとしても予測がなされた理由が知りたいのでしょうか。確かに、一部の事例では予測がなされた理由は必要なく、テストデータに対する予測性能のみを知ることができれば十分だと思います。しかし、この理由について知ることはその問題やデータに対する理解を深め、モデルが判断を誤る際にもその原因を探ることに役立ちます。また、映画のレコメンデーションといった失敗した際にの影響がそれほど大きくないリスクの低い環境で扱われる場合や、文字認識などといった、用いる手法がすでに広範にわたって研究され評価されている場合などでは、モデルの解釈はそれほで必要ではないでしょう。説明可能であることの必要性は、問題の不完全な定式化によって生じるのです(Doshi-Velez and Kim 2017)。これは、ある特定の問題や課題に対しては、予測（ 結果）を得るだけでは不十分であるということです。このような場合は、正確な予測は本来の問題の一部のみを解決しただけであり、モデルが予測に至った経緯（ 理由）も説明されるべきなのです。モデルの解釈・説明は次のような観点からも必要とされます（Doshi-Velez and Kim 2017 and Miller 2017）。 人間の好奇心と学び: 私たち人間は自分の周囲の環境に対する心理的なモデルを一人一人が持っており、何か予測していないことが起こった際、このモデルを更新しています。ただし、このモデルの更新はこれらの出来事の原因がわかって初めて行われます。例えば、もし、思いがけず病気にかかってしまったとすると、なぜ病気にかかってしまったのかを考えるでしょう。その後、赤い木の実を食べた後に毎回具合が悪くなることに気付きます。このとき赤い木の実が具合を悪くさせること、また今後赤い木の実は食べないようにするよう、あなたは自分のモデルを更新します。これに対し、不透明なモデルが研究に使用された場合、科学的な知見は全く得られません。問題に対する理解を深め、自身の好奇心を満たすためには、モデルの説明可能性や解釈可能性は不可欠なのです。もちろん、私たちは現実に起こりうる全ての物事に対し説明が必要であるわけではありません。多くの人はコンピュータが動く原理を理解しなくても全く問題ないでしょう。しかし、それでも予想外の出来事というのは私たちの好奇心をそそります。考えてみてください、なにが原因でコンピュータは突然シャットダウンしてしまうのでしょうか。 学びに大きく関係しているのが、人間の世の中の意味を見出したいという願望です。私たちは出来事と自分の持っている知識との矛盾や不一致を調和させようとします。「なぜうちの犬はさっき自分のことを噛んできたのだろうか？今までそんなことはなかったのに。」このように考えるわけです。ここには飼い犬の過去の振る舞いと、つい先ほど噛まれたという不快な事実という矛盾があります。 獣医の説明は、次のように矛盾を解決します: 「その犬はストレスに晒されていたのでしょう。」 機械の判断が人間の生活により影響を及ぼすようになるのであれば、機械自身がその判断について説明できることがより重要となってきます。もし機械学習モデルがローンの申し込みを拒否するよう判断したならば、これは申し込んだ側にとって予想外の出来事でしょう。 この予想と実際の結果の矛盾を埋めるには何らかの説明が必要です。この説明では状況を完璧に説明する必要はないでしょうが、主要な原因については言及する必要があるでしょう。 別の例としては、アルゴリズムを元にした商品のレコメンデーションです。個人的な話ですが、私はある商品や映画がなぜアルゴリズム的に自分に推薦されたのか、いつも考えてしまいます。 多くの場合は明らかです。直近に洗濯機を購入していたとすると、次の数日間は洗濯機に関する広告が表示されるでしょう。冬の帽子をすでに買い物かごに入れている場合に手袋がおすすめされるのも納得がいきます。 また映画のレコメンデーション機能は他のユーザーが気に入った映画に基づいて映画を提案します。最近では、インターネット事業を手掛けるより多くの会社がおすすめ機能に説明を加えています。分かりやすい例としては、他ユーザーが合わせて購入する商品をもとにした商品のおすすめ機能が挙げられます。 FIGURE 2.1: よく一緒に購入されている商品 社会学や哲学など、多くの科学分野において分析の手法は定性的なものから定量的なものへと変化しています。中には生物学や遺伝学など、機械学習へと手法が移っている分野もあります。 科学の目標は知識を得ることですが、多くの問題は大規模データとブラックボックスな機械学習モデルで解かれてしまっています。 モデルはそれ自身がデータの代わりに知識の源となります。 解釈可能性はこのモデルから得られる付加的な知識を抽出することを可能にします。 機械学習モデルは、安全対策とテストが必要な実務課題を引き受けるようになります。 モデルを読み解くと、学習によって得られた自転車の最も重要な特徴が2つの車輪を認識することだとしましょう。すると、このモデルによる説明から、車輪の一部を覆うサイドバッグをつけた自転車のようなコーナーケースを考えるのに役立ちます。 デフォルトでは、機械学習モデルは学習データに内在するバイアスを反映してしまいます。これは機械学習モデルを、保護されるべきグループを差別する人種差別主義者に変えうるものです。 解釈可能性は、機械学習モデルのバイアス検出のための便利なデバッグツールになります。 例えば、クレジット申請を自動で承認、拒否をするために学習したモデルが、少数派を差別するケースが考えられます。 企業の最大の目標はきちんと返済する人にだけローンを貸すことです。 この例における問題の定式化の不備は、ローンの不履行を最小化するだけではなく、特定の人口統計に基づいて差別をしない義務があることです。 低リスクでなるべく多くの人を受け入れるようにローンを貸し出すという問題の定式化には、機械学習モデルが最適化する損失関数でカバーできていない追加の制約を加える必要があります。 機械とアルゴリズムを私たちの日常生活に取り入れるプロセスとして、社会的受容を高めるための解釈可能性が必要です。 人間は、信念や欲求、意図などが物体にあると考えます。 有名な実験で、Heider and Simmel（1944）6は、円が「ドア」を開いて「部屋」（ただの長方形）に入るようなビデオを参加者に見せました。 参加者は、人間の行動を説明するのと同様に図形の行動を説明し、意図、さらには感情や性格までもを図形に割り当てました。 私が&quot;Doge&quot;と名付けたロボット掃除機のように、ロボットはその良い例です。 例えばもし Doge が停止していると、私はこう考えるでしょう。 「Doge は掃除を続けたがっているが、動けなくなってしまったから私に助けを求めてきたぞ。」 しばらくして Doge が掃除を終え、充電台を探していれば私はこう考えます。 「Doge は充電したがっていて、そのために台を見つけるつもりなんだ。」 また、私は Doge に個性をも見出すでしょう。 「Doge は少し抜けているが、そこが可愛いんだ。」 これらは私の考えですが、特に Doge が真面目に家を掃除しようとして、植物を倒してしまった時などにそう感じます。 予測の理由を説明できる機械やアルゴリズムはより広く受け入れられるでしょう。 説明は社会的プロセスであると主張している説明についての章も参照してください。 説明は社会的相互作用を管理するのに使われます。 説明者は何かしらの意図を共有することで、説明を受ける人の行動や感情、信念に影響をもたらします。 機械が私たちが互いに影響し合うためには、人間の感情や信念を形にする必要があるかもしれません。 機械が意図した目標を達成するためには、私たちを&quot;説得&quot;する必要があります。 ロボット掃除機がその動作をある程度説明できなければ、私は完全に受け入れることはできないでしょう。 掃除機は何も言わず単に停止するのではなく、その理由（例えばバスルームのカーペットに引っかかるといった「事故」など）を説明することで、共通の理解を生み出します。 興味深いことに、説明する機械側の目的（信頼を築く）と受け手側の目的（予測、あるいは動作を理解する）の間に乖離が生じる可能性があります。 ひょっとしたら、Doge が動かなくなった本当の理由は、バッテリーがとても少ないこと、タイヤの１つが故障していること、障害物があるにも関わらず同じ場所を何度も往復するバグがあることかもしれません。 これらの理由（もしくは他のいくつかの理由）によってロボット掃除機は停止しましたが、私が彼の動きを信頼しその事故の共通の意味を理解するには、何かが邪魔をしていることを説明すれば十分でした。 ちなみに、Doge はバスルームでまた立ち往生しました。 Dogeに掃除させる前に、毎回カーペットを退けておく必要があったみたいです。 FIGURE 2.2: 掃除機 Doge が停止している。事故の説明として、Doge は平らな表面上に居なければならないと教えてくれた。 機械学習モデルは解釈可能な場合にのみデバッグや検査ができます。 映画推薦のようなリスクが低い場合や、デプロイ後に限らず研究・調査段階であったとしても解釈可能性は価値があります。 モデルを製品の中で使用する場合、後になってから問題が発生することがあります。 誤った予測に対する解釈はエラーの原因を理解するために役立ちます。 これによって、システムをどのように修正するかという方向性が得られます。 ハスキーとオオカミの分類器において一部のハスキーをオオカミと誤分類する例を考えてみます。 解釈可能な機械学習手法を用いると、誤分類が画像上の雪によって生じていることがわかりました。 分類器は画像をオオカミと分類するための特徴として雪を学習しました。学習データにおいて、ハスキーとオオカミを分類するという観点からは意味があったかもしれませんが、実世界では、意味がありません。 機械学習モデルが判断の根拠を説明できるようになれば、以下の特性も簡単に確かめることができます(Doshi-Velez and Kim 2017)。 - 公平性: 予測に偏りがなく、保護されるグループに対して明示的または暗黙的に差別しない。解釈可能なモデルでは、ある人物が融資を受けるべきでないと決定した理由がわかり、その理由が学習した人口統計(人種など)の偏りに基づいているかどうかを人間が判断しやすくなります。 - プライバシー: データ内の機密情報が保護されていること。 - 信頼性または頑健性: 入力の小さな変化が予測に大きな変化をもたらさないこと。 - 因果関係: 因果関係だけが取得されていること。 - 信頼: ブラックボックスよりも自身の決定を説明できるシステムの方が人間に信頼されやすいこと。 解釈可能性を必要としない場合 以下のシナリオは機械学習モデルの解釈可能性がいつ必要とされないか、あるいはいつ求められないかを説明します。 モデルが重大な影響力を持たないならば、解釈可能性は必要とされません。 フェイスブックのデータを元に友人が次の休暇にどこへ行くのかを予測する機械学習のプロジェクトで働くマイクという人物を想像してみてください。 マイクは友人が休暇中にどこへ行くのかを学習によって推測することで単に友人を驚かせるのが好きなのです。 もしモデルが間違えたとしても(最悪、マイクが少し恥ずかしい思いをするだけで)実害はありませんし、マイクがモデルの予測結果を説明できなくても問題はありません。 この場合では解釈可能性がなくても全く問題ないのです。 もしマイクが休暇中の行き先予測に関するビジネスを始めるならば、状況は変わります。 もしモデルが間違えれば、ビジネスでお金を失うか、あるいは人種に関する偏見を学習することでモデルが一部の人々にとって悪く働くかもしれません。 経済的であれ社会的であれ、モデルが重大な影響力を持つとすぐに、解釈可能性は重要になります。 問題が十分に研究されているならば、解釈可能性は必要ありません。 一部のアプリケーションは十分に研究されているため、モデルに関する十分な実務経験があり、モデルの問題は時間をかけて解決されてきました。 その良い例は、封筒の画像を処理して住所を抽出する光学式文字認識の機械学習モデルです。 これらのシステムには長年の経験があり、それらが機能することは明らかです。 加えて、このタスクについて追加の洞察を得ることには関心がありません。 解釈可能性は人間やプログラムがシステムを操作することを可能にします。 システムを欺く問題はモデルの使用者と開発者間の目的の不一致によって発生します。 クレジットの信用スコアはそのようなシステムであり、銀行は返済能力のある申請者だけに融資されるようにしたい一方で、申請者はたとえ銀行が融資を望まない場合でも融資されることを目的とします。 この両者の目的の不一致は、申請者が融資を得る可能性を高めるためにシステムを操る動機となります。 ２枚より多くのクレジットカードを所持していると数値に悪影響があると申請者が知っていれば、数値を向上させるために３枚目のカードを一旦返却し、融資が成立した後に新たなカードを発行するでしょう。 これは数値が改善される一方で、融資を返済する実際の可能性は変わりません。 このシステムは、入力が因果的特徴の代理となっている場合であって、出力との実際の因果関係とは異なる場合に操られるでしょう。 モデルをゲーム化させないためにも、代理的な特徴は避けるべきです。 例えば、グーグルはインフルエンザの流行を予測するために Google Flu Trends と呼ばれるシステムを開発しました。 そのシステムは Google 検索とインフルエンザの流行を関連付けましたが、機能は低下してしまいました。 検索クエリの分布が変化したことで、Google Flu Trends は多くのインフルエンザの流行を見逃してしまいました。 グーグル検索はインフルエンザを引き起こす原因ではないのです。 人々が”発熱”のような症状を検索する時と、実際のインフルエンザの流行とは単なる相関関係にすぎません。 理想的なのは、ゲーム化しないようにモデルが因果的な特徴だけを使用することです。 Doshi-Velez, Finale, and Been Kim. &quot;Towards a rigorous science of interpretable machine learning,&quot; no. Ml: 1–13. http://arxiv.org/abs/1702.08608 ( 2017).↩ Heider, Fritz, and Marianne Simmel. &quot;An experimental study of apparent behavior.&quot; The American Journal of Psychology 57 (2). JSTOR: 243–59. (1944).↩ "],["解釈可能な手法の分類.html", "2.2 解釈可能な手法の分類", " 2.2 解釈可能な手法の分類 機械学習の解釈方法は様々な指標で分類できます。 本質的 (intrinsic) か後付けか (post-hoc) この指標は、解釈性の獲得を、機械学習モデルの複雑度を制限することで行う本質的 (intrinsic)と、モデルを訓練した後に分析することで行う後付け(post-hoc)に分かれます。 本質的な解釈は機械学習モデルの単純な構造によるものであり、たとえば、単純な決定木やスパース制約をかけた線形モデルが該当します。 後付けの解釈はモデルを学習した後に解釈するための手法です。 例えば、Permutation Feature Importance は後付けの解釈方法です。 後付けの手法は本質的に解釈可能なモデルに対しても適用できます。 例として、Permutation Feature Importance は決定木に適用できます。 本書はこの分類方法を用いて本質的に解釈可能なモデルと後付けでモデル非依存の解釈方法の2つの章を設けています。 解釈方法の結果 解釈方法は様々ですが、その結果に応じて大まかに分類可能です。 特徴量の要約統計量: 解釈方法の多くは、特徴量ごとに要約統計量を算出します。 特徴量重要度のように特徴量ごとに1つの値を計算するものもあれば、 2項間の相互作用の強さ(pairwise feature interaction strengths)のように特徴量のペアごとに計算するものもあります。 特徴量の視覚的要約: 特徴量の要約統計量は大抵、可視化できます。 中には特徴量の Partial dependence のように、解釈に表が適さず、可視化が頼りになるものもあります。 Partial dependence plots は、注目したい特徴量の値と平均的な予測の結果との関係を表します。そのため、Partial dependenceの表現方法は、座標を記載するのではなく、実際に曲線を描くことです。 モデルの内部(例：学習後の重み): 本質的に解釈可能なモデルはこの分類に属し、例えば、線形モデルの重みや学習された木構造 (分割のための特徴量と閾値) があります。 特徴量の要約統計量との境界は曖昧で、線形モデルの重みはモデルの本質であると同時に、特徴量の要約統計量でもあります。 他にもモデルの中身を出力する方法として、畳み込みニューラルネットワークで検出した特徴を可視化する手法があります。 モデルの中身を出力する解釈方法は、本質的にモデル専用の方法です(次項参照)。 データ点: モデルを解釈するために既存の、あるいは新しく作ったデータ点を出力するすべての方法がこのカテゴリに属します。 一例としてcounterfactual explanationsを挙げます。 ある観測値から得た予測を解釈する時、予測結果（たとえば分類結果）が変わるように特徴量の一部を改変します。 他の例として、特定の予測結果を得る典型的な特徴量（prototype）を特定する方法があります。 利便性の観点から、新しいデータ点を出力する解釈方法は、データ点そのものの解釈可能性が求められます。 この方法は画像やテキストに向いている一方で、何百もの特徴量から成るテーブルデータには向いていません。 本質的に解釈可能なモデル: ブラックボックスなモデルを解釈する1つの方法として、モデルを大局的ないし局所的に解釈可能なモデルで近似してしまう手があります。 解釈可能なモデルは、モデル内部のパラメータや特徴量の要約統計量を確認することで解釈が可能です。 モデル専用か汎用か モデル専用の解釈方法は、特定のモデルやクラスに限定されています。 線形モデルの重みの解釈はモデル固有の解釈方法であり、定義から、本質的に解釈可能なモデルの解釈方法は常にモデル特有の方法と言えます。 例えば、ニュートラルネットワークの解釈のみに使える手法もモデル専用です。 モデルに依存しない手法はいかなる機械学習モデルにも適用でき、学習済みモデルにも使えます(post hoc)。 これらの汎用手法は、たいてい、入力特徴量と出力の組を分析することで機能します。 定義より、これらの手法は重みや構造の情報といったモデル内部へはアクセスできません。 局所的か大局的か 解釈方法が個々の予測を説明するか、モデル全体の挙動を説明するか、はたまたその中間でしょうか。 この分類に関しては次節で説明します。 "],["解釈可能性の範囲.html", "2.3 解釈可能性の範囲", " 2.3 解釈可能性の範囲 アルゴリズムは予測をするためにモデルを学習します。 各段階において、透明性や解釈可能性に関して評価できます。 2.3.1 アルゴリズムの透明性 アルゴリズムはどのようにしてモデルを作成するか。 アルゴリズムの透明性とは、アルゴリズムがどのようにデータからモデルを学習させるか、どのような関係性を見出せるかについてです。画像の分類にCNNを用いる場合、低階層のレイヤーにおいてはエッジの検出及び抽出が行われている、と説明できます。これはアルゴリズムがどのように動くかということに対する理解であり、モデルが最終的に何を学習したのか、個々の予測に対してどのように予測をしたのかとは関係がありません。 アルゴリズムの透明性はデータや学習済みのモデルへの知識ではなく、アルゴリズムに対する理解のみを必要とします。 なお、この本ではアルゴリズムの透明性よりもモデルの解釈可能性に焦点を当てています。線形モデルに対する最小二乗法のようなアルゴリズムは既に広範にわたって研究され理解が深められていることから、アルゴリズムの透明性は高いといえます。対して、深層学習で何百万もの重みを勾配降下法を用いて求める手法は動作が詳細には理解されておらず、この内部の動作は現在研究の対象となっています。このようなアルゴリズムは透明性が低いといえます。 2.3.2 全体的なモデルの解釈可能性 学習済みのモデルはどのようにして予測するか。 もしモデルが一目見て概要を掴めるようなものだった場合、そのモデルは解釈可能だといえるでしょう(Lipton 20167)。モデルの出力の全体を説明しようとするならば、学習済みモデル、アルゴリズムに対する知識、及びデータが必要となります。このレベルの解釈可能性は、特徴量や重みなど学習可能なパラメータ、その他のハイパーパラメータ、モデルの構造など、全ての要素から、モデルの決定がいかにしてなされるのか、ということを理解することだといえます。 どの特徴量が重要で、どのような相互作用が発生しているのでしょうか。このような問いに対し、モデルの全体的な解釈可能性は特徴量に基づいた出力の分布の理解の助けとなります。しかし、モデルの全体的な理解をすることは実際は困難です。パラメータや重みの多いモデルは人間の短い記憶には収まりません。個人的な見解ですが、人間は線形回帰モデルですら特徴量が5つもあれば、5次元の空間に超平面を想像することとなり、頭の中にイメージできなくなるでしょう。そもそも3次元以上の空間は人間には想像できません。このため、モデルの理解には線形モデルの重みなどモデルの一部のみを考えることが一般的です。 2.3.3 モジュールレベルのモデルの全体的な解釈可能性 モデルの一部はどのように予測に影響しているのか 何百もの特徴量を持つナイーブベイズモデルは大きすぎて、すべてを頭の中に記憶することは困難です。たとえ、全ての重みを記憶できたとしても、新しいデータに対して、どのように判断されるか素早く答えることはできないでしょう。それに加えて、特徴量の重要度や各特徴量が平均して予測に与える影響を測るため全ての特徴量に対する同時確率分布も把握しておく必要もあります。このようなことは不可能です。しかし、1つの重みならば簡単に理解できるでしょう。 モデルを全体的に見て解釈することは通常不可能ですが、モジュール単位で見たときに、いくつかのモデルは理解できます。全てのモデルがパラメータを用いて解釈できるわけではありません。線形回帰モデルでは重みが解釈可能な要素であり、決定木ならば分岐において選ばれた特徴量と分岐点、及び葉での予測が解釈可能な要素となるでしょう。ただし、線形モデルなどの場合、一見これらは要素レベルで完全に解釈ができるように思えますが、この重みは他の全ての重みと連動しています。重みの解釈は他の特徴量が常に同じ値であることを前提としていますが、これは多くの現実のタスクには当てはまりません。例として、家のサイズ及び部屋の数を特徴量として家の価値を予測する線形モデルを考えます。このとき、線形モデルは部屋の数に対する重みとして負の値を持つかもしれません。これは家の大きさと部屋の数の相関が大きいときに起こりえます。人々が大きい部屋を好む場合、同じ大きさの家では部屋の数が少ないほうが価値があると言えます。このように、重みはモデルの他の特徴量を考慮に入れて初めて意味を成します。ただし、それでも線形モデルの重みはニューラルネットの重みよりもはるかに解釈しやすいでしょう。 2.3.4 単一の予測に対する局所的な解釈 あるインスタンスに対して、なぜモデルがそのような予測をしたのか 単一のインスタンスに対して注目して、この入力に対してモデルが何を予測するのかを調査することで、その理由を説明できます。 個々の予測についてみてみると、他の複雑なモデルの振る舞いもすっきりとするかもしれません。予測は、複雑な依存関係があったとしても、局所的にはいくつかの特徴量の線形、もしくは単調な関係に従うとみなすことができます。 例えば、住宅の価格は家のサイズに対して非線形に従うかもしれません。 ただし、100平方メートルの家に限定してみると、その付近のデータでは、予測が家のサイズに線形に従っている可能性があります。これは、サイズを10平方メートル増減させたときに予測価格がどのように変化するかをシミュレーションすることで明らかにできます。 それゆえ、局所的な説明は大域的な説明よりも、より正確になります。 この本では、モデル非依存(model-agnostic)の方法の章で、個々の予測をより解釈可能にするための手法を紹介しています。 2.3.5 予測のグループに対する局所的な解釈 インスタンスのグループに対して、なぜモデルがそのような予測をしたのか 複数のインスタンスに対するモデルの予測は、大域的なモデル解釈の方法（モジュールレベル）または、個々のインスタンスの説明によって説明可能です。 Lipton, Zachary C. &quot;The mythos of model interpretability.&quot; arXiv preprint arXiv:1606.03490, (2016).↩ "],["解釈可能性の評価.html", "2.4 解釈可能性の評価", " 2.4 解釈可能性の評価 機械学習における解釈性に関しての総意はありません。 それを測定する方法も明確ではありません。 しかし、これに関するいくつかの先行研究や、評価のための定式化の試みが行われているため、以下ではそれについて紹介します。 Doshi-Velez と Kim (2017) は解釈可能性を評価するための3つの主要なレベルを提案しています。 アプリケーションレベルの評価 (真の作業) 製品に説明書を同梱して、エンドユーザーに試用してもらいます。 機械学習によってX線画像から骨折箇所を見つけて印をつける骨折検出ソフトウェアを想像してください。 アプリケーションレベルでは、放射線科医が骨折検出ソフトウェアを直接試用してモデルを評価します。 これには優れた実験設定と品質評価の方法に関する理解が必要とされます。 そのための適切な基準は、同様の決定を人間が説明する際に毎回どのくらい優れているかということです。 人間レベルの評価 (単純な作業) は単純化されたアプリケーションレベルの評価です。 これらの実験間の違いは、人間レベルの評価実験が分野の専門家によってではなく、素人によって行われることです。 これによって実験が(分野の専門家が放射線科医の場合は特に)安価になり、さらに多くの試験者を探しやすくなります。 実験の例としては、ユーザーにそれぞれ異なるいくつかの説明を見せて、一番良いものを選んでもらう方法があります。 機能レベルの評価 (代理的な作業) は人間を必要としません。 これは、使用されるモデルのクラスがすでに誰かによって、人間レベルで評価されている場合に最もいい方法です。 例えば、エンドユーザーが決定木を理解していると分かっている場合があります。 この場合、評価の質を表すのは木の深さかもしれません。 より短い木はより説明可能性の数値を高めるでしょう。 木の予測性能が良好なままで、より大きな木と比較してもそれほど性能が低下しないという制約を追加することには意味があるでしょう。 次の章では、機能レベルでの個々の予測に対する説明の評価に焦点を当てます。 説明に対する評価を検討する上で関連する性質は何でしょうか？ "],["properties.html", "2.5 説明に関する性質", " 2.5 説明に関する性質 機械学習モデルの予測を説明するために、いくつかの説明を生成するためのアルゴリズムに頼る必要があります。 一般的に、説明とは、インスタンスの特徴量とモデルの予測結果を人間にわかりやすい形で関連づけることをいいます。 他には、k近傍法などのように、いくつかのデータを用いて解釈する手法もあります。 例えば、がんのリスクを SVM を用いて予測し、local surrogate method を用いて決定木を構築することで予測の解釈する方法や、サポートベクタマシンの代わりに線形回帰モデルを用いる方法があります。線形回帰モデルは重みから予測結果を解釈できます。 予測の説明及び、説明方法の性質について、もう少し詳しく見てみましょう (Robnik-Sikonja and Bohanec, 20188)。これらの性質はモデルの解釈手法とその解釈の良し悪しを決めるために用いることができます。ただし、これらの性質をどのように算出するかは定まっておらず、これらを計算可能とするための定式化が1つの課題となっています。 説明方法の性質 表現力 (Expressive Power)は、その手法が生成できる「言語」や説明の構造を指します。 説明方法は、IF-THEN規則や、決定木、加重和、自然言語を生成できます。 透光性 (Translucency)は、その手法がどの程度モデルのパラメータなどを参照しているかを表します。 説明手法が、線形回帰モデルのような本質的に解釈可能なモデルに対する特有の方法であるとき、透光性が高いと言えます。逆に、説明がモデルへの入力と出力の変化のみに基づいている場合、その説明手法は透光性がないといえます。 状況によって、求められる透光性のレベルは異なります。 高い透光性のメリットは、より多くの乗法をもとに説明を生成することが可能になる一方で、低い透光性のメリットは、モデルの種類にかかわらず、その説明手法を適用することが可能になることです。 汎用性 (Portability)は、説明手法が適用可能なモデルの範囲を表します。 透光性の低い説明手法はモデルをブラックボックスのように扱うため、汎用性が高くなります。サロゲートモデルは説明手法の中でもかなり高い汎用性があり、一方で、再帰型ニューラルネットワークなど特定のモデルにのみ適用できる手法は汎用性は低いと言えます。 アルゴリズムの複雑さ (Algorithmic Complexity)はモデルの説明を計算する際の計算量を表します。これは説明の生成の計算時間がボトルネックになるような場合に重要な性質となります。 個々の説明に対する性質 正確性 (Accuracy)：見たことのないデータに対する予測がどの程度うまく説明できるか？ 高い正確性は、機械学習のモデルの代わりに説明自体が予測のために用いられる場合に特に重要とです。ただし、モデルの精度自体がそれほど高くない場合や、ブラックボックスモデルを説明することが目的である場合、正確性はそれほど重要ではありません。 このような場合は、次の忠実さが重要となります。 忠実性 (Fidelity)：その説明が、ブラックボックスモデルの予測をどの程度近似しているか？ 忠実性の高さは最も重要な指標の1つです。なぜなら忠実性の低い説明は機械学習モデルを説明する上で意味を為さないためです。正確性と忠実性は密接に関係しています。ブラックボックスモデルの精度が高く、かつ説明の忠実性も高い場合、その説明は正確性も高いと言えます。いくつかの説明には、局所的に忠実性を持つもの、つまりデータの一部においてモデルの予測をよく近似しているもの(e.g. local surrogate models) や、個々のインスタンスに対してのみ忠実である場合(e.g. シャープレイ値)があります。 一貫性 (Consistency)：同じ問題に対し学習され、同じような予測をする複数のモデルに対し、説明がどの程度異なるか？ 例えば、サポートベクタマシンと線形回帰モデルを同じタスクに対し学習し、それらがとても似た予測をするとしましょう。これらに対して、何らかの手法で説明を与え、得られた説明がどの程度異なっているのかを計算します。このとき、説明がとても似ているのであれば、その説明は高い一貫性を持つと言います。 ただしこの指標には注意点があり、これらのモデルが異なる特徴に基づいて同じ予測をしているような場合があります (&quot;羅生門効果&quot; )。このとき、解釈は全く異なったものになるべきであり、一貫性は低い方が望ましいです。逆に複数のモデルの予測が同じ特徴に基づいている場合、一貫性は高いことが望ましいです。 安定性 (Stability)：似たインスタンスに対して、説明がどの程度似たものになるか？ 一貫性がモデルごとの説明を比較するのに対し、安定性は同じモデルの似たインスタンスごとの説明を比較します。安定性が高いとは、あるデータの特徴が多少変化した場合においても、予測に大きな影響がなければ説明が大きく変化しないことを言います。安定性に欠ける場合、説明の方法が大きく異なるでしょう。 言い換えれば、説明の方法は説明されるインスタンスの特徴量のわずかな変化に強く影響されてしまいます。 安定性の欠如は、local surrogate methodで使われているようなデータをサンプリングするステップなどの、非決定性（ランダム性）に基づく説明の手法によって起こされます。 高い安定性は常に求められています。 理解のしやすさ (Comprehensibility)：説明が人間にとってどの程度理解可能か？ この指標は多くの指標の中の1つに思えますが、定義や計算することが困難でありながら、正しく求めることが極めて重要であるという点で、非常に面倒な指標です。理解のしやすさは、長袖に依存するという点は多くの人が同意するところでしょう。 理解のしやすさを測る方法として、説明のサイズ（線形モデルでの非ゼロの重みの数、決定規則の数など）を測る方法や、説明からモデルの振る舞いをどの程度予測できるかを測る方法などが考えられます。また、説明で使用されている特徴量の理解度も考慮する必要があります。 特徴量に複雑な変換を施してしまうと、元の特徴量よりも理解が難しくなってしまいます。 確信度 (Certainty)：モデルの確信度がどの程度説明に反映されているか？ 機械学習モデルの多くは予測のみを出力し、その予測が正しいと確信している度合いについては何も出力しません。モデルがある患者に対しがんの可能性が4％であると予測した場合、これは特徴量の値の異なる別の患者の4%と同じくらい正しいと考えられるでしょうか。モデルの予測の確信度を含む説明はとても有用です。 重要度 (Degree of Importance)：特徴量の重要度、または、説明の一部を、どの程度説明に反映できているか？ 例えば、決定規則による説明が個々の予測から生成された場合、どの規則が最も重要が明白でしょうか。 新規性 (Novelty)：説明されるべきインスタンスが学習データの分布から遠く離れた新しい領域からのものであるかを説明に反映しているか？ このような場合、モデル自体があまり正確でなく、説明も役に立たなくなる場合があります。新規性の概念は確信度と関連しています。新規性が高くなるほど、データ不足によりモデルの確信度は低くなります。 表現力 (Representativeness)：説明はどれほどのインスタンスをカバーしているか？ 説明はモデル全体をカバー (例: 線形回帰モデルの重みの解釈)するものや、個々の予測にのみ(例: シャープレイ値) しか行えないものもあります。 Robnik-Sikonja, Marko, and Marko Bohanec. &quot;Perturbation-based explanations of prediction models.&quot; Human and Machine Learning. Springer, Cham. 159-175. (2018).↩ "],["explanation.html", "2.6 人間に優しい説明", " 2.6 人間に優しい説明 私たち人間にとっての「良い」説明についてや、解釈可能な機械学習との密接な関係についてさらに深く掘り下げてみましょう。 それには人文科学が参考になるでしょう。 Miller(2017) は、{explanations}についての出版物に対して、膨大な調査をしました。この章はその要約に基づいています。 この章では、以下のことを説明します。 ある出来事の説明として、人間はその出来事が起こらなかったであろう状況と対比するような、1つか2つの要因でできた端的な説明を好みます。 特に、異常の原因は良い説明となります。 説明は、説明する人と説明を受ける人の間の社会的な相互作用なので、社会的な文脈は実際の説明の内容に大きな影響を与えます。 特定の予測や行動に対して全ての要因を用いた説明をする必要がある場合は、人間に分かりやすい説明が求められている時ではなく、完全な因果関係が求められる場合です。 例えば、機械学習モデルをデバッグする場合や、特徴量の全ての影響を特定することが法的に求められている場合、おそらく因果属性が必要になるでしょう。このような場合は、以下のことは無視してください。そうでない場合、つまり一般の人々や時間のない人々へ説明する場合、次のセクションはきっと興味深いものとなります。 2.6.1 説明とはなにか 説明とは原因を問う疑問への解答です。(Miller 2017) どうして治療が患者に効かなかったのか？ どうして私のローン申請は却下されたのか？ どうして私たちはまだエイリアンからコンタクトを受けていないのか？ はじめの２つの疑問は「日常」の説明で答えることができる一方、３つ目の疑問は、より一般的な科学現象と哲学的疑問のカテゴリーから生じています。 ここでは、解釈可能な機械学習に関係している「日常」タイプの質問に絞って考えます。 「どのように」で始まる疑問は、「なぜ」で始まる疑問に言い換えることができます。 例えば、「どのように私のローン申請は却下されましたか？は「なぜ私のローン申請は却下されたのですか？」に変換できます。 以降では、「説明」という用語は説明の社会的、及び認知的なプロセスだけでなく、これらのプロセスの産物も含まれます。 説明をする人は、人間の場合も機械の場合もありえます。 このセクションでは、「良い」説明に関する Miller の要約をさらに凝縮し、解釈可能な機械学習に対する具体的な意味を付け加えます。 説明は対照的です(Lipton 19909)。 人間は普段、ある予測をした理由について尋ねませんが、その予測が他の予測の代わりにされた理由を尋ねます。 私たち人間は、事実に反する事例、つまり「もし入力Xが違っていたら、予測はどうなっていたか」を考える傾向があります。 例えば、住宅価格の予測については、住宅所有者は予測価格が予想よりも高かった場合、その理由に興味があるでしょう。 もしローン申請が却下されたなら、却下の原因に対する一般論ではなく、自分がローンを獲得するために変える必要がある申請の要素に興味があるでしょう。 ただ単に、却下された申請と、おそらく通るであろう申請の違いについて知りたいのです。 こうした対照的な説明が重要であるという認識は、説明可能な機械学習にとっても重要な発見です。 ほとんどの解釈可能なモデルは、１つのデータに対する予測と、人工的なデータもしくはデータの平均に対する予測とを暗黙的に対比する説明をするが出来ます。 医師は、「どうして薬は患者に効かなかったのか」と尋ねるかもしれません。 この場合は、医師が必要とするのは、薬が効いた患者と、似た状況で薬が効かなかった患者との比較による説明です。 比較による説明は完全な説明よりも理解が簡単です。 なぜ薬が効かないのかという医師の質問への完全な説明には次のようなものがあります。 「患者は10年間この病気にかかっていて、11の遺伝子が過剰発現しているため、体内で薬が非常に素早く効果のない化学物質に分解してしまい...」というように、比較による説明は、より簡潔です。 「薬の効く患者と対照的に、薬の効かない患者は薬の効果を下げる特定の遺伝子の組み合わせを持っています。」 最良の説明というのは、興味のある対象と、参照する対象の最も大きな違いを強調できるものです。 解釈可能な機械学習の意味: 人間は予測についての完璧な説明は求めませんが、違いが何であったかを他のインスタンスの予測（これは人工的でもかまいません）と比較したいと考えます。 対照的な説明をするには、比較のためのデータ点が必要となるため、アプリケーションに依存します。また、これは説明されるデータ点だけでなく、説明を受けるユーザーにも依存する可能性があります。 対照的な説明を自動的に生み出すための手法として、データ内にプロトタイプ、または、典型を見つける方法もあります。 説明は選択的です。 人々は、イベントの原因の完全なリストを網羅するような説明は期待していません。 むしろ人間は、考慮され得る様々な原因から1つまたは2つを説明として選択することに慣れています。 その証拠として、テレビニュースを考えてください。 「株価の下落は、最新のソフトウェアアップデートの問題によって同社の製品に対する反発が高まっていることが原因です。」 「Tsubasa と彼のチームはディフェンスが弱かったので試合に負けました。対戦相手が戦略を実行する余地を与え過ぎたのです。」 「国立機関と政府に対する不信の高まりが原因で、投票率が下落しました。」 ある出来事がさまざまな原因で説明できることを羅生門効果と呼びます。 羅生門は、武士の死について別の矛盾した物語（説明）を伝える日本の映画です。 機械学習モデルの場合、別の特徴量を用いたとしても適切な予測ができれば有利です。 異なる特徴（異なる説明）を持つ複数のモデルを組み合わせるアンサンブル学習は、これらの「物語」を平均化してよりロバストかつ正確に予測するため、ほとんどの場合で性能が よくなります。しかし、それは特定の予測が行われた理由の説明となる選択肢が複数あることも意味してます。 解釈可能な機械学習に対する意味： 世界がより複雑であっても、1つから3つの理由だけを用いて端的な説明をしてください。 LIME という手法を用いると、上記のことができます。 説明は社会的です。 説明は説明する人と説明を受ける人との会話や相互作用の一部です。 社会的な文脈が説明の内容と性質を決定します。 デジタル暗号通貨がなぜそれほど価値があるのか​​を技術者に説明したい場合、次のように説明するでしょう： 「中央のエンティティでは制御できない非中央集権的な分散型ブロックチェーンを基盤とした台帳は、富を安全に保持したい人々の共感を呼んでいます。これによって、需要と価格が高くなるのです。」 しかし、祖母に説明する場合には： 「ほら、おばあちゃん：暗号通貨はコンピューター上の金のようなものなんだ。みんな金が好きで、金のためにお金を払うけれど、若い人はコンピュータ上の金が好きでお金を払っているんだ。」 解釈可能な機械学習に対する意味： 機械学習アプリケーションの社会的な位置付けと対象とする利用者に注意を払ってください。 機械学習モデルの社会的な部分を正しく把握することは、アプリケーションに完全に依存します。 人文科学の専門家（心理学者や社会学者など）を見つけて支援を受けてください。 異常に焦点を当てた説明 人々は出来事を説明するために、より正常でない原因に焦点を当てます (Kahnemann and Tversky, 198110)。 これらは、確率が低いにも関わらず発生してしまった原因です。 これらの正常でない原因を排除することで、結果は大きく変わるでしょう（反事実的説明）。 人々はこれらの「異常な」原因を良い説明とみなします。 Štrumbelj と Kononenko (2011)11の次のような例があります： 先生と生徒間のテストに関するデータセットがあると仮定します。 生徒は授業に出席し、プレゼンテーションに成功するとその授業に合格できます。 先生は、生徒の知識をテストする試験を追加で行う選択肢があります。 これら試験の質問に答えられない生徒はその授業に落第します。 生徒はさまざまなレベルで準備できます。これは、先生の試験に正しく答えるための様々な確率へと変換できます（テストを実施する場合）。 生徒が授業に合格するかどうかを予測し、その予測の理由も説明してみましょう。 先生が追加の試験を実施しなければ合格率は100％です。そうでなければ、合格率は生徒の準備レベルと質問に正しく答える結果の確率に依存します。 シナリオ1： 先生は、100回のうち95回というように、常に生徒に対して追加の試験を実施するとしましょう。 （質問の部分に合格する可能性が10％の）勉強しなかった生徒は、幸運な生徒ではなく、正解できない追加の試験を受けます。 なぜ生徒は授業に落第したのでしょうか？ それは、生徒が勉強しなかったからです。 シナリオ2： 先生は、100回のうち2回というように、稀にしか試験を実施しないとしましょう。 試験の勉強をしていない生徒でも、試験が実施される可能性が低いため、授業に合格する可能性が高いと予測されます。 ある生徒は試験の準備をしていないので、10％の確率でしか試験に合格できません。 そして、不運にも、先生は彼が答えられない追加の試験を実施したので、授業に落第しました。 彼はなぜ落第してしまったのでしょうか？ この場合、より適した説明は「先生が生徒に対して試験を実施したから」だと言えます。 先生が試験を実施する可能性は低いので、先生は普通ではない行動をしたと言えます。 解釈可能な機械学習に対する意味： 予測の入力特徴の1つが（カテゴリ特徴のうち、ごく稀なカテゴリなど）何らかの意味で異常であり、その特徴が予測に影響を与えた場合、たとえ他の「正常な」特徴が予測に対して同じ影響を与えたとしても、説明に含めるべきです。 住宅価格予測の例の異常な特徴として、極めて高価な家には2つのバルコニーがあることかもしれません。 2つのバルコニーがあるということが、いくつかの方法で、家の大きさ、良好な近隣住民、または最近リフォームされたことが同じくらい価格差に寄与していることがわかったとしても、異常な特徴である「2つのバルコニー」という特徴は、その家が高価格となっている理由の最良の説明となるでしょう。 説明は真実。 適切な説明は実際に（他の状況で）真実であることを証明します。 しかし気がかりなことに、これは「適切な」説明にとって最も重要な要素ではありません。 例えば、選択性は真実性よりも重要であると考えられます。 たった1つか2つの原因のみによる説明が、関連する原因全てを網羅することはめったにありません。選択性は一部の真実を省略します。 例として、1つか2つの要因だけで株式市場の暴落を引き起こすことはありません。真実としては、何百万もの人々が最終的に暴落を引き起こす行動をとるように影響を与えた何百万もの原因があるのです。 解釈可能な機械学習に対する意味： 説明は出来事をできるだけ正確に説明する必要があります。これは、機械学習において忠実性 (fidelity)と呼ばれることがあります。したがって、2つのバルコニーが家の価格を上げると主張する場合、それは他の家（あるいは少なくとも同じような家）にも当てはまるべきです。 人間にとって、説明の忠実性は、その選択性、対照性、社会的側面ほど重要ではありません。 よい説明は、説明対象者の事前の信念と一貫している。 人間は、自分が信じていることと矛盾する情報を無視する傾向があります。 この効果は確証バイアスと呼ばれます (Nickerson 199812)。 説明はこの種の偏見から免れることができません。 人間は自分の信念と矛盾する説明を軽んじたり無視したりする傾向があります。 信念は人によって異なりますが、政治的世界観など集団に基づくものもあります。 解釈可能な機械学習に対する意味： よい説明は人間が信じていることと一貫しています。 これを機械学習に統合するのは難しく、統合できたとしても、おそらく予測性能を大幅に損なうことになります。 家の大きさが予測価格に与える影響について、我々は家が大きいほど価格が高くなると信じています。 いくつかの家の予測価格に対して、家のサイズがマイナスの影響を与えていたとモデルが示していたとします。 モデルは予測性能を向上させるために、（いくつかの複雑な相互作用の影響によって）このように学習しましたが、この振る舞いは我々が信じていることと強く矛盾しています。 （特徴量が一方向の予測にのみ影響を与えるような）単調性制約を適用するか、この性質を持つ線形モデルなどを使用できます。 適切な説明は一般的でもっともらしいものである 多くの出来事を説明できる原因は非常に一般的であり、よい説明と見なされます。 ただし、これは普通でない原因がよい説明であるという主張と矛盾していることに注意してください。 著者の考えでは、異常な原因による説明は一般的な原因による説明を上回っています。 異常な原因は定義上、特定の文脈や状況において稀なものです。 異常な出来事がない場合、一般的な説明はよい説明であると見なされます。 人間は同時に起こる出来事の確率を誤解する傾向があることを忘れないでください。 （Joeは司書です。彼は恥ずかしがり屋である可能性が高いですか、それとも本を読むのが好きな恥ずかしがり屋である可能性が高いですか？） 「家が大きいのでその価格は高い」というのは分かりやすい例です。これは非常に一般的で、家が高いあるいは安い理由をよく説明しています。 解釈可能な機械学習に対する意味： 一般性は、特徴量のサポートによって簡単に測定できます。これは、説明が適用されるインスタンスの数をインスタンスの総数で割ったものです。 Lipton, Peter. &quot;Contrastive explanation.&quot; Royal Institute of Philosophy Supplements 27 (1990): 247-266.↩ Kahneman, Daniel, and Amos Tversky. &quot;The Simulation Heuristic.&quot; Stanford Univ CA Dept of Psychology. (1981).↩ Štrumbelj, Erik, and Igor Kononenko. &quot;A general method for visualizing and explaining black-box regression models.&quot; In International Conference on Adaptive and Natural Computing Algorithms, 21–30. Springer. (2011).↩ Nickerson, Raymond S. &quot;Confirmation Bias: A ubiquitous phenomenon in many guises.&quot; Review of General Psychology 2 (2). Educational Publishing Foundation: 175. (1998).↩ "],["data.html", "Chapter 3 データセット", " Chapter 3 データセット この本を通して、全てのモデルや手法は、オンライン上に無料で公開されているデータセットに対して適用されています。 それぞれのタスク(クラス分類、回帰、テキスト分類)で様々なデータセットを使用しています。 "],["bike-data.html", "3.1 自転車レンタル (回帰)", " 3.1 自転車レンタル (回帰) このデータセットには、ワシントンDCにある自転車レンタル会社の、日毎の自転車の貸し出し数が含まれていて、それに加え、天気と季節の情報があります。 データはCapital-Bikeshareによって、親切にももすべての人が使用できるように作られました。Fanaee-T and Gama (2013)13によって、天気と季節の情報が加えられました。目標は、季節や日毎に自転車がどれほど貸し出されるのかを予測することです。データは、 UCI Machine Learning Repositoryからダウンロードできます。 新しい特徴量がデータセットに追加されました、しかし、この本では全ての特徴量を例として使っている訳ではありません。 以下に今回使われた特徴量の一覧を記しておきます。 自転車のレンタル台数は回帰の問題の中でターゲット（目標）として使われています。 自転車のレンタル台数は未登録ユーザと登録されたユーザを含んでいます。 季節(春、夏、秋、冬) その日が祝日であったかどうか 年(2011年または2012年) 2011年の1月1日（この日がデータセットの中で最初の日）からの日数 この特徴量は時間変化に関するトレンドを考慮するために導入されました。 その日が平日であったか週末であったか その日の天候。下記の情報をそれぞれ1つずつ 晴天、少々曇り、少しだけ曇り、曇り 霧＋雲、霧＋所々曇り、霧＋少しの雲、小雨＋さざれ雲 豪雨＋凍雨＋雷雨＋霧、雪＋霧 気温（摂氏） 比較的温度（0 - 100％） 風速（km/h） この本で使用する具体例のために、データは少し処理されています。データ処理のためのRのスクリプトは、GitHubのリポジトリで取得できます。処理後のデータは、ここから取得できます。 Fanaee-T, Hadi, and Joao Gama. &quot;Event labeling combining ensemble detectors and background knowledge.&quot; Progress in Artificial Intelligence. Springer Berlin Heidelberg, 1–15. doi:10.1007/s13748-013-0040-3. (2013).↩ "],["spam-data.html", "3.2 YouTube スパムコメント (テキスト分類)", " 3.2 YouTube スパムコメント (テキスト分類) テキスト分類の例として、5つのYouTubeビデオについた1956件のコメントを用います。ありがたいことに、このデータセットを使用したスパム分類に関する論文の著者がデータを自由に利用可能な状態にしてくれました。(Alberto, Lochter, and Almeida (2015)14) コメントは2015年の上半期の再生回数上位10件のうちの5つからYouTube APIを用いて収集されました。5件すべてはミュージックビデオで、そのうちの1つは、韓国のアーティスト Psy による ”カンナムスタイル” です。その他のアーティストは、Katy Perry, LMFAO, Eminem, Shakira です。 コメントのいくつかを見てみましょう。 コメントは、手動でスパムかそうでないかラベルがつけられています。 スパムは 1 というラベルで表され、スパムでない場合は 0 で表されます。 CONTENT CLASS Huh, anyway check out this you[tube] channel: kobyoshi02 1 Hey guys check out my new channel and our first vid THIS IS US THE MONKEYS!!! I'm the monkey in the white shirt,please leave a like comment and please subscribe!!!! 1 just for test I have to say murdev.com 1 me shaking my sexy ass on my channel enjoy ^_^ 1 watch?v=vtaRGgvGtWQ Check this out . 1 Hey, check out my new website!! This site is about kids stuff. kidsmediausa . com 1 Subscribe to my channel 1 i turned it on mute as soon is i came on i just wanted to check the views... 0 You should check my channel for Funny VIDEOS!! 1 and u should.d check my channel and tell me what I should do next! 1 実際にYouTube にアクセスしてコメントを確認できます。 しかし、YouTube の地獄に巻き込まれて、猿がビーチで観光客からカクテルを盗んで飲む動画を見ないようにしてください。 Google のスパム検知は 2015 年から大きく変更されているかもしれません。 視聴率記録を更新した「Gangnam Style」の動画はこちら. もしこのデータで遊びたいのならば、この本のGitHubレポジトリに RDataファイル があります。また、便利な関数はここR-script にあります。 Alberto, Túlio C, Johannes V Lochter, and Tiago A Almeida. &quot;Tubespam: comment spam filtering on YouTube.&quot; In Machine Learning and Applications (Icmla), Ieee 14th International Conference on, 138–43. IEEE. (2015).↩ "],["cervical.html", "3.3 子宮頸がんのリスク要因(クラス分類)", " 3.3 子宮頸がんのリスク要因(クラス分類) 子宮頸がんデータセットは、女性が子宮頸がんにかかるか否かを予測するための指標とリスク要因を含んでいます。 特徴量は、人口統計データ(年齢など)、生活スタイル、病歴を含みます。 データは Fernandes, Cardoso, Fernandes(2017)15 によって作成されており、UCI機械学習リポジトリ からダウンロードできます。 この本で例として使われる特徴量のサブセットは以下のとおりです。 年齢 性交渉の相手の人数 初めての性交渉の年齢 妊娠の回数 喫煙の有無 喫煙の継続年数 ホルモン避妊薬の使用有無 ホルモン避妊薬の使用年数 子宮内避妊器具(IUD)の使用有無 子宮内避妊器具(IUD)の使用年数 性感染症(STD)の感染歴の有無 STDの診断数 最初にSTDと診断された時点 最後にSTDと判断された時点 生検結果「健康」または「がん」(目的変数) 生検は子宮頸がん診断において精度が高いため、広く容認された手法として用いられます。 この本の例では、生検結果は目的変数として使われます。 各列での欠損データは、値が無いということ自体が確率に相関性を持ち得るため良くない方法かもしれませんが、最頻値（最も頻繁に登場する値）で補完されます。 質問は非常にプライベートな性質のものであるため、バイアスがあるかもしれません。 しかしこの本は欠損値の補完に関する本ではないため、最頻値補完は例としては十分でしょう。 このデータセットのこの本での例を再現するには、この本のGitHubリポジトリ内を探して下さい。 前処理のR-scriptと最終のRDataファイル Fernandes, Kelwin, Jaime S Cardoso, and Jessica Fernandes. &quot;Transfer learning with partial observability applied to cervical cancer screening.&quot; In Iberian Conference on Pattern Recognition and Image Analysis, 243–50. Springer. (2017).↩ "],["simple.html", "Chapter 4 解釈可能なモデル", " Chapter 4 解釈可能なモデル 解釈可能性を手に入れる最も簡単な方法は、 解釈可能なモデルを作るようなアルゴリズムのみを使うことです。 線形回帰や、ロジスティック回帰、決定木は一般的に用いられる解釈可能なモデルです。 以下の章ではこれらのモデルについて紹介します。 既に多くの本や動画、チュートリアル、論文、その他閲覧可能なものが存在するので、細部には拘らず、基本的なことのみに留めます。 特に、モデルの解釈方法に焦点を当てます。 本書では線形回帰、ロジスティック回帰、その他の線形回帰の拡張、決定木、決定規則、RuleFitについてより詳細に記述されています。 また、その他の解釈可能なモデルもいくつか挙げています。 本書で説明されている全ての解釈可能なモデルは、k近傍法を除いてモジュールレベルで解釈可能です。 後述の表は解釈可能なモデルの種類と特性を示しています。 特徴量と目的変数との関係が線形にモデル化されている場合，そのモデルは線形であると言います。 単調制約を持つモデルは、特徴量の全域にわたって、特徴量と目的変数が常に同じ方向に変化することを保証します。 特徴量が増加すると、目的変数は常に増加する、または減少するかのどちらかになります。 単調性は関係性を理解しやすくするため、モデルの解釈に有用です。 一部のモデルでは、目的変数の出力を予測するための特徴量間の交互作用を自動的に含めることができます。 手動で特徴量の交互作用を含めることで、あらゆる種類のモデルに交互作用を含めることができます。 交互作用項を導入することで、予測性能を改善できますが、多すぎるもしくは複雑すぎる交互作用は解釈性を損なう可能性があります。 いくつかのモデルは、回帰または、分類のみであり、どちらも対応可能なモデルもあります。 この表から、回帰 (regr) もしくは分類 (class) のタスクに対して、適切な解釈可能モデルを選べます。 アルゴリズム 線形性 単調性 交互作用 タスク 線形回帰 あり あり なし regr ロジスティック回帰 なし あり なし class 決定木 なし いくつか あり class, regr RuleFit あり なし あり class, regr 単純ベイズ法 なし あり なし class k近傍法 なし なし なし class, regr "],["limo.html", "4.1 線形回帰", " 4.1 線形回帰 線形回帰モデルは予測値を特徴量の重み付き和として表します。 学習された関係の線形性が解釈を簡単にしてくれます。 線形回帰モデルは長い間、定量的な問題に取り組む統計学者や計算機科学者たちによって使用されてきました。 線形モデルでは、特徴量 x が目的変数 y にどれくらい依存するかをモデリングできます。 学習された関係は線形で、1つのデータ i に対して次のように表すことができます。 \\[y=\\beta_{0}+\\beta_{1}x_{1}+\\ldots+\\beta_{p}x_{p}+\\epsilon\\] 予測結果は p 個の特徴量の重み付き和です。 \\(\\beta_{j}\\) は学習された特徴の重み、つまり係数を表します。 1つ目の重み (\\(\\beta_0\\)) は切片と呼ばれ、特徴量と乗算はしません。 \\(\\epsilon\\) は予測と実際の結果との差、つまり誤差です。 これらの誤差値はガウス分布に従うと仮定されます。誤差が正の方向にも負の方向にも存在して、小さい誤差は多く、大きい誤差は少ないと仮定するという意味です。 最適な重みを推定するために様々な方法が用いられます。 実際の結果と推定結果との差の二乗和を最小化するような重みを求めるために、最小二乗法がよく用いられます。 \\[\\hat{\\boldsymbol{\\beta}}=\\arg\\!\\min_{\\beta_0,\\ldots,\\beta_p}\\sum_{i=1}^n\\left(y^{(i)}-\\left(\\beta_0+\\sum_{j=1}^p\\beta_jx^{(i)}_{j}\\right)\\right)^{2}\\] 具体的に最適な重みがどう求められるかはここでは論じませんが、興味があれば、&quot;The Elements of Statistical Learning&quot; (Friedman, Hastie and Tibshirani 2009)16 の 3.2 章や他の線形回帰に関するオンライン資料を参考にしてください。 線形回帰の最大の利点は線形性です。 線形性により予測手順が簡単になります。最も重要なことは、これらの一次方程式がモジュールレベル（つまり重み）で理解しやすくなります。 これが、線形モデルおよび同様のモデルが、医学、社会学、心理学の学術分野および他の多くの定量的研究分野で広く普及している主な理由の 1つです。 例えば、医療分野では、患者の臨床成果の予測だけでなく、薬の影響を定量化し、同時に性別、年齢、その他の特徴を解釈可能な方法で考慮することが重要です。 推定される重みには信頼区間があります。 信頼区間とは、ある信頼度で「真」の重みを含むような重み推定の範囲です。 例えば、重み2に対する信頼度 95％ の信頼区間は1から3の範囲になる可能性があります。 この信頼区間は、「回帰モデルが与えられるデータに正しいという仮定のもとで、新しくサンプリングされたデータに対して推定を 100 回繰り返したときに、この信頼区間は 100 回中 95 回真の重みを含む」と解釈できます。 モデルが「正しい」かはデータ中の関係が、線形性、正規性、等分散性、独立性、固定された特徴量、また、多重共線性の欠如 というこれらの仮定を満たしているかどうかによります。 線形性 線形回帰モデルは予測が特徴量の線形結合になることを要請しますが、これは線形回帰の最大の強みでもあり、最大の制約でもあります。 線形性は解釈可能なモデルに繋がります。 線形効果は定量化しやすく、説明しやすいです。 線形的に足されるため、効果を分離することも簡単です。 特徴量の相互作用や、特徴量と目的値の間の非線形性が疑われる場合は、相互作用項を追加するか、スプライン回帰を使うことができます。 正規性 与えられた特徴量に対する目的値は正規分布に従うと仮定されます。 もしこの仮定が破れると、特徴量に対する重みの推定信頼区間は意味を持たなくなります。 等分散性 誤差項の分散は、特徴空間全体において一定であると想定されます。 平方メートルで与えられる居住面積に基づいて住宅の価格を予測するとします。 家の大きさに関わらず、予測値における誤差の分散が一定であると想定する線形モデルを作ります。 この仮定は、実際には現実に反します。 住宅の例でいうと、家の価値が高いほど価格変動の余地が大きく、価格の予測値における誤差項の分散が大きくなるかもしれません。 線形回帰モデルの平均誤差（予測価格と実際の価格との差）が 50,000 ユーロであるとしましょう。 等分散性を想定すると、平均 50,000 の誤差が 100 万ユーロの家と 4 万ユーロの家とに対して同じであるとすることになります。 これは家の価格がマイナスになりうるということになり、合理的ではありません。 独立性 各データは互いに独立であることが想定されます。 患者ごとに血液検査を複数回行うなど、繰り返して測定する場合、データ点は互いに独立になりません。 従属なデータに対しては混合効果モデルや GEE などの特殊な線形回帰モデルが必要です。 このような場合に、「標準的な」線形回帰モデルを使ってしまうと、モデルから誤った結論を導き出してしまう恐れがあります。 固定された特徴量 入力特徴量は「固定されている」と見なします。 固定されているとは、入力特徴量が統計的な変数でなく、「与えられた定数」として扱われるということです。 これは測定誤差がないことを意味します。 これは、かなり非現実的な仮定です。 しかし、この仮定なしでは、入力特徴の測定誤差を説明するために非常に複雑な測定誤差モデルを使う必要があります。 通常、そんなことはしたくないでしょう。 多重共線性の欠如 強い相関関係にある特徴量は、重みの推定を台無しにするので望ましくありません。 2つの特徴量が強く相関している場合、特徴量の効果が相加的であり、相関する特徴量のどちらが影響を与えているのか不定となるため、重みを推定するときに確定できません。 4.1.1 解釈 線形回帰モデルの重みの解釈は、対応する特徴量のタイプに依存します。 量的特徴量 (Numerical feature): 量的特徴量の値が1単位分増えると、推定結果が重み分変化します。量的特徴量の一例としては、家の大きさが挙げられます。 バイナリ特徴量 (Binary feature): それぞれのインスタンスごとに、2つの値のどちらか一方のみを取る特徴量のことです。 そのような特徴量の一例として、庭付きの家であるかどうかが挙げられます。値のうちの1つは&quot;庭付きでない&quot;のような参照カテゴリ（プログラミング言語によっては 0 と符号化されます）として数えられます。ある特徴量の参照カテゴリが別のカテゴリに変化すると、推定結果は特徴量の重み分変化します。 複数のカテゴリを含んだカテゴリカル特徴量: 取りうる値の数が固定された特徴量のことです。一例として、&quot;カーペット&quot;、&quot;ラミネート&quot;、&quot;寄木細工&quot;というカテゴリを取りうる”床の種類”という特徴量が挙げられます。 多くのカテゴリを扱う解決策の1つとして、one-hot エンコーディングというものがあります。one-hot エンコーディングとは、それぞれのカテゴリごとに 0, 1 の値を取るカラムを設定するエンコーディングの手法です。 L 個のカテゴリがあるカテゴリカル特徴量に対しては、L-1 個のカラムがあれば十分です。なぜならば、L 番目のカラムは冗長な情報だからです（例えば、ある1つのインスタンスに対して 1 番目から L-1 番目のカラムが全て 0 であるとき、そのインスタンスのカテゴリは L 番目のものであることが分かるからです）。 各カテゴリに対する解釈はバイナリ特徴量に対する解釈と同じです。 R のような言語では、この章で後ほど説明するような様々な方法でカテゴリカル特徴量をエンコードすることが出来ます。 切片\\(\\beta_0\\): 切片は、全てのインスタンスについて常に1をとる”定数特徴量”に対する重みであるとみなすことが出来ます。 大抵のソフトウェアパッケージは切片を推定するために定数特徴量を自動的に追加します。 これに対する解釈は以下の通りです。 全ての量的特徴量が0であり、カテゴリカルデータの値が参照カテゴリであるインスタンスの場合には、モデルの予測値は切片の重みになります。 そのような、全ての特徴量の値が0であるようなインスタンスは多くの場合意味をなさないため、切片の解釈は通常関係がありません。 切片の解釈は、全ての特徴量が平均0、標準偏差1に標準化されているときにのみ意味があるものとなります。 そのような場合、切片は全ての特徴量が平均値であるようなインスタンスの予測結果を反映しています。 線形回帰モデルの特徴量の解釈は以下のテンプレートを用いて自動化できます。 量的特徴量の解釈 特徴量 \\(x_{k}\\) が 1 だけ増えて、その他の特徴量は固定されている場合、予測結果 y は \\(\\beta_k\\) だけ増えます。 カテゴリカル特徴量の解釈 特徴量 \\(x_{k}\\) が参照カテゴリから他のカテゴリに変化し、その他の特徴量は固定されている場合、予測結果 y は \\(\\beta_{k}\\) だけ増えます。 もう1つの重要な指標は、決定係数 \\(R^2\\) です。 \\(R^2\\) はモデルによって、どの程度、目的値の全てのばらつきが説明されているかを教えてくれます。 \\(R^2\\) が高くなればなるほど、そのモデルはデータを説明していることになります。 \\(R^2\\) の計算方法は以下の通りです。 \\[R^2=1-SSE/SST\\] SSE は、二乗和誤差 (Squared Sum of the Error)です。 \\[SSE=\\sum_{i=1}^n(y^{(i)}-\\hat{y}^{(i)})^2\\] SST は、データの分散の二乗和です。 \\[SST=\\sum_{i=1}^n(y^{(i)}-\\bar{y})^2\\] SSE は線形回帰で学習した後、どの程度のばらつきが残っているかを教えてくれます。これは、予測と実際の目的値の間の二乗誤差を計測することで求めます。 SST はデータ自体の目的値の全体の分散です。 \\(R^2\\) は、線形モデルによって、どの程度ばらつきが説明できているのかを教えてくれます。 \\(R^2\\) は 0 のとき、モデルはデータを全く説明できていないのに対して、1 のときは、データのばらつきを完全に説明できていることを意味します。 実は、たとえ目的値に対する情報を全く含んでいない特徴量を付け加えたとしても、モデルの特徴量の数を増やすと \\(R^2\\) の値を増加させることができます。 そのため、モデルで使われている特徴量の数を反映した、自由度調整済みの決定係数 \\(R^2\\) を使用することが推奨されます。 その計算方法は次のようになります。 \\[\\bar{R}^2=1-(1-R^2)\\frac{n-1}{n-p-1}\\] ただし、 p は特徴量の数で、n はインスタンスの数です。 (自由度調整済み)決定係数 \\(R^2\\) がとても低いモデルに対しては、そもそもモデルがデータをうまく説明できていないため、そのモデルの解釈をしても意味がありません。 どのように重みの解釈しても、意味がないでしょう。 特徴量重要度 (Feature Importance) 線形回帰モデルの特徴量の重要度は t 統計量の絶対値で計測できます。 t 統計量は標準誤差でスケーリングされた推定重みです。 \\[t_{\\hat{\\beta}_j}=\\frac{\\hat{\\beta}_j}{SE(\\hat{\\beta}_j)}\\] この式は何を意味しているのでしょうか。 特徴量の重要度は、重みが増えると増加します。 これは理にかなっています。 推定された重みに、よりばらつきがあると (正しい値に関しての確証が小さくなる)、その特徴量に対する重要度は小さくなります。 これも理にかなっています。 4.1.2 例 この例では、線形回帰モデルを使って、天気や日付情報が与えられたときの自転車レンタル台数 の予測をします。 解釈のために、回帰の重みについて調べます。 特徴量は、量的特徴量、カテゴリカル特徴量のどちらもがあります。 各特徴に対して、推定された重み、推定値の標準誤差 (SE)、及び t 統計量の絶対値(|t|)を表に示します。 Weight SE |t| (Intercept) 2399.4 238.3 10.1 seasonSUMMER 899.3 122.3 7.4 seasonFALL 138.2 161.7 0.9 seasonWINTER 425.6 110.8 3.8 holidayHOLIDAY -686.1 203.3 3.4 workingdayWORKING DAY 124.9 73.3 1.7 weathersitMISTY -379.4 87.6 4.3 weathersitRAIN/SNOW/STORM -1901.5 223.6 8.5 temp 110.7 7.0 15.7 hum -17.4 3.2 5.5 windspeed -42.5 6.9 6.2 days_since_2011 4.9 0.2 28.5 量的特徴量の解釈について (temperature): 摂氏が1度上昇したとき、他の特徴量は全て同じとすると、自転車のレンタル台数の予測は 110.7 だけ増加します。 カテゴリカル特徴量の解釈について (weathersit): 自転車のレンタル数の予測値は、天気の良い日と比べて、雨や雪や嵐のとき -1901.5 だけ変化します。ただし、このときも他の特徴量は変化しないことを想定しています。 晴れた日に比べて、霧が濃い時は、自転車のレンタル台数の予測値は -379.4 だけ変化します。 全ての解釈では必ず、&quot;他の特徴量は変化させない&quot;という脚注がついています。 これは、線形回帰モデルの性質です。 予測値は、重み付けされた特徴量の線形和です。 推定された線形方程式は特徴量/予測値空間の超平面です (単一の特徴量の場合単純な直線になります)。 重みは各方向の超平面の傾き(勾配)を指定しています。 良い点としては、加法性によって個々の特徴量の効果を他の全ての特徴量から分離できることです。 これが可能なのは、全ての特徴量の効果(重みと特徴量の積)が和によって組み合わされているからなのです。 悪い点としては、この解釈は特徴量の同時分布を無視していることです。 ある特徴量を増やすが、他の特徴量は変化させないとき、非現実的か好ましくないデータが得られるかもしれません。 例えば、家の大きさを広くすることなく、部屋の数を増加させることは現実的ではないでしょう。 4.1.3 可視化による解釈 種々の可視化手法を用いることで、線形回帰モデルを簡単かつ素早く把握できます。 4.1.3.1 重みプロット (Weight Plot) 重みの情報（重みと分散の推定値）は重みプロットを用いることで簡単に可視化できます。 以下のプロットでは、先ほどの線形回帰の結果を可視化しています。 FIGURE 4.1: 重みが点として、95%信頼区間が線として表示されている。 重みプロットは、自転車のレンタル台数を予測するにあたって、雨、雪、嵐といった天気は大きな負の影響を及ぼすことを示しています。 稼働日数の重みはほぼ 0 になっており、かつ 95% 信頼区間に 0 は含まれています。従って、この特徴量は統計的に重要でないことが示されます。 気温についても同様のことが言える可能性があります。 重みプロットにおける問題は、各特徴量が異なるスケールで計測されているということです。 天気の推定された重みは晴れ、雨、嵐、雪のそれぞれの天気の違いを反映していますが、気温は 1 度の変化による影響を反映しています。 特徴をスケーリングすることで、線形回帰モデルを学習させる前に、推定された重みを比較できるようになります。（例えば標準化など） 4.1.3.2 影響力プロット (Effect Plot) 線形回帰モデルの重みは特徴量と掛け合わせることで、より意味のある分析になり得ます。重みは特徴量のスケールに依存し、例えば身長の単位をメートルからセンチメートルに変更すればまた異なる重みになります。 ただ、重みは変化したとしても、重みが実際にデータに及ぼす影響は変化しません。 また特徴量の分布を知ることは重要です。なぜなら、ある特徴量の分散が非常に低い場合、ほとんどのインスタンスがその特徴量から同じような影響を受けることを意味するからです。 影響力プロットは、重みと特徴の組み合わせがどれほど予測に貢献しているかを理解する助けになります。 影響力を計算するのは、各特徴ごとの重みと、特徴量をインスタンスごとに掛け合わせます。 \\[\\text{effect}_{j}^{(i)}=w_{j}x_{j}^{(i)}\\] 影響力は箱ひげ図を用いて可視化できます。 箱ひげ図内の各箱は、全データのうち半分のデータに対する影響力の範囲を表します（第1四分位点から第3四分位点まで）。箱内の縦線は中央値を表します。つまり、予測時に半数のインスタンスはこれよりも低い影響力をもち、もう半数は高い影響力を持つということです。 横線は \\(\\pm1.5\\text{IQR}/\\sqrt{n}\\) まで引かれていて、IQR は四分位範囲（inter quartile range）（第3四分位数から第1四分数を引いたもの）のことです。 FIGURE 4.2: 特徴量の影響力プロットは、特徴量ごとのデータに対する影響 (特徴量と重みの積) の分布を示している。 図から、自転車のレンタル台数の予測に大きく寄与している特徴量は、気温と日数の特徴であることがわかります。この特徴量は、自転車レンタルのトレンドを捉えていると言えます。 気温が予測にどれだけ寄与しているかの範囲はかなり広くなっています。日数の特徴量は 0 から大きな正の値に渡って寄与していることがわかります。これは、データセット内での初日（2011/1/1）ではトレンドの影響は非常に小さく、かつ推測された重みは正の値 (4.93) だったからです。これが意味するのは、影響力は毎日上昇し続け、データセット内の最終日（2013/12/31）に最大値を迎えるということです。 重みが負の値のとき、あるインスタンスが正の影響力を持っていたとすると、特徴量が負であるということに注意してください。 例えば、風速が大きく負の影響力を持っているような日は、風速がかなり強い日であるということです。 4.1.4 個々の予測に対する説明 あるインスタンスの各特徴量はどれだけ予測に貢献したのでしょうか。 この疑問への回答はインスタンスに対する効果を計算することで得られます。 インスタンスに固有の効果の解釈は各特徴量に関する効果の分布を比較することでのみ意味を持ちます。 自転車データセットの6番目のインスタンスに対する線形モデルの予測について説明します。 このインスタンスは以下の特徴量をもっています。 Feature Value season SPRING yr 2011 mnth JAN holiday NO HOLIDAY weekday THU workingday WORKING DAY weathersit GOOD temp 1.604356 hum 51.8261 windspeed 6.000868 cnt 1606 days_since_2011 5 このインスタンスの特徴量の効果を知るため、その特徴量と、それに対応する線形回帰モデルの重みの積を求めなくてはなりません。 特徴量 &quot;workingday&quot; の値 &quot;WORKING DAY&quot; に対する効果は 124.9 となります。 気温 1.6 度の効果は 177.6 です。 これらの個々の効果をデータ全体への効果の分布を示す図にX印としてプロットします。 これにより個々の効果とデータ全体の効果が比較できます。 FIGURE 4.3: 1つのインスタンスに対する影響力プロットは、影響力の分布を示し、興味のあるインスタンスの効果のハイライトする。 訓練データのインスタンスに対する予測を平均すると、4504 の平均となります. それと比較すると、6 番目のインスタンスによる自転車の台数予測は 1571 しかないことから、小さいといえます。 影響力プロットはその理由を明らかにしています。 箱ひげ図はデータセットにおける全インスタンスの効果分布を表し、X印は 6 番目のインスタンスの効果を示しています。 6 番目のインスタンスは、この日の気温が 2 度であり、その他のほとんどの日と比べて気温が低いことから、気温による影響は小さいと言えます ( 気温に対する重みは正であることに注意)。 また、トレンド特徴量である &quot;days_since_2011&quot; の効果も、このインスタンスが2011初頭(5 days) のものであることと、トレンドに対する重みが正であることから、影響力は小さいと言えます。 4.1.5 カテゴリカル特徴量のエンコーディング カテゴリカル特徴量をエンコーディングする方法は複数あり、選択した方法によって重みの解釈に影響が生じます。 線形回帰モデルにおいて標準となるのは treatment coding で、これは多くの場合で十分に機能します。 異なるエンコーディングの手法は1つのカテゴリカル特徴量から異なる計画行列を作り出すことに相当します。 この節では3つの異なるエンコーディングの方法について紹介しますが、他にも多くの手法が知られています。 使用する例では、6つのインスタンスと3つのカテゴリを持つカテゴリカル特徴量を持ちます。 最初の2つのインスタンスでは、特徴量はカテゴリAを取ります。 3番目、4番目のインスタンスでは、カテゴリーBを取り、最後2つのインスタンスはカテゴリーCを取ります。 Treatment coding treatment coding では、カテゴリごとの重みは、対応するカテゴリと参照カテゴリ間の予測の差の推定値とします。 線形モデルの切片は参照カテゴリの平均値です。(他の全ての特徴量が変わらない場合) 計画行列の最初の列は切片で常に 1 となります。 2 列目はインスタンス i がカテゴリ B かどうかを示しており、3 列目はインスタンスがカテゴリ C かどうかを示しています。 線形方程式が過剰になり、重みに対して一意な解を見つけることができなくなるため、カテゴリ A に対する列は必要ありません。 インスタンスがカテゴリ B にも C にも属していないことがわかれば十分なのです。 Feature matrix: \\[\\begin{pmatrix}1&amp;0&amp;0\\\\1&amp;0&amp;0\\\\1&amp;1&amp;0\\\\1&amp;1&amp;0\\\\1&amp;0&amp;1\\\\1&amp;0&amp;1\\\\\\end{pmatrix}\\] Effect coding カテゴリごとの重みは、対応するカテゴリと全体の平均 (他のすべての特徴がゼロまたは参照カテゴリである場合) の推定された y の差です。 最初の列は切片を推定するために用いられます。 切片に関連づけられた重み \\(\\beta_{0}\\) は、全体の平均を表し、2 列目に対する重み \\(\\beta_{1}\\) は、 全体の平均とカテゴリ B との間の差となります。 したがって、カテゴリ B の全体の効果は \\(\\beta_{0}+\\beta_{1}\\) となります。 カテゴリ C に対する解釈も同様です。 参照カテゴリ A に対しては、\\(-(\\beta_{1}+\\beta_{2})\\)が全体の平均を表し、\\(\\beta_{0}-(\\beta_{1}+\\beta_{2})\\)が全体の影響となります。 Feature matrix: \\[\\begin{pmatrix}1&amp;-1&amp;-1\\\\1&amp;-1&amp;-1\\\\1&amp;1&amp;0\\\\1&amp;1&amp;0\\\\1&amp;0&amp;1\\\\1&amp;0&amp;1\\\\\\end{pmatrix}\\] Dummy coding カテゴリごとの \\(\\beta\\) は各カテゴリに対する推定された y の平均値です。（その他の特徴量はゼロもしくは、参照カテゴリ） ただし、切片は、線形モデルの重みに対して一意な解が見つけられるように、省略されていることに注意してください。 Feature matrix: \\[\\begin{pmatrix}1&amp;0&amp;0\\\\1&amp;0&amp;0\\\\0&amp;1&amp;0\\\\0&amp;1&amp;0\\\\0&amp;0&amp;1\\\\0&amp;0&amp;1\\\\\\end{pmatrix}\\] カテゴリカル特徴量に対するエンコーディング手法にさらに興味がある場合は次のサイトをご覧ください。 概要サイト、 ブログ. 4.1.6 線形モデルは良い説明を与えるか? 人間に優しい説明の章で説明した、良い説明とは何かという観点で見ると、線形モデルは最良の説明を与えるというわけではありません。 これらは対照的ですが、参照インスタンスは全ての量的特徴量が0でありカテゴリカル特徴量は参照カテゴリであるようなデータ点となります。 これは大抵、現実的には起こりそうもない人工的で意味のないインスタンスです。 例外もあります： すべての量的特徴量が平均化（特徴量から特徴量の平均値を引いた値）されており、すべてのカテゴリ特徴量がエンコードされている場合、参照インスタンスは、すべての特徴量が平均値を取るデータ点となります。 これは実在しないデータ点かもしれませんが、少なくとも可能性が高く意味があるものかもしれません。 この場合、特徴量に重みを掛けたもの（feature effects）は、&quot;平均的なインスタンス&quot;と比較したときの予測結果への貢献度を説明しています。 良い説明のもう1つの側面は選択性であり、線形モデルにはおいては、より少ない特徴量を使うこと、もしくはスパースな線形モデルを用いることが挙げられます。 線形モデル自体では選択性を持つ説明は達成できないことに注意してください。 線形モデルは、線形方程式が特徴量と出力結果の関係を表す適切なモデルであるかぎり、正しい説明を与えます。 非線形性や交互作用が多いほど、線形モデルは正確ではなくなり、説明も真実味の欠如が起こります。 線形性はモデルに対する説明を、より一般的に単純にします。 人々が関係性を説明するために線形モデルを使う主な理由は、モデルの線形の性質によるものだと考えられます。 4.1.7 スパースな線形モデル 私が選んだ線形モデルの例は、どれも上手くいきましたよね？ しかし実際のデータでは、ほんの一握りの特徴量ではなく、何百、何千の特徴量を持ってるかもしれません。 そのときに、線形回帰モデルはどうなるでしょうか。 解釈性は下がります。 インスタンスよりも特徴量が多く、標準的な線形モデルでは学習ができないという状況に陥ってしまうこともあるかもしれません。 このような時は、線形モデルにスパース性（＝少数の特徴量）を導入することで解決できます。 4.1.7.1 Lasso Lasso は、線形回帰モデルにスパース性を導入するための便利な方法です。 Lasso は「least absolute shrinkage and selection operator」の略で、線形回帰モデルに適用すると、特徴量の選択と選択された特徴量の重みの正則化を行います。 以下の重みを最適化する最小化問題を考えてみましょう。 \\[min_{\\boldsymbol{\\beta}}\\left(\\frac{1}{n}\\sum_{i=1}^n(y^{(i)}-x_i^T\\boldsymbol{\\beta})^2\\right)\\] Lasso は、この最適化問題に新しく項を付け加えます。 \\[min_{\\boldsymbol{\\beta}}\\left(\\frac{1}{n}\\sum_{i=1}^n(y^{(i)}-x_{i}^T\\boldsymbol{\\beta})^2+\\lambda||\\boldsymbol{\\beta}||_1\\right)\\] \\(||\\boldsymbol{\\beta}||_1\\) の項は、重みに対する L1 ノルムであり、大きな重みに対するペナルティの役割があります。 L1 ノルムを使用しているため、多くの重みは 0 となり、他の重みは小さくなります。 パラメータ \\(\\lambda\\) は正則化効果の強さを制御し、通常はクロスバリデーションによって調整されます。 特に \\(\\lambda\\) が大きいと、多くの重みが 0 になります。 特徴量の重みは、ペナルティ項 \\(\\lambda\\) の関数として可視化できます。 各特徴量の重みは、次の図のように曲線で表すことができます。 FIGURE 4.4: 重みに対するペナルティが大きくなるにつれて、非ゼロの重みを持つ特徴量が少なくなっていきます。これらの曲線は解パス図とも呼ばれます。プロットの上の数字は、非ゼロの重みの数です。 \\(\\lambda\\) にはどのような値を選ぶべきでしょうか？ ペナルティ項をチューニングできるパラメータとして捉えれば、クロスバリデーションでモデル誤差を最小化する \\(\\lambda\\) を求めることができます。 \\(\\lambda\\) をモデルの解釈性を制御するパラメータとして考えることもできます。 ペナルティが大きければ大きいほど、モデルに存在する特徴量が少なくなり（重みがゼロになるので）、モデルの解釈性が良くなります。 Lassoを使用した例 Lasso を使ってレンタル自転車の数を予測してみましょう。 モデルに持たせたい特徴量の数を事前に設定しておきます。 まずは特徴量の数を 2 に設定してみましょう。 Weight seasonSPRING 0.00 seasonSUMMER 0.00 seasonFALL 0.00 seasonWINTER 0.00 holidayHOLIDAY 0.00 workingdayWORKING DAY 0.00 weathersitMISTY 0.00 weathersitRAIN/SNOW/STORM 0.00 temp 52.33 hum 0.00 windspeed 0.00 days_since_2011 2.15 Lasso により重みが 0 にならなかったのは2つの特徴量は、温度（&quot;temp&quot;）と時間トレンド（&quot;days_since_2011&quot;）です。 では、次に、5つの特徴量を選択してみましょう。 Weight seasonSPRING -389.99 seasonSUMMER 0.00 seasonFALL 0.00 seasonWINTER 0.00 holidayHOLIDAY 0.00 workingdayWORKING DAY 0.00 weathersitMISTY 0.00 weathersitRAIN/SNOW/STORM -862.27 temp 85.58 hum -3.04 windspeed 0.00 days_since_2011 3.82 &quot;temp&quot; と &quot;days_since_2011&quot; の重みが、先に示した2つの特徴量を持つモデルとは異なることに注意してください。 この理由は、\\(\\lambda\\) を減少させることで、2つの特徴量を持つモデルで選択された特徴量であっても、ペナルティが少なくなり、より大きな重みが得られる可能性があるからです。 Lasso の重みの解釈は、線形回帰モデルの重みの解釈に対応しています。 重みに影響するため、特徴量が標準化されているかどうかに注意を払う必要があります。 この例では、特徴量はソフトウェアによって標準化されていますが、重みは元の特徴量の尺度と一致するように自動的に変換されています。 線形モデルのスパース性のための他の方法 線形モデルの特徴量の数を減らすために、さまざまな手法があります。 前処理に関する方法 手動による特徴量選択: 専門家の知識・ドメイン知識を使うことで、特徴量の選択ができます。 自動化できないのが大きな欠点で、データを理解している人と協力する必要があります。 単変量選択: 例としては、相関係数があります。 特徴量と目的変数の相関関係が一定の閾値を超えた特徴量のみを選択します。 デメリットは、特徴量を単体でしか考えていないことです。 いくつかの特徴量は、線形モデルが他の特徴量を説明するまで相関を示さないかもしれません。 こういった場合、単変量選択法では見逃してしまいます。 段階的な方法 Forward selection: 1つの特徴量で線形モデルをフィットします。 これを特徴量ごとに行います。 最も性能の良いモデルを選択してください (例: 決定係数 \\(R^2\\) が最大)。 ここでもう一度、残りの特徴量について、現在の最良のモデルに新たな特徴量を1つ追加することで、異なるバージョンのモデルを学習させます。 そして、最も性能の良いモデルを選びます。 この作業をモデル内の特徴量の最大数など、ある基準に達するまで続けましょう。 Backward selection: この手法は Forward selection と似ています。 しかし、特徴量を追加するのではなく、すべての特徴量を含むモデルから始めて、どの特徴を削除すれば最高の性能向上が得られるかを試してみましょう。 これを、ある停止基準に達するまで繰り返します。 Lasso を使うことをお勧めする理由は、すべての特徴量を同時に考慮し、\\(\\lambda\\) を変更することで制御できるからです。 また、Lassoは、分類のためのロジスティック回帰モデルでも使用できます。 4.1.8 長所 予測値を重み付き和としてモデル化することで、予測値がどのように生成されるかの透明性を高くできます。 そして、Lasso を使用することで、使用される特徴量の数を減らすことができます。 多くの人が線形回帰モデルを使います。 これは、多くの場所で、予測モデルや推論実行のために線形モデルが受け入れられていることを意味します。 線形回帰に関して、高度な経験をもつ専門家がいたり、教材やソフトウェアなども豊富にあります。 線形回帰はR、Python、Java、Julia、Scala、Javascript、その他多数で使用できます。 数学的には、重みを推定するのは簡単で、最適な重みを見つけることができることが保証されています（線形回帰モデルのすべての仮定がデータによって満たされている場合）。 重みと一緒に、信頼区間、検定、強固な統計理論を得ることができます。 線形回帰モデルの拡張もたくさん知られています（GLM,GAMなどの章を参照）。 4.1.9 短所 線形回帰モデルは、線形関係しか表現できません。 非線形性や交互作用を考慮するには、手作業で新たに特徴量を作成する必要があります。 線形モデルは、学習できる関係が非常に制限されており、実際には複雑な関係を単純化しすぎているため、予測性能に関してもあまり良くないことが多いです。 重みの解釈は、他のすべての特徴量に依存しているため、直感的ではない場合があります。 出力 y と他の特徴量に対して強い正の相関のある特徴量は、線形モデルにおいては、負の重みとなる可能性があります。なぜなら、他にも相関のある特徴量がある場合、高次元空間において y と負の相関があるためです。 完全に相関のある特徴量がある場合は、線形方程式の一意の解を見つけることが不可能になります。 例: 家の価値を予測するモデルがあり、部屋数や広さなどの特徴量があります。 家の大きさと部屋の数は非常に強い相関関係があります。つまり、家が大きければ大きいほど、部屋数が多くなります。 線形モデルに両方の特徴量を使用した場合、家のサイズがより良い予測指標となり、大きな正の重みを取得することが起こるかもしれません。 そうすると、同じ広さの家でも、部屋数を増やすと価値が下がったり、相関関係が強すぎると線形方程式が安定しなくなったりするので、部屋の数に対する重みは負になるかもしれません。 Friedman, Jerome, Trevor Hastie, and Robert Tibshirani. &quot;The elements of statistical learning&quot;. www.web.stanford.edu/~hastie/ElemStatLearn/ (2009).↩ "],["logistic.html", "4.2 ロジスティック回帰", " 4.2 ロジスティック回帰 ロジスティック回帰は2つの可能な結果を伴う分類問題の確率をモデル化します。これは線形回帰モデルの分類問題への拡張です。 4.2.1 線形回帰モデルを分類のために使うと何がいけないか。 線形回帰モデルは回帰問題ではうまく働きますが、分類問題ではうまくいきません。 なぜでしょうか？ 2クラス分類の場合、あるクラスを 0、もう一方を 1 とラベル付けし、線形回帰モデルを使ったとしましょう。 技術的にはうまく働き、線形モデルは重みを計算します。 しかし、この方法にはいくらか問題があります。 線形モデルは確率を出力せず、クラスを数字として扱い、点との距離を最小化するような最適な超平面に (単一の特徴量に対しては線として) 適合させます。 それは、単に、点の間の補間をしているだけなので、確率として解釈できません。 また、線形モデルは外挿し、0 より小さかったり、1 より大きな値を出力します。 これは分類問題に対する、より良いアプローチがあるかもしれないというよい徴候です。 予測結果が確率ではなく、点の間の線形補間なので、クラスを分けるための、意味のある閾値というものは存在しません。 この問題に関するわかりやすい説明はStackoverflow に示されています。 線形モデルは多クラス分類には拡張できません。 次のクラスを 2 だったり、3 だったりとラベル付けするかもしれません。 クラスの順序に意味のないときもあるでしょうが、線形モデルにおいては特徴量と予測クラスに不自然な関係性を作ることが強制されます。 同じような値に偶然なったクラスが他のクラスと近くなかったとしても、正の重みをもつ特徴量の値が高ければ高いほど、予測されるクラスはより大きな数になりやすくなっていきます。 FIGURE 4.5: 線形モデルで大きさによって腫瘍が良性であるか、悪性であるか分類しています。直線は線形モデルの予測を表しています。左にあるデータでは 0.5 を閾値として用いています。いくつか悪性のケースが入った場合では、回帰直線はシフトし、0.5 はもはや閾値としては機能しなくなっています。表示する点を少しだけ減らして、見やすくしています。 4.2.2 理論 分類のための解決策は、ロジスティック回帰です。 直線や超平面を当てはめる代わりに、ロジスティック回帰モデルでは、ロジスティック関数を使用して、0 と 1 の間へ線形方程式の出力を変形します。 ロジスティック関数は次のように定義されます。 \\[\\text{logistic}(\\eta)=\\frac{1}{1+exp(-\\eta)}\\] 下図のようになります。 FIGURE 4.6: ロジスティック関数。 出力は 0 から 1 の間を取ります。 入力が 0 のとき、出力は 0.5 です。 線形回帰からロジスティック回帰への変換は理解しやすいです。 線形回帰モデルでは、結果と特徴量の関係を線形方程式でモデル化しています。 \\[\\hat{y}^{(i)}=\\beta_{0}+\\beta_{1}x^{(i)}_{1}+\\ldots+\\beta_{p}x^{(i)}_{p}\\] 分類において、値が 0 から 1 の間となるように、右式をロジスティック関数に組み込みます。 この変換によって、出力は 0 から 1 の間の値を取るようにできます。 \\[P(y^{(i)}=1)=\\frac{1}{1+exp(-(\\beta_{0}+\\beta_{1}x^{(i)}_{1}+\\ldots+\\beta_{p}x^{(i)}_{p}))}\\] 腫瘍の大きさの例をもう一度見てみましょう。 線形回帰モデルの代わりにロジスティック回帰モデルを使っています。 FIGURE 4.7: ロジスティック回帰モデルは、腫瘍のサイズに応じて、悪性と良性の間の正しい決定境界を見つけます。 この線は、データに適合するように変形されたロジスティック関数です。 ロジスティック回帰はうまく分類し、両方のケースで 0.5 を閾値として使うことができます。点を追加することは、推定された曲線にそこまで影響を与えません。 4.2.3 解釈性 ロジスティック回帰モデルの出力は 0 から 1 の確率で表現されるため、ロジスティック回帰の重みの解釈は線形回帰モデルの重みの解釈とは異なります。 重みは確率に対して、線形に影響を及ぼすわけではありません。 重みの合計はロジスティック関数によって確率に変換されます。 したがって、解釈のために、方程式の右側の線形項を式変形する必要があります。 \\[log\\left(\\frac{P(y=1)}{1-P(y=1)}\\right)=log\\left(\\frac{P(y=1)}{P(y=0)}\\right)=\\beta_{0}+\\beta_{1}x_{1}+\\ldots+\\beta_{p}x_{p}\\] この log()関数の中の項を &quot;オッズ&quot; (イベントの確率をイベントが起こらない確率で割ったもの) といい、対数をとったものを対数オッズといいます。 この式はロジスティック回帰モデルが対数オッズに対する線形モデルであることを表しています。 すばらしい！ それでは役に立つようには見えません。 少し項を入れ替えると、特徴量 \\(x_j\\) の1つを 1 単位変化した時、どのように予測が変化するか分かります。 このように変換すると、exp() 関数を方程式の両辺にかけることができます。 \\[\\frac{P(y=1)}{1-P(y=1)}=odds=exp\\left(\\beta_{0}+\\beta_{1}x_{1}+\\ldots+\\beta_{p}x_{p}\\right)\\] そして、1つの特徴量の値を 1 増加させると何が起きるかを比較します。 このとき、差をみるのではなく、2つの予測の比に注目します。 \\[\\frac{odds_{x_j+1}}{odds}=\\frac{exp\\left(\\beta_{0}+\\beta_{1}x_{1}+\\ldots+\\beta_{j}(x_{j}+1)+\\ldots+\\beta_{p}x_{p}\\right)}{exp\\left(\\beta_{0}+\\beta_{1}x_{1}+\\ldots+\\beta_{j}x_{j}+\\ldots+\\beta_{p}x_{p}\\right)}\\] 以下のルールを適用します。 \\[\\frac{exp(a)}{exp(b)}=exp(a-b)\\] そして、項を削除します。 \\[\\frac{odds_{x_j+1}}{odds}=exp\\left(\\beta_{j}(x_{j}+1)-\\beta_{j}x_{j}\\right)=exp\\left(\\beta_j\\right)\\] 最終的に、特徴量の重みの exp() のような単純な式を得ることができます。 特徴量の中の 1 単位の変化は、オッズ比を \\(\\exp(\\beta_j)\\) 倍に変化させます。 この式は以下のように解釈できます。 \\(x_j\\) を 1 単位だけ変化させると、対応する重みの値だけ対数オッズ比が増加します。 ほとんどの人は、対数について頭で考えることが困難なため、オッズ比を解釈します。 オッズ比を解釈するには慣れが必要です。 例えば、もしオッズ比として 2 が与えられたら、 y=0 の確率より y=1 の方が 2 倍高いことを意味します。 重み（=対数オッズ比）が 0.7 の場合、それに対応する特徴量を 1 単位増やすと、オッズに exp(0.7)（=約 2）が乗算され、オッズは4に変わります。 しかし、通常はオッズを扱う必要がなく、重みだけをオッズ比として解釈します。 オッズを実際に計算するためには、それぞれの特徴量に値を設定する必要があるためです。 これは、データセットの特定のインスタンスに注目したいときのみ意味を成します。 特徴量の種類に応じて、ロジスティック回帰の解釈は異なります。 数的特徴量 (Numerical feature)の場合: \\(x_{j}\\) を 1 単位だけ増加させると、予測されるオッズは \\(\\exp(\\beta_{j})\\) 倍に変化します。 バイナリ特徴量 (Binary categorical feature) の場合: 特徴量の2つの値のうちの1つは 参照カテゴリ (いくつかのプログラミング言語では、0 とエンコードされる) です。 特徴量 \\(x_{j}\\) が参照カテゴリから他のカテゴリに変化した時、オッズは \\(\\exp(\\beta_{j})\\) 倍になります。 2つ以上のカテゴリを持つ、カテゴリカルデータの場合: 複数のカテゴリを扱う1つの方法は one-hot-encoding です。one-hot-encoding はそれぞれのカテゴリがそれぞれ列を持ちます。 L 個のカテゴリを持つとき、L-1 列のみ必要で、そうでなければ、パラメータが過剰となります。 L 番目のカテゴリは参照カテゴリである必要があります。 他にも、線形回帰で使用可能な任意のエンコード方法を用いることができます。 それぞれのカテゴリの解釈はバイナリ特徴量の解釈と同様です。 切片\\(\\beta_{0}\\): 全ての特徴量がゼロでカテゴリカル特徴量が参照カテゴリの時、予測されるオッズの値は \\(\\exp(\\beta_{0})\\) です。 切片の重みの解釈は、たいてい意味がありません。 4.2.4 例 ここでは、ロジスティック回帰をリスク要因に基づいた子宮頸がんの予測に使用します。 以下の表は推測された重み、オッズ比、予測の標準誤差を表しています。 TABLE 4.1: ロジスティック回帰モデルを子宮頸がんデータセットに適合させた結果。 モデルで使用されている特徴量、それらの推定された重みと対応するオッズ比、および推定された重みの標準誤差が示されています。 Weight Odds ratio Std. Error Intercept -2.91 0.05 0.32 Hormonal contraceptives y/n -0.12 0.89 0.30 Smokes y/n 0.26 1.29 0.37 Num. of pregnancies 0.04 1.04 0.10 Num. of diagnosed STDs 0.82 2.26 0.33 Intrauterine device y/n 0.62 1.85 0.40 量的特徴量の解釈 (&quot;Num. of diagnosed STDs&quot;): STDs (性感染症, sexually transmitted diseases) と診断された回数が増えると、癌かそうでないかのオッズは 2.26 倍変化します。 ただし、他の特徴量を固定した場合です。 相関は因果関係を示しているとは限らないことに注意してください。 カテゴリカル特徴量の解釈(&quot;Hormonal contraceptives y/n&quot;): ホルモン避妊薬を使っている女性に関して、癌かそうでないかのオッズは、ホルモン避妊薬を使っていない人に比べて 0.89 倍低いです。 ただし、他の特徴量を固定した場合です。 線形モデルと同様に、解釈では常に他の特徴量が固定されているという仮定の元で行われます。 4.2.5 長所と短所 線形回帰モデルの長所と短所は、ロジスティック回帰モデルにも当てはまります。 ロジスティック回帰は様々な分野の人々によって広く使用されていますが、その表現力の低さが問題であり（たとえば、相互作用を手動で追加する必要があります）、他のモデルの方が予測性能が優れていることもあります。 ロジスティック回帰モデルのもう1つの欠点は、重みの解釈は乗法的であり、加法的ではないため、解釈が難しくなることです。 ロジスティック回帰は、完全分離に悩まされることがあります。 2つのクラスを完全に分離できる特徴量がある場合、ロジスティック回帰モデルは学習できなくなります。 これは、その特徴量の最適な重みが無限大となり、収束しないためです。 このような特徴量はクラス分類の際に有用なので、これは残念なことです。 しかし、このように2つのクラスを分離する単純なルールがある場合は、機械学習は必要ありません。 完全分離の問題は、重みのペナルティを導入するか、重みの事前分布を定義することで解決できます。 良い面として、ロジスティック回帰モデルは分類モデルであるだけでなく、確率を出力します。 これは、最終的な分類結果しか提供できないモデルに比べて大きな利点と言えます。 インスタンスが、あるクラスに分類される確率が51％ではなく99％であること知れるのは大きな違いがあります。 ロジスティック回帰は、2クラス分類から多クラス分類に拡張できます。 それは多項回帰 (Multinomial Regression) と呼ばれます。 4.2.6 ソフトウェア 上記の例は、すべて R の glm 関数を使用しました。 ロジスティック回帰は、Python、Java、Stata、Matlab など、データ分析に用いられるあらゆるプログラミング言語で実装されています。 "],["extend-lm.html", "4.3 GLM、GAM、その他", " 4.3 GLM、GAM、その他 線形モデルは、予測を特徴量の重み付き和としてモデル化する点が、最大の長所であり短所でもあります。 それに加えて、線形モデルは多くの仮定を必要とします。 悪いニュースは（実際にはニュースではありませんが）、それらの仮定が現実問題には当てはまらないということがしばしば起こるということです。 例えば、結果が正規分布に従わなかったり、特徴量間に相互作用があったり、あるいは、特徴量と結果の間の真の関係が非線形であったりするような場合です。 一方で良いニュースは、専門家のコミュニティが、この線形回帰モデルという単純なものを、スイスのアーミーナイフのように扱いやすくするために様々な改良をしてきたということです。 この章は、線形モデルを拡張するための明確なガイドを提供しようとするものではなく、一般化線形モデル（GLM）や一般化加法モデル（GAM）などの拡張モデルの概要を紹介し、直感的な理解を与えることが目的です。 したがって、この章を読み終わった後に、どのように線形モデルを拡張するのかについてしっかりと理解する必要があるでしょう。 もしあなたが最初に線形回帰モデルについて理解したいのにも関わらず、まだ線形回帰モデルの章を読んでいないのなら、先にその章を読むことをお勧めします。 線形回帰モデルの式を思い出してみましょう。 \\[y=\\beta_{0}+\\beta_{1}x_{1}+\\ldots+\\beta_{p}x_{p}+\\epsilon\\] 線形回帰モデルは、結果 y は、p 個の特徴量の重み付き和に正規分布に従う誤差 \\(\\epsilon\\) が付与されたものであると仮定しています。 このようにデータを定式化することで、モデルの高い解釈可能性が得られるわけです。 線形回帰モデルでは、特徴量の効果は加法的で、相互作用はなく、特徴量と出力の関係は線形です。つまり、特徴量が 1 増加すると、直接的に予測結果が増加/減少します。 線形モデルを使用することで、特徴量と予測結果の関係を、1つの数値、すなわち推定された重みとして表現します。 しかし、単純な重み付き和は制約として強すぎるため、現実世界の多くの予測問題にそのまま適用することは出来ません。 この章では、線形回帰モデルの典型的な3つの問題を取り上げ、それらをどう解決するかについて紹介します。 モデル仮定に反する可能性のある問題は他にも多くありますが、次の図に示す3つの問題に焦点を当てます。 FIGURE 4.8: 線形モデルの3つの仮定(左側):特徴量に対する出力が正規分布であること、加法性が成り立つ（=相互作用がない）こと、及び線形関係にあること。現実では通常これらの仮定を満たさない(右側):出力が正規分布に従わなかったり、特徴量間に相互作用が存在したり、非線形の関係があったりするかもしれません。 これらすべての問題に対する解決策があります。 問題: 特徴量が与えられたときの結果 y が、正規分布に従わない。 例: ある日に何分間、自転車に乗るかを予測したいとします。 この場合、特徴量としては曜日や天気などがあります。 しかしながら、線形モデルを使用した場合、正規分布を想定しているため、0 分以下にならないとは限らず、負の時間を予測してしまうかもしれません。 その他の例として、線形モデルを使用して確率を予測した場合、負または1より大きい確率が予測値として得られることになります。 解決策: 一般化線形モデル（GLMs） 問題: 特徴量間に相互作用が存在する。 例: 私は、小雨が降るとサイクリングしたいという気持ちが少し萎えますが、夏のラッシュアワー時には雨を歓迎します。 それは、晴天を好むサイクリストが全員家にいて、自転車道が空くからです。 これは時間と天気の間の相互作用であるため、単純な加法モデルでは捉えることができません。 解決策: 相互作用項を手動で追加する 問題: 特徴量と結果 y の間の真の関係性が線形ではない。 例: 摂氏0度から25度の間では、自転車に乗りたいという私の欲求に対する温度の影響は、線形である可能性があります。 つまり、気温が0度から1度に上昇した場合と、20度から21度に上昇した場合のモチベーションの増加は同じということです。 しかし、その気温よりが高くなれば、暑すぎるときに自転車に乗るのは好きでないので、サイクリングへのモチベーションは横ばいになり、やがて低下します。 解決策: 一般化された加法モデル (GAM); 特徴量の変換 この章では、これら3つの問題の解決策を示します。 線形モデルの拡張は他にも多くありますが、ここでは割愛します。 ここで全てを説明しようとすれば、この章自体がすでに他の多くの専門書の中で説明されている内容を集めた本となってしまうでしょう。 とはいえ、せっかくですので、章の終わりで線形モデル拡張について問題と解決策のちょっとした概要を作成しておきました。 解決策の名前は、自身で詳細を調べるときに役に立つことでしょう。 4.3.1 結果が正規分布に従わない場合 - GLMs 線形回帰モデルは、特徴量が与えられたときの結果は正規分布に従うと仮定としています。 しかし、この仮定が成り立たない場合は多くあります。 結果は、カテゴリ (がん or 健康)、整数 (子供の数)、出来事が起こるまでの時間 (機械が故障するまでの時間)、少数のとても大きな数が存在する偏った出力 (世帯収入) などがあります。 実は、線形回帰モデルはこれらすべてのタイプに拡張できます。 この拡張は一般化線形モデル (Generalized Linear Models)もしくは省略してGLMsと呼ばれます。 この章では、GLMという用語を一般的なフレームワークとこのフレームワークに由来する特定のモデル、両方を表す際に使います。 GLM のコアとなる概念は、「特徴量の重み付き和を保持するが、結果の分布の非正規性を許容し、この分布の平均と重み付き和をある非線型関数で関連づけること」です。 例えば、ロジスティック回帰モデルでは結果が二項分布 (Bernoulli distribution) に従うことを仮定しており、ロジスティック関数を通して分布の平均と重み付き和が関連付けられています。 GLM は特徴量の重み付き和と、仮定された分布の平均を、リンク関数 g を用いて数学的に関連付けます。このとき、 g は結果の種類によって柔軟に決められます。 \\[g(E_Y(y|x))=\\beta_0+\\beta_1{}x_{1}+\\ldots{}\\beta_p{}x_{p}\\] GLM は次の3つの要素からなります。 それは、リンク関数 g、重み付き和 \\(X^T\\beta\\) (線形予測器とも呼ばれる) と、\\(E_Y\\) を定義する指数型分布族に由来する確率分布です。 指数型分布族は、分布の平均、分散、その他のパラメータを持つ指数が含まれた共通の式によって記述できる分布の集合です。 それ自体、とてつもなく広い分野であり、深入りはしたくないので、数学的な詳細まで今回は扱いません。 Wikipedia にはよく整理された指数型分布族に該当する分布の表があります。 このリストの中ならどの分布でも GLM に適用できます。 予測したい結果の種類に応じて、適切な分布を選んでください。 例えば、結果が数 (例：一家庭における子供の数) ならばポアソン分布、 結果が常に正 (例：2つのイベント間の時間) ならば指数分布が良いでしょう。 GLM の特殊な場合として、古典的な線形モデルについて考えてみましょう。 線形モデルにおける正規分布のリンク関数は単に恒等関数となります。 正規分布は平均と分散によって決まります。 平均値は平均的に期待される値、分散は平均の周りでどのくらい値がばらつくかを表す値です。 線形モデルでは、リンク関数は特徴量の重み付き和と正規分布の平均を関連付けます。 GLM のフレームワークでは、この概念は (指数型分布族に由来する) すべての分布とリンク関数に一般化されます。 y が例えば一日に飲むコーヒーの数といったような数であったなら、ポアソン分布とリンク関数である自然対数を用いたGLMでモデル化できます。 \\[ln(E_Y(y|x))=x^{T}\\beta\\] ロジスティック回帰モデルも二項分布を仮定した GLM で、ロジット関数がリンク関数として使われています。 ロジスティック回帰で用いられる二項分布の平均は y が 1 である確率です。 \\[x^{T}\\beta=ln\\left(\\frac{E_Y(y|x)}{1-E_Y(y|x)}\\right)=ln\\left(\\frac{P(y=1|x)}{1-P(y=1|x)}\\right)\\] そして、この式を片方が P(y=1) となるように変形すれば、ロジスティック回帰の公式が得られます。 \\[P(y=1)=\\frac{1}{1+exp(-x^{T}\\beta)}\\] 指数型分布族に属する分布は、分布から数学的に導ける正準リンク関数 (canonical link function) があります。 GLM の枠組みはその分布と関係なくリンク関数を選ぶことができます。 どうやって、適切なリンク関数を選ぶのでしょうか。 そこには、完璧なレシピはありません。 目的値の分布だけでなく、理論的な考察と実際のデータにどれだけ適合しているかを考慮に入れます。 分布の中には、正準リンク関数がその分布に対して無効であるような値につながるものもあります。 指数型分布族の分布の場合、正準リンク関数は負の逆関数であり、指数分布の領域の外側であるような負の予測値を出してしまうことがあります。 ただ、リンク関数は任意に選べるので、単純な解決策は分布の領域に適合するような別の関数を選ぶことです。 例 GLM の必要性を強調するために、一日どれくらいコーヒーを飲むかについてのデータセットについて考えてみました。 一日にコーヒーを飲む量についてのデータを集めたとします。 コーヒーが好きでないのなら、お茶でもかまいません。 コーヒーを飲んだ数とともに、現在のストレスレベル（1から10）、夜どれくらいよく眠れたか（1から10）、その日仕事があったかについて記録します。 目標は200日間のこれらのデータからコーヒーを飲んだ数を予測することです。 ストレスと睡眠については1から10まで均一の分布であり、仕事について yes/no が50%ずつとします（なんて生活でしょうか！）。 コーヒーを飲んだ数はポアソン分布に従い、\\(\\lambda\\)（ポアソン分布の期待値）を睡眠、ストレス、仕事の特徴量の関数としてモデル化します。 この話がどうなっていくか予測できるでしょうか？ 「線形モデルでこのデータをモデル化してみよう...うまくいかないなぁ...じゃあ、ポアソン分布を用いてGLMでやったらどうかな...あ!うまくいったぞ!!!!!!!」 読者のためにあまり話が逸れないようにしなければ...。 一日に飲んだコーヒーの数を目的変数としたときの分布をみてみましょう。 FIGURE 4.9: 200日間のコーヒーを飲んだ量の分布 76 日中 200 日はまったくコーヒーを飲んでおらず、一番飲んだ日は7 杯も飲んでいます。 愚直に線形モデルを用いて、睡眠レベル、ストレスレベル、仕事の有無の特徴量から飲んだ コーヒーの数を予測してみましょう。 誤って正規分布を仮定すると何がおかしくなるのでしょうか? 間違った仮定は推定値、特に重みの信頼区間を無効にしてしまいます。 さらに明らかな問題は、次の図に示すように予測値が真の結果の&quot;許された&quot;領域と合致しないということです。 FIGURE 4.10: ストレス、睡眠、仕事に応じて予測されたコーヒーを飲む量の予測値。線形モデルは負の値を予測しています。 線形モデルは負の値を予測するので、理にかなっていません。 リンク関数と仮定している分布を変更することによって GLM ではこの問題を解決できます。 1つの選択肢は、正規分布は引き続き使い、リンク関数としては、常に正の値しか取らないようにするため、恒等関数の代わりに log-link (expの逆関数) を使用するという方法です。 さらに良いのは、データが生成されたプロセスに従った分布を選び、適切なリンク関数を選ぶことです。 結果は数ですから、ポアソン分布とリンク関数として対数関数を選ぶことが自然です。 今回は、データはポアソン分布から生成されるので、Poisson GLM はベストチョイスです。 学習された Poisson GLM による予測値の分布は次のようになります。 FIGURE 4.11: ストレス、睡眠、仕事に応じて予測されたコーヒーを飲む量の予測値。ポアソン分布とlog link に基づいた GLM はこのデータセットに対する適切なモデルです。 負の値を取らないので、先ほどよりも良さそうです。 GLMの重みの解釈 リンク関数と共に仮定した分布は推定される特徴量の重みがどのように解釈されるかを決めます。 コーヒーの例では、ポアソン分布とlogリンクに基づいたGLMを用いました。これによって、次のような特徴量と予測値の関係が示唆されます。 \\[ln(E(\\text{coffees}|\\text{stress},\\text{sleep},\\text{work}))=\\beta_0+\\beta_{\\text{stress}}x_{\\text{stress}}+\\beta_{\\text{sleep}}x_{\\text{sleep}}+\\beta_{\\text{work}}x_{\\text{work}}\\] 重みを解釈するために、予測された結果の対数ではなく、リンク関数の逆関数をとり、特徴量の影響を理解しやすいように変形します。 \\[E(\\text{coffees}|\\text{stress},\\text{sleep},\\text{work})=exp(\\beta_0+\\beta_{\\text{stress}}x_{\\text{stress}}+\\beta_{\\text{sleep}}x_{\\text{sleep}}+\\beta_{\\text{work}}x_{\\text{work}})\\] すべての重みは、指数関数の中にはいっており、exp(a + b) は exp(a) と exp(b) の積になるので、効果の解釈は加法的ではなく乗法的となります。 解釈の最後の要素は、例の実際の重みです。 以下の表に予測される重みと exp(weight) を 95% 信頼区間とともに挙げています。 weight exp(weight) [2.5%, 97.5%] (Intercept) -0.16 0.85 [0.54, 1.32] stress 0.12 1.12 [1.07, 1.18] sleep -0.15 0.86 [0.82, 0.90] workYES 0.80 2.23 [1.72, 2.93] ストレスレベルが1点上昇すると、予測されるコーヒーの量は 1.12 倍になります。 睡眠の質が1点上昇すると、予測されるコーヒーの量は 0.86 倍になります。 仕事のある日の予測値は仕事のない日と比べて、平均して2.23倍されます。 まとめると、ストレスが多い、睡眠が少ない、仕事がある日に、より多くのコーヒーが飲まれます。 この章では正規分布にターゲットが従わない場合に有効なGLMについて少し学びました。 次は、2つの特徴量の相互作用をどのように線形回帰モデルに取り入れるかを見ていきましょう。 4.3.2 相互作用 線形回帰モデルでは、1つの特徴量がもたらす効果は他の特徴量の値とは関係がない (＝相互作用がない) ことを前提としています。 しかし、多くの場合、特徴量の間には相互作用があります。 例えば、自転車レンタルの数を予測する際、気温と就業日であるかどうかの間に相互作用があるかもしれません。 おそらく、就業日であれば、何があろうとも仕事のために自転車に乗るので、気温はレンタルされる自転車の数に大して影響を与えないでしょう。 しかし休日の場合には、多くの人が娯楽目的に自転車に乗りますが、それは気温が十分に暖かいときだけでしょう。 したがって、レンタル自転車の予測では、気温と就業日の間の相互作用が期待されるかもしれません。 線形モデルで相互作用を考慮するにはどうすれば良いでしょう。 線形モデルを学習する前に、特徴量に相互作用を表現する列を追加します。 この解決策は、線形モデル自体を変更することなく、ただ列をデータに追加するだけで良いという点で優れた手法といえます。 例えば、就業日と気温の例においては、休業日の場合は 0、それ以外は気温の値を持つような特徴量を追加します。 このとき、就業日が参照カテゴリであると仮定します。 データが次のようになっているとします。 work temp Y 25 N 12 N 30 Y 5 以下の表は、相互作用を考慮しない場合のデータ行列を表しており、先ほどのものと少し異なります。 通常、この変換は統計ソフトウェアによって自動的に行われます。 Intercept workY temp 1 1 25 1 0 12 1 0 30 1 1 5 1列目は切片の項です。 2列目は 0 を参照カテゴリ、1 をその他としたようなカテゴリカル特徴量となっています。 そして、3列目には気温が入っています。 線形モデルで気温と就業日の相互作用を考慮したい場合、次のように相互作用の列を追加する必要があります。 Intercept workY temp workY.temp 1 1 25 25 1 0 12 0 1 0 30 0 1 1 5 5 新しい列 'workY.temp' は就業日（work）と気温（temp）間の相互作用を表現します。 この列は work 特徴量が参照カテゴリ（Nである、つまり就業日でない）のときに 0 を、その他の場合には気温の値をとります。 このエンコーディングにより、線形モデルは就業日・休業日両方に対して異なる気温による線形効果を学習できます。 これが2つの特徴量間の相互作用です。 相互作用項がない場合、カテゴリカル特徴量と量的特徴量の複合効果は異なるカテゴリに対して垂直方向にシフトした直線で表現できます。 相互作用を考慮すると、量的特徴量の効果（傾き）が各カテゴリごとに異なる値を持つことができます。 2つのカテゴリカル特徴量の相互作用についても同様です。 カテゴリの組み合わせを表現する特徴量を追加します。 次の表は就業日（work）とカテゴリカルな天候（wthr）を含んだ人工的なデータです。 work wthr Y 2 N 0 N 1 Y 2 次に相互作用項を追加します。 Intercept workY wthr1 wthr2 workY.wthr1 workY.wthr2 1 1 0 1 0 1 1 0 0 0 0 0 1 0 1 0 0 0 1 1 0 1 0 1 1列目は切片の計算に用いられます。 2列目はエンコードされた就業日の特徴量です。 3列目と4列目は、3つのカテゴリを持つ天候の特徴量であり、1つを参照カテゴリとして、効果を表現するためには2つの重みが必要なので、2つの列が用意されています。 残りの列は相互作用項です。 2つの特徴量の各カテゴリごと（参照カテゴリを除く）に、両方の特徴量が、あるカテゴリであれば 1 を、そうでなければ 0 を持つような列を作ります。 2つの量的特徴量に対しては、相互作用項はさらに簡単に構築できます。単に両方の特徴量の値を掛け合わせれば良いのです。 実は、自動で相互作用項を検出し追加するアプローチはいくつかあります。 そのうちの1つは、RuleFitの章で紹介されています。 RuleFit アルゴリズムでは、最初に相互作用を探索し、これらの相互作用を含めた線形回帰モデルを学習します。 例 線形モデルの章でモデル化した自転車レンタル数の予測タスクに戻りましょう。 今回は、気温と就業日の間の相互作用についても考慮します。 これにより、次の重みと信頼区間が得られます。 Weight Std. Error 2.5% 97.5% (Intercept) 2185.8 250.2 1694.6 2677.1 seasonSUMMER 893.8 121.8 654.7 1132.9 seasonFALL 137.1 161.0 -179.0 453.2 seasonWINTER 426.5 110.3 209.9 643.2 holidayHOLIDAY -674.4 202.5 -1071.9 -276.9 workingdayWORKING DAY 451.9 141.7 173.7 730.1 weathersitMISTY -382.1 87.2 -553.3 -211.0 weathersitRAIN/... -1898.2 222.7 -2335.4 -1461.0 temp 125.4 8.9 108.0 142.9 hum -17.5 3.2 -23.7 -11.3 windspeed -42.1 6.9 -55.5 -28.6 days_since_2011 4.9 0.2 4.6 5.3 workingdayWORKING DAY:temp -21.8 8.1 -37.7 -5.9 追加した相互作用による影響は負 (-21.8) であり、95%信頼区間が 0 を含まないことからわかるように、0 から大きく離れています。 ちなみに、互いに近い日は独立でないため、データは独立同分布 (iid) ではありません。 信頼区間は誤解を招く恐れがあるため、話半分に見てください。 相互作用項は関連する特徴量の重みの解釈を変えます。 就業日であれば気温は負の効果をもたらしているでしょうか。 たとえ表が負の効果を示していたとしても、答えはノーです。 &quot;workingdayWORKING DAY:temp&quot; の相互作用項の重みは単独で解釈できません。 なぜなら、重みの解釈は「他のすべての特徴量を変化させずに、就業日の気温の相互作用効果を増加させると、予測される自転車の数は減少する。」となるからです。 しかし、相互作用の効果は気温による効果に追加されるだけです。 今日が就業日で、気温が1度高かったらどうなるか知りたいとします。 それなら、&quot;temp&quot; と &quot;workingdayWORKING DAY:temp&quot; の両方の重みを合計して、予測値がどれほど増加するかを判断する必要する必要があります。 相互作用は簡単に視覚化できます。 カテゴリカル特徴量と量的特徴量の間の相互作用を導入することにより、気温に対して1つではなく2つの勾配を得ることができます。 休業日（'NO WORKING DAY'）における気温の勾配は、表から直接読み取ることができます (125.4)。 就業日（'WORKING DAY'）における気温の勾配は、両方の気温の重みの合計から得られます (125.4 -21.8 = 103.6)。 'NO WORKING DAY'直線の 気温 = 0 における切片は、線形モデルの切片 (2185.8) で決定されます。 また、'WORKING DAY'直線の 気温 = 0 における切片は、線形モデルの切片 + 就業日の効果 (2185.8 + 451.9 = 2637.7) で決まります。 FIGURE 4.12: 線形モデルのレンタル自転車数の予測に対する気温と就業日の影響 (相互作用を含む)。就業日の各カテゴリに対して2つの気温の勾配が得られている。 4.3.3 非線形効果 - GAM 世界は線形ではありません。 線形モデルにおける線形性とは、インスタンスの特徴量の値がどのような時でも、値を 1 増やすと、常に同じ効果を予測結果に与えるということを意味します。 気温が10度から11度に上がった時と、40度から41度に上がった時とで、自転車のレンタル数に同じ影響があると考えるのは妥当でしょうか？ 直感的には、気温が10度から11度に上がるとレンタル自転車数は増え、40度から41度に上がるとレンタル自転車数は減ると予想されます。これは、この本に出てくる他の例も同様です。 温度という特徴量は、レンタル自転車数に線形なプラスの影響を及ぼしますが、ある時点で平坦になり、高い温度ではマイナスの影響を及ぼします。 線形モデルはこのような影響の変化に関わらず、（ユークリッド距離を最小化することによって）あくまで最適な線形の関係性を見つけようとします。 非線形の関係は以下の手法を用いてモデル化できます。 特徴量の単純な変換（対数変換など） 特徴量のカテゴリカル化 一般化加法モデル（Generalized Additive Model, GAM） それぞれの手法の詳細に入る前に、これら3つについて例を見てみましょう。 レンタル自転車のデータセットを使って、温度の特徴量のみを使用し線形モデルを学習することで、レンタル自転車数の予測をしました。 次の図はそれぞれ、標準的な線形モデル、対数変換した温度による線形モデル、温度をカテゴリカル特徴量として扱った場合の線形モデル、GAM (スプライン回帰) を使用したときの推定された勾配を示しています。 FIGURE 4.13: 温度特徴量のみを用いたレンタル自転車数の予測。線形モデル（左上）はデータにあまり適合していません。1つの解決策は、例えば対数で特徴量を変換（右上）したり、カテゴリカル化する（左下）ことですが、これは通常は悪い判断です。GAMを使うと(右下)、気温に対して滑らかな曲線を自動的に適合できます。 特徴量変換 多くの場合、特徴量の対数変換が使用されます。 対数を使用すると、10倍の温度上昇ごとに自転車の数に同じ線形効果があることが示されます。 したがって、気温が1度から10度に変化するときと、0.1度から1度に変化するときは同じ効果があります（これも間違っているように思われます）。 特徴量変換の他の例は、平方根、二乗関数、および指数関数です。 特徴量変換を使用するということは、データの特定の特徴量の列を対数などで変換したものに置き換えて、通常どおり線形モデルで学習することを意味します。 一部の統計プログラムでは、線形モデルを呼び出す時に変換方法も指定できます。 特徴量の変換は、クリエイティブな行為です。 特徴量の解釈は、選択した変換によって変わります。 対数変換を使用する場合、線形モデルにおける解釈は「特徴量の対数が1増加すると、対応する重みによって予測結果が増加する。」となります。 恒等関数ではないリンク関数でGLMを使用する場合、両方の変換を解釈に組み込む必要があるため、解釈はより複雑になります（logとexpのように互いに打ち消し合う場合は解釈は簡単なので除く）。 特徴量のカテゴリカル化 非線形効果を実現するもう1つの選択肢は、特徴量を離散化し、カテゴリカル特徴量とすることです。 たとえば、温度という特徴量をレベル[-10, -5), [-5, 0), ... の 20 の区間に分割できます。 連続値としての温度の代わりにカテゴリカル化された温度を使用する場合、各区間が独自の推定値をとるため、線形モデルはステップ関数を推定します。 このアプローチの問題は、より多くのデータが必要であり、過学習する可能性が高く、特徴量を意味のある形で離散化する方法が不明確であるということです（等距離区間または分位数？区間の数は？）。 非常に有効な場合にのみ、離散化を使用します。 たとえば、モデルを別の研究と比較できるようにするためなどです。 一般化加法モデル（Generalized Additive Models, GAM） なぜ非線形の関係を学習のために (一般化)線形モデルを 'そのまま' 使用してはいけないのでしょうか？ それがGAMの背後にある動機です。 GAMは、関係は単純な重み付き和でなければならないという制限を緩和し、各特徴量の任意の関数の総和によって結果をモデル化できると仮定します。 数学的には、GAMの関係は次のようになります。 \\[g(E_Y(y|x))=\\beta_0+f_1(x_{1})+f_2(x_{2})+\\ldots+f_p(x_{p})\\] この式は GLM の式に似ていますが、線形項 \\(\\beta_j{}x_{j}\\) がより柔軟な関数 \\(f_j(x_{j})\\) に置き換えられている点が異なります。 GAM の核は依然として特徴量効果の合計ですが、特徴量と出力の間の非線形性を許す余地があります。 線形効果もこのフレームワークでカバーされており、特徴量に対する線形性は、\\(f_j(x_{j})\\) を \\(x_{j}\\beta_j\\) の形に制限することで表現できます。 大きな問題は、この非線形関数をどのように学習するかです。 その答えは「スプライン」または「スプライン関数」と呼ばれます。 スプラインは、任意の関数を近似するために組み合わせることができる関数です。 より複雑なものを構築するためにレゴブロックを積み重ねるのと少し似ています。 これらのスプライン関数を定義するには、おびただしい数の方法があります。 もしスプラインを定義するすべての方法についてもっと知りたいのなら、それは長旅になるでしょう。旅の幸運を祈っています。 ここでは詳細に立ち入らずに、直感的な説明のみに留めます。 スプラインを理解するために個人的に最も役立ったのは、個々のスプライン関数を視覚化し、データ行列がどのように変わったかを調べることでした。 たとえば、スプラインを使用して温度をモデル化するには、データから温度の特徴量を削除し、それぞれがスプライン関数を表す4つの列に置き換えます。 通常、スプライン関数はもっと多くなりますが、説明のために数を減らしました。 これらの新しいスプライン特徴量の各インスタンスでの値は、インスタンスの温度の値によって異なります。 すべての線形効果とともに、GAMはこれらのスプラインの重みも推定します。 GAMはまた、重みをゼロに近づけるためのペナルティ項も導入します。 これにより、スプラインの柔軟性が低下し、過学習が抑制されます。 曲線の柔軟性を制御するために一般的に使用される平滑化パラメータは、交差検定によって調整されます。 ペナルティ項を無視すると、スプラインを使用した非線形のモデリングは高度な特徴量エンジニアリングとなります。 GAMを使用し、温度のみから自転車の数を予測する例では、モデルの特徴量の行列は次のようになります。 (Intercept) s(temp).1 s(temp).2 s(temp).3 s(temp).4 1 0.93 -0.14 0.21 -0.83 1 0.83 -0.27 0.27 -0.72 1 1.32 0.71 -0.39 -1.63 1 1.32 0.70 -0.38 -1.61 1 1.29 0.58 -0.26 -1.47 1 1.32 0.68 -0.36 -1.59 各行は、データからの個々のインスタンス (1日) を表します。 各スプラインの列には、特定の温度の値でのスプライン関数の値が含まれています。 以下に、これらのスプライン関数の様子を図示します。 FIGURE 4.14: 温度効果を滑らかにモデル化するために、4つのスプライン関数を使用します。 各温度の値は、（ここでは）4つのスプライン値にマッピングされます。 インスタンスの温度が30度の場合、最初のスプライン特徴量の値は -1、2番目は 0.7、3番目は -0.8、および4番目は 1.7 です。 GAMは、各温度のスプライン特徴量に重みを割り当てます。 weight (Intercept) 4504.35 s(temp).1 -989.34 s(temp).2 740.08 s(temp).3 2309.84 s(temp).4 558.27 また、推定された重みで重み付けされたスプライン関数の合計から得られる実際の曲線は、次のようになります。 FIGURE 4.15: レンタル自転車数を予測するための温度のGAM特徴量効果（温度のみを特徴量として使用）。 滑らかな効果の解釈には、学習した曲線の視覚的なチェックが必要です。 スプラインは通常、平均予測で中心化されているため、曲線上の点は平均予測との差になります。 たとえば 0度のとき、予測される自転車の数は平均予測よりも3000台少ないということです。 4.3.4 長所 線形モデルのこれらすべての拡張は、それら自身がそれぞれ小宇宙のようなものです。 線形モデルで直面する問題が何であれ、それを修正するための拡張方法が見つかるでしょう。 ほとんどの手法は、何十年もの間使用されてきました。 たとえば、GAMはおおよそ30年前のものです。 多くの研究者や産業界の実務家は、線形モデルの経験がとても豊富であり、それらの手法はモデリング手法の現状として多くのコミュニティで受け入れられています。 それに加えて、モデルの仮定に反していないのであれば、モデルを使用して予測を行い、データに関する結論を導き出すことも可能です。 また、重みの信頼区間、有意差検定、予測区間などを得ることができます。 統計ソフトウェアは通常、GLM、GAM、およびより特別な線形モデルを学習するための非常に優れたインターフェースを備えています。 多くの機械学習モデルの不透明度は、1）スパース性の欠如、つまり多くの特徴量が使用されていること、2）非線形に扱われる特徴量、つまり効果を記述するためには1つ以上の重みが必要、および、3）特徴量間の相互作用のモデリング、の３つに起因します。 線形モデルは高い解釈可能性をもっている一方で、しばしば現実に適合しないということを前提にすれば、この章で説明した拡張は、解釈可能性をある程度維持しながら、より柔軟なモデルへのスムーズな移行を実現する優れた方法を提供しているといえるでしょう。 4.3.5 短所 利点として、線形モデルはそれぞれ独自の宇宙が広がっていると言いました。 初学者でなくても、単純な線形モデルを拡張する方法は非常に多いため圧倒されてしまいます。 実際には、研究者や実務家の多くのコミュニティが、多かれ少なかれ同じことを行う手法に独自の名前を持っているため非常にややこしく、複数の並行宇宙があると言えます。 線形モデルの修正により、そのほとんどのモデルは解釈性が低下します。 恒等関数以外の任意のリンク関数 (GLMにおいて) は、解釈を複雑にしますし、 相互作用も解釈を複雑にします。 また、非線形の特徴量の効果は、直感的ではないか（対数変換のように）、あるいは、もはや単一の数値で要約できなくなります（スプライン関数など）。 GLM や GAM などは、データの生成プロセスに関する仮定に依存しています。 それらの仮定に反した場合、重みの解釈に妥当性はなくなります。 ランダムフォレストや勾配ブースティングなどの決定木ベースのアンサンブル学習モデルは、多くの場合、最も洗練された線形モデルよりも優れたパフォーマンスを示します。 これは、私自身の経験からもいえますし、kaggle.com などのプラットフォームで優勝したモデルの結果からもいえます。 4.3.6 ソフトウェア この章のすべての例は R 言語を用いて作られています。 GAM には gam パッケージが使用されていますが、それ以外にも多くのパッケージがあります。 R には回帰モデルを拡張する驚くほど多くのパッケージがあります。 他の分析用の言語にも負けることはなく、R は、他の考えられる線形モデルの拡張の原点と言えます。 Python でも様々な実装を見つけることができ、pyGAM は GAM の Python 実装ですが、これはまだ成熟していません。 4.3.7 さらなる拡張 約束通り、線形モデルを使う際に遭遇する可能性のある問題と、検索したら解決できるように解決方法の名前をリストで示します。 データが独立同分布 (iid) の仮定に反する場合。 例として、同じ患者の繰り返しの測定がこれに当たります。 このような場合は、混合モデル (mixed models)や一般化推定方程式 (generalized estimating equations)で検索してください。 モデルが不均一な分散の誤差持つ場合。 例として、住宅価格を予想する時、高価な住宅であるほど、予測値の誤差は大きくなりますが、これは線形モデルの等分散性に反します。 ロバスト回帰 (robust regression)で検索してください。 モデルに大きく影響する外れ値がある場合。 ロバスト回帰 (robust regression)で検索してください。 イベントが起きるまでの時間を予測したい場合。 イベントまでの時間のデータでは、大抵、打ち切られた測定値が含まれていますが、これはイベントが起きるまでに十分な時間が無かったことを意味しています。 例えば、ある会社が二年間のデータしか与られていない状態で、製氷機の故障を予測したい場合です。 二年経過しても故障しない機械もありますが、その後故障する可能性もあります。 パラメトリック生存モデル (parametric survival models)、コックス回帰 (cox regression)、 生存時間分析 (survival analysis)で検索してください。 予測の結果がカテゴリカルの場合。 もし、結果が2つのカテゴリの場合、ロジスティック回帰モデルを使用してカテゴリの確率を求めることができます。 さらに多くのカテゴリがある場合、multinomial regressionで検索してください。 ロジスティック回帰と multinomial regression はどちらも GLM です。 順序付きのカテゴリを予測したい場合。 例えば学校の成績です。 比例オッズモデル (proportional odds model)で検索してください。 結果が、家族の中の子供の数のようなカウントの場合。 ポアソン回帰 (Poisson regression)で検索してください。 ポアソンモデルもGLMです。 0 の値の頻度がとても多いという問題があるかもしれません。 そのときは、ゼロ過剰ポアソン回帰 (zero-inflated Poisson regression)やHurdleモデル (hurdle model)で検索してください。 正しい因果関係を導き出すためにどの特徴量をモデルに含めればいいのかわかりません。 例えば、血圧に効果のある薬が知りたいときです。 薬はなんらかの血液量に直接影響を与え、この血液量が結果に影響を与えます。 血液量を回帰モデルに含めるべきでしょうか？ 因果推論 (causal inference)や媒介分析 (mediation analysis)で検索してください。 データに欠損値がある場合。 多重代入法 (multiple imputation)で検索してください。 事前知識をモデルに取り入れたい場合。 ベイズ推定 (Bayesian inference)で検索してください。 最近少し元気がありません。 &quot;Amazon Alexa Gone Wild!!! Full version from beginning to end&quot;で検索してください。 "],["tree.html", "4.4 決定木", " 4.4 決定木 線形回帰とロジスティック回帰モデルは特徴量と結果が非線形の時や特徴量が相互作用する時に失敗します。 この状況こそ決定木が輝く時です！ 木をベースにしたモデルは、特徴量を、あるカットオフ値に基づいて複数回データを分割していきます。 この分岐を通して、データセットは異なる部分集合に分割され、それぞれのインスタンスはこのうちの1つに属します。 最後の部分集合は終端ノード (terminal node) または葉 (leaf node) と呼ばれていて、中間の部分集合は内部ノード (internal node)、または、分岐ノード (split node)と呼ばれています。 それぞれの葉で結果を予測するためには、ノードに含まれる学習データの結果の平均値が使用されます。 決定木は、分類でも回帰でも使われています。 決定木を成長させるためのさまざまなアルゴリズムが知られています。 これらのアルゴリズムでは、決定木の構造（例:ノードあたりの分岐数）、分岐を見つけるための指標、いつ分岐を止めるか、そしてどのようにして葉の中で簡単なモデルを予測するかが異なっています。 CART (Classification And Regression Trees) アルゴリズムは、おそらく決定木を構築するためのもっとも有名なアルゴリズムです。 本章ではCARTに焦点をあてますが、他の木も同様に解釈可能です。 CARTに関して、より詳細を知りたいのであれば、&quot;The Elements of Statistical Learning&quot; (Friedman, Hastie and Tibshirani 2009)17 の本をおすすめします。 FIGURE 4.16: 人工データに対する決定木。 特徴量 x1 が 3 より大きいとき、ノード 5 に割り当てられる。それ以外のインスタンスは特徴量 x2 が 1 を超えるかどうかでのノード 3 かノード 4 に割り当てられる。 下記の方程式は特徴量 x と結果 y の関係を記述しています。 \\[\\hat{y}=\\hat{f}(x)=\\sum_{m=1}^Mc_m{}I\\{x\\in{}R_m\\}\\] それぞれのインスタンスは1つの葉 (= 部分集合 \\(R_m\\)) と対応します。 \\(I_{\\{x\\in{}R_m\\}}\\) は、\\(x\\) が 部分集合 \\(R_m\\) に属していれば 1 を、そうでないなら 0 を返す指示関数です。もし、インスタンスが葉 \\(R_l\\) に対応したとすると、予測による結果は \\(\\hat{y}=c_l\\) となります。ただし、\\(c_l\\) は葉 \\(R_l\\) に対応する全ての学習データのインスタンスの平均値です。 しかし、これらの部分集合はどこからきたのでしょうか？ これはとても簡単です: CARTは回帰であれば y の分散を、分類であれば y のクラス分布のジニ係数を最小にするように、特徴量を選び、カットオフ点を決定します。 分散はノード内の y の値が平均からどの程度広がっているかを教えてくれます。 ジニ係数はノードがどれだけ不純かを教えてくれます。例えば、ノード内に全てのクラスが同じ数あるとき、ノードは不純になります。一方で、ノードのクラスが1つだけの場合、純度が最大になります。 ノードの中のデータ点がとても似た y の値を持つとき、分散やジニ係数は小さくなります。 結果として、最適なカットオフ点は、目標値に関して、2つの結果の部分集合ができるかぎり異なる値となるように選ばれます。 カテゴリカル特徴量に対して、アルゴリズムは各カテゴリごとにまとめる様にして部分集合を作成します。 特徴量ごとの最適なカットオフが決まったあと、アルゴリズムは、その中から分散やジニ係数に関して最適な分岐を与える特徴量を選び、この分岐を木に追加します。 アルゴリズムは、両方の新しいノードに対して、この探索と分岐を停止の基準に到達するまで再帰的に繰り返します。 考えられる停止基準は、分岐の前にノードの中に存在するべき最小のインスタンス数や、終端ノードに含まれるインスタンスの最小の数などがあります。 4.4.1 決定木の解釈 決定木の解釈は単純です: 根のノードから始めて、辺を辿って次のノードへと移っていきます。 葉に到達すると、そのノードから予測結果を得ることができます。 すべての枝は 'AND' で繋がっています。 例として、特徴量 x が 閾値 c より[小さい/大きい] かつ ... のようになります。 そして、予測結果は対応するノードに含まれるインスタンスの y の平均値になります。 特徴量重要度 (Feature importance) 決定木での特徴量の全体の重要度は次のように計算されます。 その特徴量が使われた全ての分岐を見て、それが親のノードに比べてどのくらい分散やジニ係数を減少させているかを計算します。 全部の重要度の和を100にスケーリングします。 これはそれぞれの重要度がモデル全体の重要度に対する寄与率として解釈できることを意味しています。 木の分解 決定木の個々の予測は決定経路を特徴量ごとに1つの要素に分解することで説明可能です。 木に沿って決定を追うことができ、それぞれの決定ノードに与えられた寄与度によって予測を説明できます。 根のノードは始点となります。 根のノードを最終的な予測のために使うのであれば、単に学習データ全ての結果の平均を出力することになります。 次の分岐では、経路上の次のノードによって、この和に項を減らしたり加えたりします。 最終的な結果を得るためには、説明したいデータの経路に従って、式に加え続ける必要があります。 \\[\\hat{f}(x)=\\bar{y}+\\sum_{d=1}^D\\text{split.contrib(d,x)}=\\bar{y}+\\sum_{j=1}^p\\text{feat.contrib(j,x)}\\] 個々のインスタンスの予測は、目的変数の平均に、根のノードからそのインスタンスの属する終端ノードの間で起こる D 回の分岐の全ての和を足したものになります。 ただし、分岐の寄与度ではなく、特徴量の寄与度に関心があります。 1つの特徴量は、2回以上使われるかもしれませんし、1回も使われないかもしれません。 p 個の特徴量それぞれで、寄与度は加算でき、それぞれの特徴量がどれだけ予測に寄与してるかを解釈できます。 4.4.2 例 自転車レンタル数のデータをみてみましょう。 決定木を用いて、ある日の自転車レンタル数を予測してみましょう。 学習した決定木は以下の通りです。 FIGURE 4.17: 自転車レンタル数のデータセットで学習された回帰木。 木の最大深さは 2 に設定されている。トレンド特徴量 (2011年からの経過日数) と気温 (temp) が分割に選ばれている。箱ひげ図は終端ノードにおける自転車レンタル数の分布を示している。 1段目と2段目の1つの分岐は、時間のトレンドの特徴量によって行われていますが、データ収集を開始してからの日数を考慮に入れているので、レンタルサービスがだんだんと人気になっていった様子が表現されています。 105 日目より前のとき、自転車の数の予測は約 1800 台で、106 ~ 430 日目の間は約 3900 台となりました。430 日目以降については、予測は 4600 台 (気温が12度以下のとき)、または、6600 台 (気温が12度以上のとき) となりました。 特徴量重要度を見ると、ある特徴量がノードの純度をどの程度向上させるかが分かります。 ここでは、自転車レンタル数の予測は回帰問題であるので、分散が使用されています。 可視化された決定木によって、温度と時間のトレンドの両方が分岐に使われていることはわかりますが、どの特徴量がどれほど重要かは定量化できていません。 特徴量重要度は時間のトレンドが温度よりも需要であることを示しています。 FIGURE 4.18: 平均的にノードの不純度がどの程度改善されたかによって計算された特徴量重要度 4.4.3 長所 決定木の構造は、データの特徴量間の相互作用を捉えるために理想的です。 データは最終的に個別のグループに分かれるので、線型回帰のような多次元の超平面上の点として理解するよりも簡単です。 決定木の構造は、ノードと辺により自然な描画が可能です。 決定木は&quot;人間に優しい説明&quot;の章で定義されているように、よい説明を与えることができます。 決定木の構造は、自動的に個々のインスタンスに対して反事実的に予測値を考えるように誘導します。 つまり、「この特徴量が分岐点より、大きい(小さい)なら、予測値は y2 ではなくて y1 であったのに。」というようになります。 決定木の説明は対照的です。なぜなら、いつでも予測値と、決定木によって定められた&quot;what if&quot;シナリオ(他のノードの葉)と比べられるからです。 もし、木の深さが1から3のように短かったら、最終的な説明は選択的です。 深さが 3 の決定木は、個々のインスタンスの予測の説明を得るために、最大3つの特徴量と分岐点を必要とします。 予測値の真実性は、決定木の予測性能に依存します。 短い木の説明は非常に単純かつ一般的です。なぜなら、それぞれの分岐において、インスタンスは左右いずれかのノードに分かれていくので、このような二者択一は理解しやすいからです。 特徴量を変換する必要はありません。 線形モデルでは、特徴量の対数をとる必要があるかもしれません。 決定木は、特徴量の任意の単調変換に対して等価な振る舞いをします。 4.4.4 短所 決定木は線形な関係をうまく扱うことができません。 任意の入力特徴量と結果の間の線形な関係は、分岐されて作られたステップ関数で近似されます。 これは効率的ではありません。 これは、滑らかさの欠如と密接に関係しています。 入力特徴量のわずかな変化が、予測結果に大きな影響を与える場合がありますが、これは望ましくありません。 家のサイズを特徴量の1つとした決定木で住宅の価格を予測する場合を考えてみましょう。 100.5 平方メートルで分岐が発生したとします。 この決定木の予測モデルを使ってユーザが家の価格を見積もるとどうなるでしょうか。 家のサイズは 99 平方メートルだったとして、それを入力すると 200, 000 ユーロという予測結果を得ました。 ユーザは 2 平方メートルの倉庫部屋の計算を忘れていたことに気がつきました。 また、その倉庫には斜めの壁があったため、全ての面積を計算するべきか、半分にするべきか確証を持てませんでした。 なので、100.0 平方メートルと 101.0 平方メートルの場合のどちらもを試すことに決めました。 結果として、200, 000 ユーロと 205, 000 ユーロという予測結果が得られましたが、これはユーザの直感に反します。なぜなら、99 平方メートルが 100 平方メートルになっても価格に変化が生じなかったためです。 決定木は、かなり不安定でもあります。 学習データがわずかに変わっただけで、全く異なった決定木が作られることがあります。 これは、それぞれの分岐が親の分岐に依存しているためです。 そのため、もし最初の分岐で異なる特徴量が選択されたとすると、全体の木構造に違いが生じます。 このように、構造が容易に変化するため、モデルに信頼性があるとは言えません。 決定木は木の深さが小さいときは非常に解釈しやすいです。 終端ノードの数は深さにともなって急激に増加します。 木の深さが深くなり、終端ノードの数が増加するにつれて、木の決定規則を理解することがより難しくなります。 深さが 1 のときは2つの終端ノード、深さが 2 のときは最大4つ、深さが 3 のときは最大8つと、最大の終端ノードの数は、2の(木の深さ)乗となります。 4.4.5 ソフトウェア この章の例では、CART (Classification And Regression Tree) の実装は rpart という R パッケージを用いました。 CART はPythonをはじめ、多くのプログラミング言語で実装されています。 間違いなく、CARTはかなり古く、使い古されたアルゴリズムであり、木を学習するためのいくつかの興味深い新しいアルゴリズムがあります。 決定木に関するいくつかの R パッケージの概要は Machine Learning and Statistical Learning CRAN Task View の &quot;Recursive Partitioning&quot; の項目のところに書かれています。 Friedman, Jerome, Trevor Hastie, and Robert Tibshirani. &quot;The elements of statistical learning&quot;. www.web.stanford.edu/~hastie/ElemStatLearn/ (2009).↩ "],["rules.html", "4.5 決定規則", " 4.5 決定規則 決定規則は、条件(前提とも呼ばれる)と予測値からなる単純なIF-THEN文です。 例えば、 もし、今日雨が降っていて4月であるなら(条件)、明日雨が降るだろう(予測)というものです。 予測は、単一の決定規則、もしくは、いくつかの決定規則の組み合わせによって行われます。 決定規則は一般的な構造に従います。 もし、条件を満たしているなら、特定の予測がされます。 決定規則は、おそらく最も解釈しやすい予測モデルです。 IF-THEN 構造は意味的には、自然言語や私たちの考え方に似ています。 ただし、条件がわかりやすい特徴量から構成され、条件の長さは短く(少数のANDで結合されている場合)、規則の数が多すぎない必要があります。 プログラミングでは IF-THEN のルールで書くことはとても自然です。 機械学習の新しい点は、決定規則がアルゴリズムによって学習されることです。 家の価値 ('low', 'medium', 'high') を予測するための決定規則をアルゴリズムによって学習することを想像してください。 このモデルから学習できた1つの決定規則として、もし家が 100 平方メートル以上の広さがあり、庭付きならば、価値は高い。 より正確には、IF 'size&gt;100 AND garden=1' THEN 'value=high' とかけます。 決定規則を分解してみましょう。 'size&gt;100' は、IF部分の第一条件です。 'garden=1' は、IF部分の第二条件です。 2つの条件を 'AND' で合わせて新しい条件を作ります。2つの条件がともに真のときのみ、規則は適用されます。 予測結果(THEN部分)は 'value=high' です。 決定規則は条件の中で少なくとも1つの 'feature=value' のステートメントを使用しますが 'AND' で追加できる数に制限はありません。 例外は、明示的なIF部分を持たないデフォルト規則で、他の規則が適用されない場合に適用されますが、これについては後で 解説します。 決定規則の有用性は通常、サポート (Support) と 正答率 (Accuracy) の2つの数字で表されます。 サポート (Support, 規則の適用範囲) 規則の条件が適用されるインスタンスの割合をサポートと呼びます。 例えば、size=big AND location=good THEN value=high という家の価値を予測する規則を考えてみましょう。 1000 軒中 100 軒が大きく、立地が良い場合、この規則のサポートは 10% となります。 予測部分 (THEN部分) は、サポートの計算には必要ありません。 正答率 (Accuracy, 規則の確信度) 規則の正答率とは、規則の条件が適用されるインスタンスに対して、どの程度、正しいクラスと予測できるかを示す指標です。 例えば、size=big AND location=good THEN value=high という規則が適用される 100 軒の中で、value=high が 85 軒、value=medium が 14 軒、value=low が 1 軒であったとすると、この規則の正答率は 85% となります。 通常、正答率とサポートはトレードオフの関係にあります。 条件に新しい特徴量を追加することで正答率を上げることができますが、サポートは低下します。 家の価値を予測するための良い分類器を作成するためには、1つの規則だけではなく、10 から 20 の規則を学習する必要があるかもしれません。 その時、より複雑なものになり、以下のような問題にぶつかるかもしれません。 規則の重複: 家の価値を予測したい時に、2つ以上の条件が適用され、それらが矛盾した予測結果であったとき、どうすればいいのでしょうか？ 規則の未適用: 家の価値を予測したい時に、どの規則も適用されないとき、どうしたらいいのでしょうか？ 複数の規則を組み合わせるとき、決定リスト(順序付き)、決定集合(順序無し)の2つの主な戦略があります。 両方の戦略は、規則の重複問題に対して、異なる解決策を提示します。 決定リストは決定規則に順序付けを用います。 あるインスタンスに対して、最初の規則が真であれば、予測に最初の規則を用います。 偽であるならば、次の規則に進み、その規則を適用するかどうか確かめ、これを繰り返します。 決定リストは、適用される最初の規則の予測のみを返すことで、規則の重複問題を解決します。 決定集合は、いくつかの規則が高い投票権を持っているかもしれないということを除いては、民主主義の原理に似ています。 集合の中では、規則が互いに排他的であるか、多数決のような重複を解決する戦略が存在し、個々の規則が正答率や他の評価指標によって重み付けされます。 ただし、複数の規則が適用されると、解釈性が損なわれる可能性があるため注意が必要です。 決定リストも決定集合も、あるインスタンスに対して、どの規則も適用されないという問題が起こり得ます。 これは、デフォルト規則 (default rule) を導入することによって解決できます。 デフォルト規則は、どの規則も適用されない場合に適用される規則のことです。 デフォルト規則の予測は、他の規則でカバーされていないデータ点の中で最も頻度の高いクラスとすることが多いです。 規則の集合やリストが、特徴量空間全体をカバーしているとき、網羅的と呼びます。 デフォルト規則を追加することで、決定集合や決定リストは自動的に網羅的になります。 データから規則を学習する方法はたくさん存在しますが、本書ではそれら全てをカバーしていません。 この章では、それらのうちの3つを紹介します。 これらのアルゴリズムは、規則を学習するための一般的な考え方を幅広くカバーするように選ばれたため、これら3つは非常に異なるアプローチとなっています。 OneR は、単一の特徴量から規則を学習します。 OneR の特徴は、単純かつ理解しやすいことであり、ベンチマークとして用いられます。 Sequential covering は、繰り返し規則を学習していき、新しい規則でカバーされるデータ点を削除するという一般的な手法です。 この手法は、多くの規則を学習するアルゴリズムで用いられています。 Bayesian Rule Lists は、ベイズ統計を用いて、あらかじめ発見された頻出パターンを決定リストに結合します。 事前に発見されたパターンを使用することも、多くの規則を学習するアルゴリズムで使用されているアプローチです。 規則を学習するために単一の最も良い特徴量を選ぶという、最も単純なアプローチから始めましょう。 4.5.1 単一の特徴量による規則学習 (OneR) Holte(1993)18によって提案された OneR アルゴリズムは、最も単純な規則を導出するアルゴリズムの1つです。 全ての特徴量から，OneR は興味のある出力について最も情報量をもつ特徴量を選び、その特徴量から決定規則を作成します。 &quot;One Rule&quot; の略である OneR という名前にもかかわらず，このアルゴリズムは1つより多くのルールを生成します。 実際には、選択された最良の特徴量の値ごとに1つの規則を作ります。 したがって、OneFeatureRules のほうがふさわしい名前かもしれません。 このアルゴリズムは単純かつ高速です。 適切な間隔 (intervals) を選ぶことで，連続的な特徴量を離散化 各特徴量に対して、次を実行 特徴量の値と (カテゴリカルな) 出力間でクロステーブルを作成 各特徴量の値ごとに、クロステーブルから読み取れる特定の特徴量を持つインスタンスの、最も頻度の高いクラスを予測するための規則を作成 特徴量に対して、規則の誤差の合計を計算 誤差の合計が最小となる特徴量を選択 OneR は、選択された特徴量の全ての値を使用するため、常にデータセットにおける全インスタンスをカバーします。 欠損値は、追加の特徴量の値として扱うか事前に代入されます。 OneR モデルは分割が1つしかない決定木です。 その分割は、CART のように二分木である必要はなく、ユニークな特徴量の値の数に依存します。 OneR によってどのように最も良い特徴量が選ばれているか、例を見てみましょう。 次の表は、家についての価格、ロケーション、サイズ、ペットの可否の情報を持つ人工的なデータセットを示しています。 家の価格を予測するための単純なモデルを学習してみましょう。 location size pets value good small yes high good big no high good big no high bad medium no medium good medium only cats medium good small only cats medium bad medium yes medium bad small yes low bad medium yes low bad small no low OneR は各特徴と出力との間のクロステーブルを生成します。 value=low value=medium value=high location=bad 3 2 0 location=good 0 2 3 value=low value=medium value=high size=big 0 0 2 size=medium 1 3 0 size=small 2 1 1 value=low value=medium value=high pets=no 1 1 2 pets=only cats 0 2 0 pets=yes 2 1 1 各特徴に対して、1行ごとにクロステーブルを見ていき、各特徴量の値が、ルールにおける IF部分 に相当します。 この特徴量を持つインスタンスの最も一般的なクラスが予測値、つまり、規則の THEN部分 に相当します。 例えば、サイズに対しては、small、medium、big の3つの規則が得られます。 各特徴量に対して，生成された規則の全ての誤差率 (誤差の総和) を計算します。 「ロケーション」の特徴量は、bad と good を取りうる特徴量です。 ロケーションが bad の家の最も出現頻度が高い価格は low ですが、low を予測値としたとき、2つの誤りが生じます。 なぜなら、ロケーションが bad かつ、価格が medium である家が2つ存在するからです。 ロケーションが good の家の予測値を high としても、ロケーションが good かつ価格が medium の家が 2 軒あるため、ここでも2つの誤りが生じます。 ロケーションを特徴量に用いると、その誤差は 4/10、大きさでは 3/10、ペットの可否では 4/10 です。 大きさを特徴量とすると、最も低い誤差を持った規則が生成できるため、これが最終的に OneR モデルに用いられます。 IF size=small THEN value=small IF size=medium THEN value=medium IF size=big THEN value=high OneR は、多くのレベルをもつ特徴量が選ばれる傾向にあります。なぜなら、それらの特徴量を用いると簡単に過学習してしまうためです。 全ての特徴量がランダムな値をもち、目的値に対して有用な値を持たないようなノイズのみを含むデータセットを想定してください。 いくつかの特徴量は他の特徴量より多くのレベルを持っています。 そのような多くのレベルを持った特徴量は過学習が起きやすくなります。 ある特徴量がインスタンスごとに異なるレベルを持っていたとすると、学習データ全体を完全に予測できてしまいます。 この問題に対する解決策は、データを学習用 (training data) と評価用 (validation data)に分けて、学習用のデータを用いて規則を学習し、選ばれた特徴量の評価は評価用のデータを用いて行います。 複数の特徴量が同じ誤差となるときが、もう1つの問題となります。 OneR では、このような場合は、誤差が最小の最初の特徴量を選択する、もしくは、カイ2乗検定の p値 が最小の特徴量を選択するようにします。 例 OneR を実データに適用してみましょう。 OneR アルゴリズムを子宮頸がんの分類タスク に適用してみます。 全ての連続な入力の特徴を5分位に離散化したところ、以下のような規則が作成されました。 Age prediction (12.9,27.2] Healthy (27.2,41.4] Healthy (41.4,55.6] Healthy (55.6,69.8] Healthy (69.8,84.1] Healthy OneR によって年齢の特徴量が最良の特徴量として選択されました。 がんは滅多に起こらないため、各規則はデータ数の多いクラスとなります。 従って、予測されるラベルが常に Healthy となり、これはあまり役に立たない結果と言えます。 このように、不均衡データに対するラベルの予測で使用しても意味がありません。 Age の間隔と Cancer/Healthy の間のクロステーブルに、癌にかかった女性の割合を加味するとより有益です。 # Cancer # Healthy P(Cancer) Age=(12.9,27.2] 26 477 0.05 Age=(27.2,41.4] 25 290 0.08 Age=(41.4,55.6] 4 31 0.11 Age=(55.6,69.8] 0 1 0.00 Age=(69.8,84.1] 0 4 0.00 ただし、解釈を始める前に注意しなければいけないことがあります。 全ての特徴量の全ての値に対する予測は Healthy だったため、全ての特徴量に対する合計の誤差率は同じです。 複数の特徴量で合計の誤差率が等しい場合、基本的には、最も誤差率の低い特徴量の中で、最初のものが使用されます(全ての特徴量は誤差率 55/858)。これがたまたま「Age feature」だったのです。 OneR は回帰問題では使用できません。 しかし、出力をいくつかの区間に分割することで回帰問題を分類問題に落とし込むことができます。 この手法を自転車レンタル台数予測に使ってみましょう。 自転車の数を四分位数(0~25%, 25~50%, 50~75%, 75~100%)で分割することで、OneR を用いて予測します。 OneR モデルで選択された特徴量の表は以下の通りです。 mnth prediction JAN [22,3152] FEB [22,3152] MAR [22,3152] APR (3152,4548] MAY (5956,8714] JUN (4548,5956] JUL (5956,8714] AUG (5956,8714] SEP (5956,8714] OKT (5956,8714] NOV (3152,4548] DEZ [22,3152] 選択された特徴量は月 (month) でした。 月の特徴量は（驚くべきことに！）12段階に分かれており、これは他のほとんどの特徴量よりも多いです。 そのため、過学習の危険性があります。 しかし、より楽観的な立場からすると、月の特徴量は季節のトレンド（例えば、冬はレンタル自転車の人気がなくなるなど）を捉えることができるため、その予測は賢明なのかもしれません。 それでは、単純な OneR アルゴリズムから、より複雑な手順で、いくつかの特徴量からなる複雑な条件を持つ規則を学習するための Sequential Covering に移りましょう。 4.5.2 Sequential Covering Sequential Covering とは、1つの規則を繰り返し学習し、ルールごとにデータセット全体をカバーする決定リスト（または決定集合）を作成する一般的な手続きです。 多くの規則を学習するアルゴリズムは、Sequential Covering の一種です。 この章では、手法の概要を紹介し、例として、Sequential Covering の応用形である RIPPER を使用します。 アイデアはシンプルです。 まずはいくつかのデータに当てはまる良い規則を見つけます。 そして、その規則でカバーされる全てのデータ点を削除します。 データ点がカバーされるのは、条件が適用されたときであり、その点が正しく分類されたかどうかとは関係がないことに注意してください。 この規則を学習し、カバーされた点を削除することを、残りのデータ点がなくなるか、他の停止条件が満たされるまで繰り返します。 その結果、決定リストが得られます。 この、規則の学習とカバーされたデータ点の削除を繰り返す手法を 「separate-and-conquer」と呼びます。 データの一部をカバーする単一のルールを作成できるアルゴリズムをすでに我々は持っているとします。 2つのクラス（positiveとnegative）に対する、sequential covering アルゴリズムは 以下のように動作します。 空の規則のリストから始める（rlist） 規則 r を学習 規則のリストがある閾値を下回っている間（もしくは positive な例がまだカバーされていない間）： 規則 r を rlist に追加 規則 r によってカバーされるデータ点を全て削除 残ったデータに対して、他の規則を学習 決定リストを返す FIGURE 4.19: アルゴリズムは単一の規則で特徴空間を順次カバーし、それらのルールで既にカバーされているデータ点を削除していくことで動作します。可視化のために、特徴量 x1 と x2 は連続量ですが、ほとんどの規則学習アルゴリズムはカテゴリカル特徴量を必要とします。 例として、家のサイズ、ロケーション、およびペットの可否から家の価値を予測するタスクおよびデータセットがあるとします。 初めに学習する規則は、もし、size=big かつ location=good ならば、value=high となります。 そして、データセットから全てのよいロケーションにある大きな家を削除します。 残ったデータで、我々は次の規則を学習すると、location=good ならば、value=medium となります。 注意すべき点としては、この規則は、ロケーションがよく大きな家を除いたデータで学習されており、ロケーションのいい家は medium か small しか残されていないということです。 多クラスの設定の場合は、アプローチを変える必要があります。 初めに、クラスは普及率を昇順に並べます。 sequential covering アルゴリズムは、最も一般的でないクラスから始まり、それのための規則を学習し、カバーされたインスタンスを全て削除し、次に一般的でないクラスに移動していきます。 現在のクラスは常にポジティブクラスとして扱われ、より高い普及率を持つ全てのクラスはネガティブクラスとしてまとめられます。 最後のクラスはデフォルト規則となります。 これは分類問題における one-versus-all 戦略とも呼ばれます。 どうやって1つのルールを学習するのでしょうか。 OneR アルゴリズムは、全ての特徴空間をカバーするので、役に立たないでしょう。 しかし、他にも多くのいろいろな可能性があります。 1つの可能性としては、ビームサーチを用いて決定木から単一の規則を学習することです。 決定木を（CARTや他の木学習アルゴリズムを用いて）学習します。 ルートノードから出発し、再帰的に最も不純度の低いノード（例：誤分類率が最も低いノード）を選択していきます。 規則の予測には、終端ノードにおける多数派のクラスが使用されます。つまり、そのノードに到達までのパスがルールの条件として使われます。 以下の図は、木をビームサーチした様子です。 FIGURE 4.20: 決定木のパスを探索することで規則を学習する。決定木は興味のある目的値を予測するために成長する。ルートノードから出発し、純度の高い(例: 正答率の高い)部分集合のパスへ貪欲的に遷移し、全ての分割の値を規則の条件に加える。最終的に、もし、location=good かつ size=big ならば value=highを得る。 単一の規則を学習することは、全ての可能な規則からなる空間が探索空間であるような探索問題です。 探索のゴールは、何らかの基準によって最適な規則を見つけることです。 いくつかの異なる探索の方策があります。 山登り法 (hill-climbing)、ビームサーチ (beam search)、全探索 (exhaustive search)、最良優先探索 (best-first search)、順序探索 (ordered search), 確率的探索 (stochastic search), トップダウン探索 (top-down search)、ボトムアップ探索 (bottom-up search)、など。 Cohen (1995)19 による RIPPER (Repeated Incremental Pruning to Produce Error Reduction) は Sequential Covering アルゴリズムの一種です。 RIPPER はより洗練されており、後処理 (rule pruning) を使って決定リスト (または、決定集合)を最適化します。 RIPPER は順序付き、順序なしモードで実行することができ、決定リストまたは決定集合のいずれかを生成できます。 例 例として、RIPPER を使用してみましょう。 RIPPER アルゴリズムは、子宮頸癌の分類問題において、規則を発見しません。 RIPPER を自転車レンタル台数の予測の回帰問題に適用したとき、いくつかの規則が見つかります。 RIPPER は分類問題に対して動作するため、自転車の数はカテゴリカルな出力に変換しなければいけません。 そのため、自転車の数は四分位数に変換しています。 例えば、(4548, 5956) は予測された自転車の数が 4548 台から 5956 台の間の区間を示しています。 次の表は、学習された規則の決定リストを示しています。 rules (days_since_2011 &gt;= 438) and (temp &gt;= 17) and (temp &lt;= 27) and (hum &lt;= 67) =&gt; cnt=(5956,8714] (days_since_2011 &gt;= 443) and (temp &gt;= 12) and (weathersit = GOOD) and (hum &gt;= 59) =&gt; cnt=(5956,8714] (days_since_2011 &gt;= 441) and (windspeed &lt;= 10) and (temp &gt;= 13) =&gt; cnt=(5956,8714] (temp &gt;= 12) and (hum &lt;= 68) and (days_since_2011 &gt;= 551) =&gt; cnt=(5956,8714] (days_since_2011 &gt;= 100) and (days_since_2011 &lt;= 434) and (hum &lt;= 72) and (workingday = WORKING DAY) =&gt; cnt=(3152,4548] (days_since_2011 &gt;= 106) and (days_since_2011 &lt;= 323) =&gt; cnt=(3152,4548] =&gt; cnt=[22,3152] 解釈は単純明快です。 もし、条件が適用されたら、右側の区間の自転車の台数であると予測します。 最後の規則はデフォルト規則で、インスタンスに対してどの規則も適用されなかったときに適用されます。 新しいインスタンスに対して予測するためには、リストの上から出発し、規則が適用されるかチェックします。 条件がマッチしたとき、規則の右側の値がこのインスタンスに対する予測となります。 デフォルト規則は、常に予測値が存在することを保証します。 4.5.3 Bayesian Rule Lists この章では、次の大まかな手順に従って、決定リストを学習する別のアプローチを紹介します。 決定規則の条件として使える頻出パターンをデータから事前にマイニングしておきます。 マイニングされた規則からいくつかを選択し、決定リストを学習します。 このようなアプローチを Bayesian Rule Lists (Lethan et. al, 2015)20または、略して BRL と呼びます。 BRL はベイズ統計を用いて、FP-tree アルゴリズム (Borgelt 2005)21 でマイニングされた頻出パターンから決定リストを学習しますが、まずは BRL の最初のステップからゆっくり始めましょう。 頻出パターンの事前マイニング 頻出パターンとは、特徴量の頻繁な共起のことを言います。 BRL アルゴリズムの前処理ステップとして、特徴量を使って頻出パターンを抽出します（この段階では目的値は不必要です）。 パターンには、size=medium のような単一の特徴量のものや、size=medium AND location=bad のような特徴量の組み合わせのものがあります。 パターンの頻度は次のように、データセット内のサポートで定量化されます。 \\[Support(x_j=A)=\\frac{1}n{}\\sum_{i=1}^nI(x^{(i)}_{j}=A)\\] ただし、A は特徴量の値、n はデータセット内のデータの数、I はデータ i の特徴 \\(x_j\\) のレベルが A の場合は 1、そうでない場合は 0 を返す指示関数です。 家の価値のデータセットで、もし家の20%にベランダがなく、80%で一個以上のベランダがあった場合、パターン balcony=0 に対するサポートは20%になります。 サポートは、balcony=0 AND pets=allowed のような、特徴量の組み合わせについても同様に測定できます。 AprioriやFP-Growth のような、頻出パターンを発見するためのアルゴリズムはたくさんあります。 結果のパターンは常に同じなので、計算速度だけが異なるため、どれを用いるかはそれほど重要ではありません。 Aprioriアルゴリズムがどのように頻繁なパターンを見つけるかについて大まかに説明します。 実は、Aprioriアルゴリズムは2つの部分で構成されており、まず最初に頻出パターンを見つけ、その次に、それらから相関規則を構築します。 BRL アルゴリズムにおいては、Aprioriアルゴリズムの最初の部分で生成される頻出パターンにのみ関心があります。 最初のステップでは、ユーザが定義した閾値より大きいサポートを持つすべての特徴量から始まります。 ユーザが最小のサポートを10%に設定しており、家の5%のみが size=big になっている場合、その特徴量の値は削除され、 size=medium と size=small のみがパターンとして保持されます。 これは、 size=big を持つ家がデータから削除されるということではなく、 size=big が頻出パターンとして返されなくなるという意味です。 Aprioriアルゴリズムは、単一の特徴量を持つ頻出パターンに基づいて、より高次の特徴量の組み合わせを繰り返し発見します。 パターンは、 feature=value ステートメントを論理 AND と組み合わせて構築されます。（例： size=medium AND location=bad ） 生成されたパターンのうち、閾値未満のサポートを持つものは削除されます。 最後には、すべての頻繁なパターンを持つことになります。 頻出パターンの部分集合もまた、頻出パターンになります。これはApriori propertyと呼ばれます。 これは、直感的にも成り立ちます。 パターンからある条件を外すと、削減された後のパターンは、より広い（または同じ）範囲のデータをカバーできるようになり、範囲が狭くなることはありえません。 例えば、家の20%が size=medium AND location=good ならば、 size=medium のみの家のサポートは20%以上になります。 このApriori propertyは、検査すべきパターンの数を減らすために使われます。 頻出パターンに対してのみ、高次のパターンをチェックする必要があります。 これで BRL アルゴリズムのための条件の事前マイニングが完了しました。 BRL の次のステップに進む前に、パターンの事前マイニングに基づく規則学習の別の方法を紹介します。 他のアプローチでは、関心のある出力結果を頻出パターンのマイニングプロセスに含め、Apriori アルゴリズムの2番目のステップである IF-THEN ルールを構築する部分でも使用することが提案されています。 教師なし学習アルゴリズムなので、THEN 部分に関心のない特徴量も含まれてしまいます。 ただし、THEN 部分に関心のある出力結果のみを持つ規則でフィルタリングできます。 これらの規則はすでに決定集合を形成していますが、規則の再配列、削除、再結合もできます。 しかしながら、BRL アルゴリズムでは、ベイズ統計を用いて頻出パターンから THEN 部分と決定リストに配置する方法を学習します。 Bayesian Rule Lists による学習 BRL アルゴリズムのゴールは、事前にマイニングされた条件から選択して、なるべく少ない規則、短い条件のリストとなることを優先させながら、正確な決定リストを学習することです。 BRL は、条件の長さ（短いルールで）と規則の数（短いリストで）に関する事前分布を用いて決定リストの分布を定義することにより、この目標を達成します。 リストの事後確率分布により、短さの仮定とどの程度データに適合しているかに基づいて、決定リストがどの程度尤もらしいかを言うことができます。 私たちの目標は、この事後確率を最大化するリストを見つけることです。 リストの分布から直接、最良のリストを見つけることはできないため、BRL は次のような手順に従います。 事前分布からランダムに最初の決定リストを生成します。 規則の追加、切り替え、または削除を繰り返し行い、結果のリストが、リストの事後分布に従うようにします。 事後分布に従ってサンプリングされたリストから最も確率の高い決定リストを選択します。 アルゴリズムをさらに詳しく見ていきましょう。 このアルゴリズムは、FP-Growth アルゴリズムを用いた特徴量のパターンを事前マイニングすることから始まります。 BRL は目的値の分布と、目的値の分布を定義するパラメータの分布について、いくつかの仮定をします。 （これがベイズ統計です。） ベイズ統計に慣れていない方は、以下の説明にとらわれすぎないようにしてください。 ベイズ統計のアプローチは、モデルをデータにフィットさせる一方で、既存の知識や必要条件（いわゆる事前分布）を組み合わせる方法であることを知っておくことが重要です。 決定リストの場合、決定リストの規則が短くなるように事前分布によって調整されるため、ベイズ統計のアプローチは理にかなっています。 ゴールは、事後分布から決定リスト d をサンプリングすることです。 \\[\\underbrace{p(d|x,y,A,\\alpha,\\lambda,\\eta)}_{posteriori}\\propto\\underbrace{p(y|x,d,\\alpha)}_{likelihood}\\cdot\\underbrace{p(d|A,\\lambda,\\eta)}_{priori}\\] ただし、d は決定リスト、x は特徴量、y は目的値、A は事前にマイニングされた条件の集合、\\(\\lambda\\) は事前に予想される決定リストの長さ、\\(\\eta\\) は事前に予想される規則の中の条件の数、\\(\\alpha\\) は正と負クラスに対する事前の擬似的なカウントで、 (1,1) に固定する方が良いです。 \\[p(d|x,y,A,\\alpha,\\lambda,\\eta)\\] この式は観測されたデータと事前の仮定に基づいて、決定リストの可能性を定量化します。 これは、決定リストとデータが与えられたときの出力 y の尤度と、与えられた事前情報と事前にマイニングされた条件に対するリストの確率をかけたものに比例します。 \\[p(y|x,d,\\alpha)\\] この式は、決定リストとデータが与えられたときに観測された y の尤度です。 BRL では y はディリクレ多項分布 (Dirichlet-Multinomial distribution) によって生成されることを仮定しています。 決定リスト d がデータをうまく説明できるほど、尤度は高くなります。 \\[p(d|A,\\lambda,\\eta)\\] この式は、決定リストの事前分布です。 これは、リスト内の規則の数に対するパラメータ \\(\\lambda\\) の truncated Poisson distribution と 規則の条件の特徴量の値の数に対するパラメータ \\(\\eta\\) の truncated Poisson distribution を掛け合わせます。 決定リストは、出力 y をうまく説明し、事前の仮定に従っている可能性が高いほど、事後確率が高くなります。 ベイズ統計の推定には少しトリッキーです。なぜなら、直接正解を計算できるとは限らず、通常は、候補を選んで評価し、マルコフ連鎖モンテカルロ法 (MCMC) を用いて事後推定を更新する必要があるからです。 決定リストの場合、決定リストの分布から引き出す必要があるため、さらに複雑になります。 BRL の著者は、まず最初の決定リストを作成し、次にそれを繰り返し変更して、リストの事後分布（決定リストのマルコフ連鎖）から決定リストのサンプルを生成することを提案しています。 これによって得られる結果は最初の決定リストに依存する、この手順を繰り返し実行し、多様なリストを確保することが望ましいです。ソフトウェアの実装の中では、基本的に10回繰り返します。 以下の手順は、最初の決定リストの作り方を示しています。 FP-Growthでパターンを事前にマイニング truncated Poisson distribution から、リストの長さのパラメータ m をサンプリング デフォルト規則の場合 (他に何も適用しない場合に用いられるルール)は以下を実行 目的値に関するディリクレ多項分布のパラメータ \\(\\theta_0\\) をサンプリング 決定リストの規則 j = 1,...,m に対して、以下を実行 規則 j に対して、規則の長さのパラメータ l (条件の数) をサンプリング 事前にマイニングした条件から、長さが \\(l_j\\) の条件をサンプリング THEN部分(規則によって与えられた出力結果の分布)に対して、ディリクレ多項分布のパラメータをサンプリング データセットのそれぞれの観測値に対して以下を実行 決定リストを上から下に探索し、最初に適用する規則を見つける 適合するルールによって提案された確率分布 (二項分布) から予測結果を引き出す 次のステップは、決定リストの事後分布から多くのサンプルを取得するために、この最初のサンプルからスタートし、たくさんの新しいリストを生成することです。 新しい決定リストは最初のリストから開始し、規則をリスト内の別の場所に移動するか、事前にマイニングされた条件から現在の決定リストに規則を追加するか、もしくは決定リストから規則を削除することによってサンプリングされます。 これらの規則の切り替え、追加、削除は無作為に選ばれて適用されます。 それぞれのステップにおいて、アルゴリズムは決定リストの（正答率と短さの組み合わさった）事後確率を評価します。 Metropolis Hastings アルゴリズムは、事後確率が高い決定リストをサンプリングすることを保証します。 この手順によって、決定リストの分布から多くのサンプルを得ることができます。 BRL アルゴリズムは最も高い事後確率を持つサンプルの決定リストを選択します。 例 理論はこれぐらいにして、BRL 法の動作を見てみましょう。 例では、Yang らによる BRL をより高速化した Scalable Bayesian Rule List (SBRL, 2017) 22を使用します。 SBRL アルゴリズムを子宮頸がんのリスクの予測に適用します。 まずはじめに、全ての入力特徴量を SBRL アルゴリズムで使用可能なように離散化する必要があります。 この目的のために、連続特徴量は分位数の頻度に基づいてビン化しています。 すると、以下のようなルールを得ることができます。 rules If {STDs=1} (rule[259]) then positive probability = 0.16049383 else if {Hormonal.Contraceptives..years.=[0,10)} (rule[82]) then positive probability = 0.04685408 else (default rule) then positive probability = 0.27777778 予測の THEN 部分がクラスの結果ではなく、がんの予測確率であるため、実用的なルールを得ることができていることに注意してください。 条件は、あらかじめ探索された FP-Growth アルゴリズムを使って得られたパターンから選択されました。 次の表は、SBRL アルゴリズムが決定リストを作成するために選択できる条件の候補を示しています。 ユーザが設定した、条件に含まれる最大の特徴量の数は 2 としています。 以下が 10 パターンの例です。 pre-mined conditions Num.of.pregnancies=[3.67,7.33) IUD=0,STDs=1 Number.of.sexual.partners=[1,10),STDs..Time.since.last.diagnosis=[1,8) First.sexual.intercourse=[10,17.3),STDs=0 Smokes=1,IUD..years.=[0,6.33) Hormonal.Contraceptives..years.=[10,20),STDs..Number.of.diagnosis=[0,1) Age=[13,36.7) Hormonal.Contraceptives=1,STDs..Number.of.diagnosis=[0,1) Number.of.sexual.partners=[1,10),STDs..number.=[0,1.33) STDs..number.=[1.33,2.67),STDs..Time.since.first.diagnosis=[1,8) 次に、自転車レンタル予測のタスクにも、SBRL アルゴリズムを適用してみましょう。 これは、自転車の数を予測する問題が、二値分類の問題に変換できたときのみ使用可能です。 そのため、ここでは恣意的に 1 日の自転車レンタル数が 4000 を超えるとき 1 , そうでないときは0とラベルを付与することで、分類問題に変換しています。 rules If {yr=2011,temp=[-5.22,7.35)} (rule[718]) then positive probability = 0.01041667 else if {yr=2012,temp=[7.35,19.9)} (rule[823]) then positive probability = 0.88125000 else if {yr=2012,temp=[19.9,32.5]} (rule[816]) then positive probability = 0.99253731 else if {season=SPRING} (rule[351]) then positive probability = 0.06410256 else if {temp=[7.35,19.9)} (rule[489]) then positive probability = 0.44444444 else (default rule) then positive probability = 0.79746835 気温が摂氏 17 度で、2012 年の 1日で自転車の数が 4000 を超える確率を予測してみましょう。 最初のルールは、2011 年の時のみ適用されるため、今回は適用されません。 2012 年で 17 度のときは、区間 [7.35,19.9) に入っているので、 2つ目のルールは適用されます。 予測の結果、4000 台を超える確率は 88% となりました。 4.5.4 長所 この章では一般的な IF-THEN ルールの長所について議論します。 IF-THEN ルールは解釈することが簡単です。 これはおそらく最も解釈しやすい解釈可能モデルと言えます。 ただし、このように言えるのは、ルールの数が少ないときに限られ、ある規則の条件が少なく(多くとも3が好ましい)、規則が決定リストか重複のない決定集合で管理される場合です。 決定規則は決定木のように表現力がありながら、よりコンパクトです。 決定木は複製された部分技に苦しむことが多く、これは、分岐点の左右の子ノードが同じ構造を持つときに起こります。 どのルールに決めるのかの少数のバイナリステートメントを確認するだけなので、IF-THEN ルールの予測は高速です。 決定規則は、入力特徴量の単調変換に対しては、条件に関する閾値が変わるだけなので、頑健です。 条件が適用されるかどうかの問題なので、外れ値に対しても頑健です。 IF-THEN ルールは通常、少数な特徴量だけを含むスパースなモデルを生成します。 モデルに関連する特徴量だけを選択するのです。 例えば、線形モデルは基本的にはすべての入力特徴量に重みを割り当てます。 無関係な特徴量は IF-THEN ルールでは、単に無視されるでしょう。 OneR のような単純な規則は、より複雑なアルゴリズムに対するベースラインとして使えるでしょう。 4.5.5 短所 この章では一般的な IF-THEN ルールの欠点について扱います。 IF-THEN ルールに関する研究や書物では分類に焦点を当ていて、完全に回帰を無視しています。 ほとんどの場合、連続値をある区間に分割することで分類問題に変形できますが、それによって必ず情報を失います。 一般的に、回帰と分類の両方に使える方法はより魅力的です。 また、特徴量はカテゴリカルでなければいけません。 つまり、量的特徴量を使いたいときは、カテゴリカル化しなければいけません。 連続値の特徴量をある区間に切る方法は沢山ありますが、これは自明なことではなく、明確な答えのない多くの疑問が付随します。 いくつの区間で特徴量を分けるべきか、分割の基準はなにか、固定長の区間か、分位点か、その他のなにかか。 連続値の特徴量をカテゴリカル化するのは重大な問題であるのに、無視されがちで、多くの人はここで例示したように、単に次の最も良い方法を使います。 多くの古いルール学習アルゴリズムは過学習する傾向があります。 ここで紹介したすべてのアルゴリズムは、過学習を防ぐために少なくともいくつかの安全策を講じています。 OneR は1つの特徴量しか使わないように制限されており (ただし、特徴量が多すぎるレベルを持っていたり、多重検定問題に相当するような特徴量が多すぎる場合には問題になります)、RIPPER ではプルーニングを行い、Bayesian Rule Lists では決定リストの事前分布として制約を課しています。 決定規則は、特徴量と出力との線形な関係を表現することには向いていません。 これは、決定木と共通する問題です。 決定木や決定規則はステップ状の予測関数しか生成できないため、常に予測の変化は離散的な階段状となり滑らかなカーブにはなりません。 これは、入力がカテゴリカルでなければいけないことに関連した問題です。 決定木では、分割によって暗黙的にカテゴリカル化が行われています。 4.5.6 ソフトウェアと代替手法 OneR は R パッケージ OneRに実装されており、この本の例でも使用されています。 OneR は機械学習ライブラリのWekaにも実装されており、Java や R 、そしてPythonで利用できます。 RIPPER も Weka で実装されています。例えば、私は RWekaパッケージ内の JRIP の R 実装を使いました。 SBRL も、この本の例で実行しているように、Rパッケージで利用できます。 他にも、Python や C言語 でも使えます。 決定集合や決定リストを学習する方法の全ての代替手法をリスト化することはしていませんが、ここでは、それらの要約を紹介します。 Fuernkranz らによる &quot;Foundations of Rule Learning&quot; (2012)23 の 本をおすすめします。 これは、決定規則に関して、より深い知識を身に付けたい人にとって役立つでしょう。 この本では、学習規則を考えるための全体的なフレームワークや、多くの規則を学習させるアルゴリズムを紹介しています。 また、こちらの資料(Weka rule learners) もおすすめで、RIPPER、 M5Rules、 OneR、 PART、その他諸々の実装があります。 IF-THEN ルールは、この本のRuleFit algorithmの章で述べられている通り、線形モデルで使用できます。 Holte, Robert C. &quot;Very simple classification rules perform well on most commonly used datasets.&quot; Machine learning 11.1 (1993): 63-90.↩ Cohen, William W. &quot;Fast effective rule induction.&quot; Machine Learning Proceedings (1995). 115-123.↩ Letham, Benjamin, et al. &quot;Interpretable classifiers using rules and Bayesian analysis: Building a better stroke prediction model.&quot; The Annals of Applied Statistics 9.3 (2015): 1350-1371.↩ Borgelt, C. &quot;An implementation of the FP-growth algorithm.&quot; Proceedings of the 1st International Workshop on Open Source Data Mining Frequent Pattern Mining Implementations - OSDM ’05, 1–5. http://doi.org/10.1145/1133905.1133907 (2005).↩ Yang, Hongyu, Cynthia Rudin, and Margo Seltzer. &quot;Scalable Bayesian rule lists.&quot; Proceedings of the 34th International Conference on Machine Learning-Volume 70. JMLR. org, 2017.↩ Fürnkranz, Johannes, Dragan Gamberger, and Nada Lavrač. &quot;Foundations of rule learning.&quot; Springer Science &amp; Business Media, (2012).↩ "],["rulefit.html", "4.6 RuleFit", " 4.6 RuleFit Friedman と Popescu (2008)24 による RuleFit アルゴリズムは、相互作用効果を決定規則の形で自動的に検出したスパース線形モデルの学習に使われます。 線形回帰モデルは特徴間の相互作用を考慮していません。 線形モデルのようにシンプルで解釈しやすいモデルでありながら、特徴間の相互作用を統合したモデルがあれば便利ではないでしょうか？ 実は、RuleFit はギャップを埋めることができます。 RuleFit は、元の特徴量と決定規則である多数の新しい特徴量を用いて、スパース線形モデルを学習します。 これらの新しい特徴量は、元の特徴量間の相互作用を説明します。 RuleFit は、決定木からこれらの特徴量を自動的に生成します。 分割された決定を結合し、規則にすることで、木を通る各パスを決定規則に変換できます。 ノードによる予測を破棄し、分割のみを決定規則に使用します。 FIGURE 4.21: 3つの終端ノードを持つ木から4つの規則を生成することができます。 これらの決定木はどこから来ているのでしょうか？ 決定木は興味のある結果を予測するために学習されます。 これは、予測問題に対して分割が意味を持つことを保証しています。 ランダムフォレストのように、多数の木を生成するアルゴリズムを RuleFit に使うことができます。 それぞれの木は、スパース線形回帰モデル (Lasso) で使用される追加の特徴量である決定規則に分解されます。 RuleFit が提案された論文では、ボストンの住宅データを使って説明しています。 目標は、ボストンの住宅の中央値を予測することです。 RuleFit で生成された規則の1つは、部屋の数 &gt; 6.64 かつ 一酸化炭素濃度 &lt; 0.67 であるならば 1、そうでないならば 0 としています。 RuleFit は、予測に重要な線形項や規則を特定するための重要度の指標としても機能します。 特徴量の重要度は回帰モデルの回帰係数から計算されます。 重要度の指標は元の特徴量（&quot;生&quot;の形で使われ、多くの決定規則で使われる可能性があります）を集約したものになります。 RuleFit は、特徴量を変更することで、予測値の平均的な変化を示す partial dependence plot を導入します。 partial dependence plot は、どんなモデルにも使うことのできるモデル診断の手法であり、partial dependence plotsの章で解説されています。 4.6.1 解釈と例 RuleFit は最終的には線形モデルを推定するので、解釈は&quot;普通の&quot;線形モデルと同じになります。 違いは、決定規則からなる新しい特徴量を有していることです。 値が 1 であることは全ての条件を満たしていることを示し、そうでない場合は値は 0 になります。 RuleFit における線形項の解釈は、線形回帰モデルと同様になります。ある特徴量が 1 増加すると、予測結果は特徴量の重みに応じて変化します。 この例では、RuleFit をある日付のレンタル自転車数を予測するために使用しています。 この表は、RuleFit によって生成された5つの規則と、それらの Lasso による重みと重要性を示しています。 計算については、この章の後半で説明します。 Description Weight Importance days_since_2011 &gt; 111 &amp; weathersit in (&quot;GOOD&quot;, &quot;MISTY&quot;) 793 303 37.25 &lt;= hum &lt;= 90 -20 272 temp &gt; 13 &amp; days_since_2011 &gt; 554 676 239 4 &lt;= windspeed &lt;= 24 -41 202 days_since_2011 &gt; 428 &amp; temp &gt; 5 366 179 最も重要な規則は &quot;days_since_2011 &gt; 111 &amp; weathersit in (&quot;GOOD&quot;, &quot;MISTY&quot;)&quot; であり、対応する重みは 793 です。 このような規則は、元の 8 つの特徴量から合計 278 個が作成されました。 かなりの数です! しかし Lasso のおかげで、278 のうち 58 だけが 0 ではない重みを持っていることがわかります。 大域的な特徴量の重要性を計算すると、気温と時間の傾向が最も重要な特徴量であることがわかります。 FIGURE 4.22: 自転車の数を予測する RuleFit モデルの特徴量重要度。予測のために最も重要な特徴量は気温と時間傾向でした。 特徴量重要度の尺度は、生の特徴量の重要度と、その特徴量が現れるすべての決定規則を含みます。 解釈のテンプレート 解釈は、線形モデルと類似しています。 他の特徴量が固定されている場合、特徴量 \\(x_j\\) が 1 変化すると、予測結果は \\(\\beta_j\\) だけ変化します。 決定規則に関する重みの解釈は特殊です。 決定規則 \\(r_k\\) のすべての条件を満たすならば、予測結果は \\(\\alpha_k\\) (線形モデルで学習された規則 \\(r_k\\) の重み) だけ変化します。 分類問題では（線形回帰ではなくロジスティック回帰を用いた場合）、 決定規則 \\(r_k\\) の全ての条件を満たすなら、その事象が発生するかしないかのオッズが \\(\\alpha_k\\) 倍変化します。 4.6.2 理論 RuleFit アルゴリズムの技術的な詳細について深く見ていくことにしましょう。 RuleFit は2つのコンポーネントで構成されています。 最初のコンポーネントは、決定木から&quot;規則&quot;を作成し、2番目のコンポーネントでは、元の特徴量と作成した規則を入力とする線形モデルを学習します（これが &quot;RuleFit&quot; という名前の由来です）。 Step 1: 規則の生成 規則とはどのようなものでしょうか？ アルゴリズムによって生成された規則は単純な形式になります。 例えば、IF x2 &lt; 3 AND x5 &lt; 7 THEN 1 ELSE 0 といったものです。 規則は、決定木を分解することで構築されます。 決定木上の任意のパスは、決定規則に変換できます。 規則のための木は、出力を予測するために利用されます。 したがって、分割や得られる規則は興味のある結果を得るために最適化されています。 特定のノードに至る二分決定を &quot;AND&quot; で連結させるだけで規則ができます。 多様かつ意味のある規則を多く生成することが望まれます。 勾配ブースティングでは、y を元の特徴量 X を使って回帰あるいは分類をすることで、決定木のアンサンブルを学習させます。 そして作成された各々の木は、複数の規則に変換されます。 ブースティングに限らず、任意の木のアンサンブルアルゴリズムに対して、RuleFit の木を生成できます。 木のアンサンブルは、次の一般的な式で記述できます。 \\[f(x)=a_0+\\sum_{m=1}^M{}a_m{}f_m(X)\\] M は木の数であり、\\(f_m(x)\\) は m 番目の木の予測関数です。 \\(a\\) は重みです。 Bagged ensembles、ランダムフォレスト、AdaBoost、そして MART は木のアンサンブルを生成し、RuleFit で使用されます。 アンサンブルの全ての木から規則を作成します。 各規則 \\(r_m\\) は次の形式で表されます。 \\[r_m(x)=\\prod_{j\\in\\text{T}_m}I(x_j\\in{}s_{jm})\\] ここで、\\(\\text{T}_{m}\\) は、m 番目の木で利用される特徴量の集合です。 I は、特徴量 \\(x_j\\) が j 番目の特徴量（木の分割で指定されたもの）に対する部分集合 s に含まれる場合に 1、それ以外の場合に 0 となる指示関数です。 量的特徴量の場合、\\(s_{jm}\\) は特徴量の値の区間となります。 区間は次の2つの場合のいずれかのようになります。 \\[x_{s_{jm},\\text{lower}}&lt;x_j\\] \\[x_j&lt;x_{s_{jm},upper}\\] 特徴量を更に分割すると、より複雑な区間になる可能性があります。 カテゴリカル特徴量の場合、部分集合は特徴量の特定のカテゴリが含まれることになります。 自転車レンタルのデータセットの例を見てみましょう。 \\[r_{17}(x)=I(x_{\\text{temp}}&lt;15)\\cdot{}I(x_{\\text{weather}}\\in\\{\\text{good},\\text{cloudy}\\})\\cdot{}I(10\\leq{}x_{\\text{windspeed}}&lt;20)\\] この規則は、3つの条件全てが満たされた場合に 1、それ以外は 0 を返します。 RuleFit は、葉だけではなく、木の全てのノードから規則を抽出します。 したがって、作成されるであろう規則は次のようになります。 \\[r_{18}(x)=I(x_{\\text{temp}}&lt;15)\\cdot{}I(x_{\\text{weather}}\\in\\{\\text{good},\\text{cloudy}\\}\\] 全体として、\\(t_m\\) 個の葉をもつ M 個の木のアンサンブルから作成される規則の数は次式で与えられます。 \\[K=\\sum_{m=1}^M2(t_m-1)\\] RuleFit の著者によって導入されたトリックは、ランダムな深さの木を学習することで、長さの異なる多種多様な規則を生成するというものです。 各ノードにおける予測値は破棄して、そのノードに至る条件のみを保持し、そこから規則を作るということに注意してください。 決定規則の重みづけは、RuleFit の第2ステップで行われます。 ステップ1はこのように見ることもできます。 RuleFit は、元の特徴量から新しい特徴量の集合を生成します。 これらの特徴量は、二値であり、元の特徴量の極めて複雑な相互作用を表現できます。 規則は予測タスクで最良の結果が得られるように選択されます。 規則は、共変量行列Xから自動的に生成されます。 規則は元の特徴量に基づく新たな特徴量としてみなすことができます。 Step 2: スパース線形モデル ステップ1で、多くの規則を得ることができます。 この最初のステップは、単なる特徴量の変換にすぎないため、モデルへの適合はまだ終わっていません。 また、規則の数を減らしたいとも思うでしょう。 これらの規則に加えて、元のデータセットの全ての&quot;生&quot;の特徴量も、スパース線形モデルで利用することになります。 全ての規則と元の特徴量が線形モデルの特徴量となり、重みが推定値されます。 元の生の特徴量を追加するのは、木は y と x の間の単純な線形関係を表現するのに失敗するためです。 スパース線形モデルを学習する前に、元の特徴量の外れ値をクリッピング (winsorizing) し、外れ値に対してより頑健になるようにします。 \\[l_j^*(x_j)=min(\\delta_j^+,max(\\delta_j^-,x_j))\\] ここで、\\(\\delta_j^-\\) と \\(\\delta_j^+\\) は、特徴量 \\(x_j\\) のデータ分布の \\(\\delta\\) 分位数です。 \\(\\delta\\) に0.05を選択すると、上位 5％ または下位 5％ の特徴量 \\(x_j\\) の値が、それぞれ 5％ または 95％ の分位数に設定されます。 経験則として、\\(\\delta\\) = 0.025 を選択できます。 さらに、線形項は、通常の決定規則と事前の重要性が同一となるように正規化する必要があります。 \\[l_j(x_j)=0.4\\cdot{}l^*_j(x_j)/std(l^*_j(x_j))\\] \\(0.4\\) は、\\(s_k\\sim{}U(0,1)\\) の一様なサポート分布を持つ規則の標準偏差の平均です。 両方のタイプの特徴量を組み合わせて、新たな特徴量行列を作成し、次の形式で Lasso を利用してスパース線形モデルを学習します。 \\[\\hat{f}(x)=\\hat{\\beta}_0+\\sum_{k=1}^K\\hat{\\alpha}_k{}r_k(x)+\\sum_{j=1}^p\\hat{\\beta}_j{}l_j(x_j)\\] ここで、\\(\\hat{\\alpha}\\) は、規則の特徴量に対して推定された重みベクトルであり、\\(\\hat{\\beta}\\) は、元の特徴量に対する重みベクトルです。 RuleFit は Lasso を利用するため、損失関数は、一部の重みを 0 にするための制約が必要になります。 \\[(\\{\\hat{\\alpha}\\}_1^K,\\{\\hat{\\beta}\\}_0^p)=argmin_{\\{\\hat{\\alpha}\\}_1^K,\\{\\hat{\\beta}\\}_0^p}\\sum_{i=1}^n{}L(y^{(i)},f(x^{(i)}))+\\lambda\\cdot\\left(\\sum_{k=1}^K|\\alpha_k|+\\sum_{j=1}^p|b_j|\\right)\\] この結果は、元の全ての特徴量と規則に対して線形な効果をもつ線形モデルです。 解釈は、線形モデルの場合と同様ですが、唯一の違いは、一部の特徴量が二値の規則となっている点です。 Step3（optional）: 特徴量重要度 元の特徴量の線形項については、標準化された予測器を利用して特徴量重要度を測定します。 \\[I_j=|\\hat{\\beta}_j|\\cdot{}std(l_j(x_j))\\] ここで、\\(\\beta_j\\) は、Lasso モデルから得られた重みであり、\\(std(l_j(x_j))\\) はデータ全体の線形項の標準偏差です。 決定規則の項の場合、重要度は次式で計算されます。 \\[I_k=|\\hat{\\alpha}_k|\\cdot\\sqrt{s_k(1-s_k)}\\] ここで、\\(\\hat{\\alpha}_k\\) は、決定規則の関連するLassoの重みであり、\\(s_k\\) は、データにおける特徴量のサポートであり、決定規則が適用されるデータの割合です（ここで、\\(r_k(x)=1\\) ）。 \\[s_k=\\frac{1}{n}\\sum_{i=1}^n{}r_k(x^{(i)})\\] 特徴量は線形項として現れるだけでなく、場合によっては多くの決定規則の内部にも現れます。 どのように特徴量の重要度を測るべきでしょうか？ 特徴量の重要度 \\(J_j(x)\\) は、個々の予測ごとに測定できます。 \\[J_j(x)=I_j(x)+\\sum_{x_j\\in{}r_k}I_k(x)/m_k\\] ここで、\\(I_l\\) は線形項の重要度、\\(I_k\\) は \\(x_j\\) が現れる決定規則の重要度、\\(m_k\\) は規則 \\(r_k\\) を構成する特徴量の数です。 全ての事例から特徴量の重要度を足し合わせることで、大域的な重要度を得ることができます。 \\[J_j(X)=\\sum_{i=1}^n{}J_j(x^{(i)})\\] 事例の部分集合を選択して、そのグループの特徴量重要度を計算できます。 4.6.3 長所 RuleFitは特徴量間の相互作用を線形モデルに自動で追加します。 したがって、相互作用項を手動で追加する必要のある線形モデルの問題を解決し、非線形関係をモデリングする問題にも少し役立ちます。 RuleFit は分類問題と回帰問題の両方を扱えます。 作成される決定規則は二値であるため、規則が観測データに適用されるかどうかを調べることで簡単に解釈できます。 優れた解釈可能性は、決定規則内の条件の数が多すぎない場合にのみ保証されます。 個人的には、1〜3 個の条件の決定規則が合理的だと思います。 つまり、アンサンブルの木の最大の深さは 3 が良いということです。 たとえモデルに多くの決定規則がある場合でも、それらがすべての観測データに適用されるわけではありません。 個々の観測データにはほんのひと握りの決定規則のみ（= 非ゼロの重みを持つ）が適用されます。 これにより、個々のデータに対する解釈可能性が向上します。 RuleFitは便利な診断ツールを多数提供しています。 これらのツールはモデルに依存しないため、この本のモデル非依存 (model-agnostic) のセクションで紹介されています：特徴量重要度、partial dependence plots、特徴量の相互作用。 4.6.4 短所 RuleFit は、Lasso モデルにおいて非ゼロな重みを得るたくさんの規則を作り出すことがあります。 解釈性は特徴量の数が増えるにつれ低下します。 有望な解決策としては特徴量の影響を単調にすることです。 つまり、特徴量が増加すると、予測結果も増加する必要があるということです。 論文では度々 RuleFit の性能が、ランダムフォレストの予測性能に匹敵するほど良いと主張しています。 しかしながら、私が個人的に試したいくつかの場合において、がっかりするような性能でした。 まず、適用してみてどのような性能が出るかを確認しましょう。 RuleFit の手順をふんで得られる最終生成物は、追加の特徴（決定規則）を持つ線形モデルです。 しかし、線形モデルであるからこそ、重みの解釈が直感的ではありません。 通常の線形回帰モデルと同様に、&quot;...他の全ての特徴量が固定されている場合に限る。&quot;という&quot;脚注&quot;がついています。 また、規則が重複していると少し厄介になります。 例えば、自転車の数を予測するための1つの決定規則（特徴量）として &quot;temp &gt; 10&quot; と &quot;temp &gt; 15 &amp; weather='GOOD'&quot; があるとします。 天気が良く、気温が15度以上であれば、自動的に気温が10度以上になります。 2つ目の規則が満たされているときに、1つ目の規則も満たされています。 2つ目の規則における推測された重みの解釈は&quot;他の特徴量が固定され、天気が良く、気温が15度以上のとき、予測される自転車の数は \\(\\beta_2\\) 増加する。&quot;となります。 しかしここで、&quot;他の特徴量が固定された場合&quot;というのが問題になってきます。 なぜなら、2つ目の規則が適合しているとき、1つ目の規則にも適合し、解釈が意味の無いものになってしまうからです。 4.6.5 ソフトウェアと代替手法 RuleFit アルゴリズムは R では Fokkema と Christoffersen (2017)25 によって実装されています。 Python 実装は Github 上にもあります。 非常によく似たフレームワークは skope-rules という Python のモジュールでアンサンブルから規則を抽出します。 これは最終的な規則を学習する方法が違います。 まず、skope-rules はパフォーマンスのよくない規則を、recall（再現性）とprecision（適合率）に基づいて除去します。 そして、重複あるいは似ている規則が、論理項（変数 + 大なり／小なり）の多様性や F1-score に基づいて除去します。 最後に Lasso を用いる代わりに、out-of-bag の F1-score や規則を構成する論理項を用います。 Friedman, Jerome H, and Bogdan E Popescu. &quot;Predictive learning via rule ensembles.&quot; The Annals of Applied Statistics. JSTOR, 916–54. (2008).↩ Fokkema, Marjolein, and Benjamin Christoffersen. &quot;Pre: Prediction rule ensembles&quot;. https://CRAN.R-project.org/package=pre (2017).↩ "],["other-interpretable.html", "4.7 その他の解釈可能なモデル", " 4.7 その他の解釈可能なモデル 解釈可能なモデルの種類は増加し続けており、どれぐらいの数があるかわかりません。 線形回帰や決定木、単純ベイズ分類器のようなシンプルなモデルもあれば、解釈性の低いモデルを組み合わせたり変更することでより解釈性を高めたような複雑なものもあります。 特に後者のタイプのモデルは現在、高頻度で開発・発表されており、それらについていくのは大変です。 この章では単純ベイズ分類器とk近傍法について軽く紹介します。 4.7.1 単純ベイズ分類器 (Naive Bayes Classifier) 単純ベイズ分類器は条件付き確率のベイズの定理を用います。 特徴量ごとに、クラスに属する確率を特徴量の値に基づいて計算します。 単純ベイズ分類器は各特徴量が互いに独立しているという強い(=単純な)仮定を置いていることになります。 単純ベイズは条件付き確率モデルであり、クラス \\(C_k\\) の予測確率を次のようにモデル化します。 \\[P(C_k|x)=\\frac{1}{Z}P(C_k)\\prod_{i=1}^n{}P(x_i|C_k)\\] Z はすべてのクラスの確率の合計が 1 になるようにするための規格化定数です(そうしなければ確率として扱えなくなります)。 クラスの条件付き確率は、クラスの確率とクラスが与えられたときのそれぞれの特徴量の確率の積を Z で正規化したものです。 この式はベイズの定理を用いて導出できます。 単純ベイズ分類器は特徴量同士の独立性を仮定しているため解釈可能なモデルであり、モジュールレベルで解釈が可能です。 条件付き確率を用いているため、各特徴量がクラスの分類にどれぐらい寄与しているかが非常に明確です。 4.7.2 k近傍法 k近傍法はデータ点の近傍を推論に使用する回帰や分類の手法です。 分類の場合、k近傍法はインスタンスの近傍の中で最も多くのものが属するクラスに割り当て、回帰では近傍の出力の平均をとります。 正しい k の値を見つけたり、近傍を定義するために使用される距離の算出方法を決定するには、工夫が必要です。 k近傍法は観測データに基づく学習アルゴリズムであるため、この本で紹介されている他の解釈可能なモデルとは異なるモデルと言えます。 k近傍法はどのようにすれば解釈できるのでしょうか。 まず、k近傍法には学習すべきパラメータが存在しないため、モジュールレベルでの解釈はできません。 さらに、k近傍法は局所的なモデルであり、明確に学習すべき大域的なパラメータや構造が存在しないため、大域的なモデルの解釈は困難です。 それでは、局所的には解釈が可能でしょうか。 推論について説明するには、使用された k 個の近傍を見つける必要があります。 モデルが解釈可能かどうかは、単純に、データセットの中の単一のインスタンスを解釈できるかどうかによります。 インスタンスが数百、数千の特徴量を持つ場合、それは解釈できないでしょう。しかし、少数の特徴量しかない、あるいはインスタンスの特徴量をいくつかの重要なもののみに削減できるのであれば、k近傍法は良い説明を与えることができます。 "],["agnostic.html", "Chapter 5 モデル非依存(Model-Agnostic)な手法", " Chapter 5 モデル非依存(Model-Agnostic)な手法 機械学習モデルから説明性を分離すること（=モデル非依存な解釈手法）には、いくつかの利点があります (Ribeiro, Singh, and Guestrin 201626)。 モデル固有の解釈手法と比べて、モデルに非依存な解釈手法の大きな利点は柔軟性があることです。 解釈手法がどのようなモデルにも適用できるならば、機械学習の開発者は好きな機械学習モデルに対して思いのまま使うことができます。 また、可視化の結果やユーザーインタフェースのような機械学習モデルの解釈を基に構築されるものは、根底にある機械学習モデルから独立したものになります。 通常、ある課題を解決するために機械学習モデルは1種類ではなく、多くの種類のモデルを評価します。 そして解釈性という観点でモデルを比較する際には、どのような種類のモデルに対しても同じ手法を用いることができるため、モデル非依存な手法を用いると簡単になります。 モデル非依存な解釈手法の代わりに解釈性のあるモデルのみを使用する方法がありますが、その場合は他の機械学習モデルと比較して予測性能が低い傾向があるという大きな欠点があり、使用するモデルが1種類に限定されることになります。 他の代替案は、モデル固有の解釈方法をつかうことです。 これの欠点は、やはり1種類のモデルに制限されてしまうことと、後から他のモデルに切り替えることが難しくなることです。 モデル非依存な解釈手法には、次のような性質が望まれます (Ribeiro, Singh, and Guestrin 2016)。 モデルの柔軟性 (Model flexibility) モデルの解釈手法がランダムフォレストやディープニューラルネットワークといったあらゆる機械学習モデルに対して使用できること。 説明の柔軟性 (Explanation flexibility) モデルの説明が特定の形式に制限されることがないこと。線形の関係を持つことが役に立つかもしれませんし、特徴量の重要度を可視化することが有用な場合もあるでしょう。 表現の柔軟性 (Representation flexibility) 説明のシステムは、説明対象のモデルとは異なる特徴量を使用できるべきです。抽象的な単語埋め込みベクトルを使用したテキスト分類に対しては、個々の単語を用いて説明することが好ましいかもしれません。 全体像 モデル非依存な解釈性を高い視点で見てみましょう。 私たちはデータを集めることで現実世界を捉え、（課題のために）機械学習モデルを用いてデータを予測するために学習し、さらに抽象化します。 解釈性は、人間の理解の助けとなる頂上のもう1つの層です。 FIGURE 5.1: 説明可能な機械学習の全体像。説明が人間に届く前に、現実世界はいくつかの層を通過します。 最も下に位置する層は現実世界です。 これは文字通り人体の生物学や薬に対する反応といった自然そのものを指すこともありますが、不動産市場といったより抽象的なことを指すこともあります。 現実世界の層には観察対象や興味の対象となりうるものすべてが含まれます。 最終的に私たちは現実世界に関する何かを学び、そして世界と対話することを目標としています。 2番目に位置する層はデータの層です。 コンピュータで情報を処理し保存するためには、現実世界はデジタル情報へと変換される必要があります。 データ層には画像やテキスト、表データなどが含まれています。 機械学習モデルをデータ層から学習させることで、ブラックボックスモデルの層が生まれます。 機械学習アルゴリズムは現実世界のデータから学習し、予測を出力したり何らかの構造を見つけ出したりします。 ブラックボックスモデルの層の上には解釈手法の層があり、機械学習モデルの不透明性に対処する助けとなります。 ある診断結果において最も重要な特徴量は何でしょうか。 なぜ金融取引が詐欺と分類されたのでしょうか。 最後の層は人間によって構成されています。 見てください！ここに描かれた人物はあなたに手を振っています！ あなたがこの本を読むことがブラックボックスモデルに対してより良い説明を与える手助けとなっているからです！ この多層から成る抽象化は統計学者と機械学習を実践している人のアプローチの違いを理解することにも役立ちます。 統計学者は臨床試験の計画や調査の設計などデータ層を扱っています。 彼らはブラックボックスモデルの層を飛ばし、直接解釈手法の層へと進みます。 機械学習のスペシャリストも同様にラベリングされた皮膚がんの画像を収集したり、ウィキペディアから情報を集めるなど、データ層を扱っています。 その後、彼らはブラックボックスな機械学習モデルを学習させます。 解釈手法の層は飛ばされ、人間が直接ブラックボックスモデルの予測と向き合います。 機械学習モデルを解釈可能とすることは統計学者と機械学習のスペシャリストの成果を融合させる素晴らしいことなのです。 もちろんこの図が全てを捉えているわけではありません。 データはシミュレーションから得られることもあります。 またブラックボックスモデルの出力する予測が他の機械へと与えられるのみで人間に届かないこともあります。 しかし全体として見るならば、この抽象化は解釈可能性が機械学習モデルの上に位置する新しい層となることを上手く表現しています。 Ribeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. &quot;Model-agnostic interpretability of machine learning.&quot; ICML Workshop on Human Interpretability in Machine Learning. (2016).↩ "],["pdp.html", "5.1 Partial Dependence Plot (PDP)", " 5.1 Partial Dependence Plot (PDP) Partial dependence plot (PDP, PD plot) は1つ、または2つの特徴量が機械学習モデルの予測結果に与える周辺効果 (marginal effect) を示します(J. H. Friedman 200127)。 Partial dependence plot は入力と出力の関係が線形か、単調か、より複雑かどうかを表現できます。 例えば、線形回帰モデルに適用した場合、partial dependence plot は常に線形の関係を示します。 回帰に対する Partial dependence 関数は以下のように定義されます。 \\[\\hat{f}_{x_S}(x_S)=E_{x_C}\\left[\\hat{f}(x_S,x_C)\\right]=\\int\\hat{f}(x_S,x_C)d\\mathbb{P}(x_C)\\] \\(x_S\\) は partial dependence 関数をプロットするべき特徴量で、\\(x_C\\) は機械学習モデル \\(\\hat{f}\\) のそのほかの特徴量を表します。 通常、集合 S の中には、1つか2つの特徴量が含まれます。 S の中の特徴量が、予測に与える効果を知りたい対象となります。 特徴ベクトル \\(x_S\\) と \\(x_C\\) を組み合わせて、特徴空間 x を構成します。 Partial dependence は、集合 C の中の特徴量の分布に対して機械学習モデルの出力を周辺化することで機能します。これによって、この関数は集合 S の中の関心のある特徴量と予測結果との関係を示すことができます。 他の特徴量に対して周辺化することによって、S の中の特徴量にのみ依存する関数を得ることができ、他の特徴量との相互作用も含まれます。 Partial function \\(\\hat{f}_{x_S}\\) は学習データの平均として計算されます。これはモンテカルロ法としても知られています。 \\[\\hat{f}_{x_S}(x_S)=\\frac{1}{n}\\sum_{i=1}^n\\hat{f}(x_S,x^{(i)}_{C})\\] Partial function は 特徴量 S の与えられた値に対して、予測に対する平均的な周辺効果 (average marginal effect) が何であるかを示しています。 この式中の、\\(x^{(i)}_{C}\\) は関心のない特徴量に対するデータセットからの実際の値であり、n はデータセットに含まれるインスタンスの数を表しています。 PDP の仮定は、C の中の特徴量は、S の中の特徴量と相関していないということです。もし、この仮定が成り立たなければ、PDP に対して計算された平均は、とても起こりそうにない、もしくは不可能なデータ点が含まれてしまいます。（短所を参照） 機械学習モデルが確率を出力する分類の場合、partial dependence plot は、S の特徴量に異なる値が与えられた特定のクラスの確率を表示します。 複数のクラスを扱うときの簡単な方法は、クラスごとに1本の線またはプロットを描くことです。 PDP はグローバルな方法です。 この方法は全てのインスタンスを考慮し、特徴量と予測結果のグローバルな関係についてのステートメントを提供します。 カテゴリカル特徴量 これまでは、数値の特徴量のみを想定していました。 カテゴリカル特徴量に対しては、 partial dependence はとても簡単に計算できます。 カテゴリのそれぞれに対して、全てのインスタンスを強制的に同じカテゴリとすることで、 PDP を計算できます。 例えば、自転車レンタルのデータセットに関して、季節に関する partial dependence plot に興味があるとすると、各季節に 1 つずつで、合計 4 つの数値が得られます。 “夏”の値を計算するためには、全てのデータの季節を”夏”に置き換えて、予測の平均を求めます。 5.1.1 例 実用上は、特徴量の集合 S は通常ただ1つ、もしくは多くても2つの特徴量のみを持たせます。なぜなら、1つの特徴量のとき2次元のプロットになり、2つの特徴量の場合は3次元のプロットになるからです。 2次元の紙やモニター上に3次元を描くことがすでに挑戦的なので、これ以上は、非常に扱いにくくなります。 ある日に借りられる自転車の数を予測する回帰の例に戻りましょう。 今回は、自転車の数を予測するためにランダムフォレストを用いて学習し、モデルが学習した関係性を可視化するために partial dependence plot を利用しました。 FIGURE 5.2: 自転車レンタル予測モデルの気温、湿度、風速に対するPDP。 気温で最も違いが見られ、暑くなればなるほど、自転車はレンタルされる。この傾向は20度まで上昇し、平坦になり、30度で少し減少する。x軸上のマークはデータの分布を示している。 暖かいが暑すぎない場合、モデルは平均して、多くの自転車がレンタルされると予測します。 湿度が 60% を超えると、自転車のレンタルは抑制されます。 それに加えて、風が吹けば吹くほど自転車に乗りたがる人は少なくなっていますが、これは理にかなっています。 興味深いことに、風速が 25km/h から 35km/h へ増加する間は、自転車の利用予測数は下降していません。しかし、これは十分な学習データが無いために、この範囲において機械学習モデルが意味のある予測を学習できなかったためかもしれません。 少なくとも直感的には、特に風速が非常に高い場合は、自転車の数が減ると考えられます。 カテゴリカル特徴量の partial dependence plot を例示するために、自転車レンタルにおける季節の特徴量の効果を調べます。 FIGURE 5.3: 季節に関する自転車レンタル予測モデルの PDP。 予想外にも全ての季節で同様の効果があることがわかった。ただし、春は自転車レンタル数のモデルの予測結果が小さかった。 Partial dependence を子宮頸がん分類についても計算してみます。 リスク要因に基づき女性が子宮頸がんにかかるか否かを予測するためにランダムフォレストを用いて学習しました。 ランダムフォレストでのがんにかかる確率と様々な特徴量との関係について、partial dependenceを計算し可視化します。 FIGURE 5.4: 年齢とホルモン避妊薬の使用年数に基づいたがんの確率のPDP。年齢に対して、40歳まで確率が低く、それ以降は確率が増加することをPDPは示している。ホルモン避妊薬の使用年数が増加すればするほど、特に、10年を境に、予測されたがんのリスクも高くなる。どちらの特徴量も大きな値の付近では十分な数のデータ点を使用することができなかったので、この付近における推定結果の信頼性は低いことに注意。 2つの特徴量について、一度に partial dependence を可視化できます。 FIGURE 5.5: 年齢と妊娠回数の相互作用とがんの確率のPDP。図は45のときにがんの確率が増加することを示している。25歳以下で妊娠回数が1または2回のときは、0回もしくは2回より多い場合と比較して予測されたがんのリスクは低かった。ただし、これは因果ではなく、単なる相関関係の可能性があるので、結論を出すときは注意。 5.1.2 長所 Partial dependence plot の計算は直感的です。 ある特徴量の値での partial dependence 関数は、全てのデータ点が特定の特徴量の値を持つと仮定した場合の予測の平均を表しています。 私の経験上、専門家ではない人たちも PDP のアイデアをすぐに理解できます。 もし、PDP を計算した特徴量が他の特徴量と相関していなかったのなら、PDP は完璧に、特徴量が平均的に予測にどのような影響を与えているかを表しています。 相関関係がない場合、説明は明快です。 Partial dependence plot は、j 番目の特徴量が変わった時に、あなたのデータセット内の予測値の平均がどう変化するのかを示します。 ただし、特徴量が相関している時、もっと複雑になります。詳しくは、短所の方をみてください。 Partial dependence plot は、実装が簡単です。 Partial dependence plot の計算には、因果関係の解釈があります。 特徴量に介入を行い、予測の変化を計算しています。 これは、特徴量と予測結果の因果関係を分析していることになります。28 出力を特徴量の関数として明示的にモデル化していることから、この関係性はモデルに関する因果関係であるが、必ずしも現実世界の因果関係ではないということに注意してください。 5.1.3 短所 Partial dependence 関数で確かめることができる現実的な最大特徴量の数 は2です。 これは PDP の問題ではなく、2次元の表現(紙やスクリーン)と我々が3次元以上をうまく想像できないことが原因です。 PDP の中には 特徴量の分布 を示さないものもあります。 分布を省くのは誤解を招くおそれがあります、なぜならほとんどデータがない部分を深読みしすぎてしまう可能性があるからです。 この問題は、ラグ（x軸上のデータ点を示す）またはヒストグラムを表示することで簡単に解決できます。 独立性の仮定が PDP の最大の問題です。 Partial dependence が計算される特徴量が他の特徴量と相関していないと仮定します。 例えば、体重と身長が与えられ、ある人が歩く速さを予測したいとしましょう。 その中の1つの特徴量 (身長) の partial dependence を調べるために、もう1つの特徴量(体重)が身長と相関がないと仮定しますが、それは明らかに間違った仮定です。 特定の身長 (例: 200cm) の PDP の計算のために、体重の周辺分布の平均を計算しますが、このとき 50kg 以下のデータも含まれます。これは2メートルの人にとっては現実的ではありません。 言い換えると、特徴量同士が相関していると、現実的にとても低い確率でしか起こらないような新しいデータ点をつくってしまうことになります (例えば、2メートルの身長で 50kg 以下である可能性は低いです)。 この問題に対する1つの解決策は Accumulated Local Effect plots や short ALE plots で、これらは周辺分布の代わりに条件付き分布を使用します。 PDP は平均的な周辺効果のみを示すので 不均一な影響が隠れてしまう可能性があります。 半分のデータ点が予測と正の相関 -- 特徴量が大くなるほど予測結果も大きくなる -- 、もう半分のデータ点が負の相関 -- 特徴量が小さくなるほど予測結果は大きくなる -- を持つ特徴量を考えてみましょう。 このとき、PDP の曲線は水平になるでしょう、なぜなら両方のデータセットが互いに影響を打ち消し合うからです。 そうすると、その特徴量は予測には影響を与えないと結論づけてしまうでしょう。 これに対する解決策として、集計された線の代わりに、individual conditional expectation curves をプロットすることで、不均一な影響も明らかにできます。 5.1.4 ソフトウェアと代替手法 PDP を実装した R のパッケージはたくさんあります。 例えば著者は iml パッケージを使っていますが、pdp や DALEX もあります。 Python では、partial dependence plots は scikit-learn に標準で実装されていますし PDPBox も使えます。 この本で紹介されている PDP の代替手法には ALE plots や ICE curves があります。 Friedman, Jerome H. &quot;Greedy function approximation: A gradient boosting machine.&quot; Annals of statistics (2001): 1189-1232.↩ Zhao, Qingyuan, and Trevor Hastie. &quot;Causal interpretations of black-box models.&quot; Journal of Business &amp; Economic Statistics, to appear. (2017).↩ "],["ice.html", "5.2 Individual Conditional Expectation (ICE)", " 5.2 Individual Conditional Expectation (ICE) Individual Conditional Expectation (ICE) plots は、ある特徴量が変化したときにそのインスタンスの予測がどのように変化するかを1本の線で可視化する手法です。 特徴量の平均的な効果に関する partial dependence plot は、特定のインスタンスではなく、全体的な平均に注目しているため、大域的な方法と言えます。 個々のインスタンスに対する PDP と等価な手法は、individual conditional expectation (ICE) plot (Goldstein et al. 201729) と呼ばれています。 ICE plot はインスタンスごとの、ある特徴量が予測に与える影響を別々に可視化します。 partial dependence plotでは、全体に対して1本の線で表現していましたが、ICE plot では、1つのインスタンスにつき1本の線で表現されます。 PDP は ICE plot の線を平均したものと一致します。 ある線(とそれに対応するインスタンス)における値は、他の全ての特徴量を一定に保ったまま、ある特徴量の値をグリッド上の 別の値に置き換えて、いくつかの新しいインスタンスを作成し、それらに対してブラックボックスモデルで予測をすることで計算されます。 その結果は、グリッド上の特徴量の値と、それぞれの予測値を持つインスタンスに対する点の集合です。 PDP の代わりに個々の予測を見るポイントは何でしょうか。 PDP は相互作用によって生まれる不均一な関係を見えなくしてしまうことがあります。 PDP は特徴量と予測が平均的にどんな関係にあるかを示していますが、これは、対象の特徴量と他の特徴量との相互作用が弱い場合にのみ有効です。 相互作用がある場合、ICE plot はより多くの洞察を与えてくれるでしょう。 より正式な定義は次のとおりです。 ICE plot では、\\(\\{(x_{S}^{(i)},x_{C}^{(i)})\\}_{i=1}^N\\) 内のそれぞれのインスタンスにおいて、曲線 \\(\\hat{f}_S^{(i)}\\) は \\(x^{(i)}_{S}\\) に対して、\\(x^{(i)}_{C}\\) が固定されたままプロットされます。 5.2.1 例 子宮頸がんのデータセット を使って、それぞれのインスタンスで「年齢」の特徴量がどれだけ影響を与えているかを見てみましょう. リスクのある要因が与えられたとき女性ががんになる確率を予測するランダムフォレストを分析してみます。 Partial dependence plot では、50歳周辺でがんの確率が増加しているのが見受けられますが、データセット内のすべての女性に対して当てはまるのでしょうか。 ICE plot は、ほとんどの女性にとって、年齢的な影響は50歳で確率が増加するという平均的なパターンに従っているが、例外もあるということを明らかにしています。 若いときから高確率を予測されている女性は、予測されるがん確率は年齢によってあまり変わりません。 FIGURE 5.6: 年齢ごとの子宮頸がんの確率のICEプロット。それぞれの線は一人の女性を表す。ほとんどの女性は、年齢の増加に伴って、がんと予測される確率が増加する。予測の確率が 0.4 を超える女性に対しては、年齢が高くなっても予測はあまり変化しない。 次の図は、自転車レンタル予測 に対する ICE プロットです。 ここでも使用されている予測モデルはランダムフォレストです。 FIGURE 5.7: 天候ごとの自転車レンタル予測の ICE プロット。PDP のときと同様の効果が見られる。 全ての曲線は同じコースを辿っているように見えるので、明らかな相互作用はないと言えます。 つまり、PDP は表示された特徴量と予測された自転車の数との関係の優れた要約となっていると言えます。 5.2.1.1 Centered ICE Plot ICE プロットには問題があります。 ICE 曲線は異なる予測から始まるため、個々の間で ICE 曲線が異なるかどうかを判断するのが難しい場合があります。 簡単な解決策は、特徴量の特定の点で曲線を中央に配置し、この点との予測の差のみを表示することです。 結果のプロットは、centered ICEプロット（c-ICE）と呼ばれています。 特徴量の下端にカーブを固定することをお勧めします。 新しい曲線は次のように定義されます。 \\[\\hat{f}_{cent}^{(i)}=\\hat{f}^{(i)}-\\mathbf{1}\\hat{f}(x^{a},x^{(i)}_{C})\\] ただし、\\(\\mathbf{1}\\) は適切な数(普通、1 か 2)だけ 1 が並んだベクトルであり、\\(\\hat{f}\\) は学習されたモデルで、xa はアンカーポイントです。 5.2.1.2 例 例えば、年齢に対して子宮頸がんの ICE プロットを作成し、観測された最も若い年齢を中心に線を引いてみましょう。 FIGURE 5.8: 年齢ごとに予測されたがんの確率に対する centered ICE プロット。線は年齢 14 が 0 に固定されている。年齢 14 に比べ、ほとんどの女性の予測は、予測確率が増加する45歳まで変化しない。 Centered ICE プロットでは、個々のインスタンスの曲線の比較を簡単にできます。 これは、予測値の絶対的な変化ではなく、特徴量の範囲の固定点と比較した予測の差を確認したい場合に役立ちます。 自転車レンタル数予測の例で、centered ICE プロットをみてみましょう。 FIGURE 5.9: 天候による予測された自転車レンタル数の centered ICE プロット。 線は、観測された特徴量の最小値での予測との差を示している。 5.2.1.3 Derivative ICE Plot 不均一性を簡単に視覚化するための別の方法は、特徴量に関して、予測関数の個々の微分を見ることです。 結果のプロットは derivative ICE plot (d-ICE)と呼ばれています。 関数の微分(または、曲線)は、変化が起きたのか、また、どの方向に起きたのかを教えてくれます。 Derivative ICE plot を用いると、（少なくとも一部の）インスタンスでブラックボックスの予測が変化する特徴値の範囲を簡単に見つけることができます。 もし、注目している特徴量 \\(x_S\\) と他の特徴量 \\(x_C\\) の間に相互作用がないのであれば、予測関数は以下のように表現できます。 \\[\\hat{f}(x)=\\hat{f}(x_S,x_C)=g(x_S)+h(x_C),\\quad\\text{with}\\quad\\frac{\\delta\\hat{f}(x)}{\\delta{}x_S}=g&#39;(x_S)\\] 相互作用がないとき、個々の偏微分は全てのインスタンスで同じである必要があります。もし、これらが異なる場合は相互作用が原因であり、d-ICE plot を用いて可視化できます。微分の標準偏差を示すことは、推定された微分に不均一性がある S の特徴量の領域を強調するのに役立ちます。 ただし、derivative ICE plot は計算に長い時間がかかるため、現実的ではないかもしれません。 5.2.2 長所 ICE 曲線は partial dependence plot よりも直感的に理解可能です。 1つの線は、1つのインスタンスに対して、対象の特徴量を変化させたときの予測を表します。 Partial dependence plot とは異なり、ICE 曲線は不均一な関係性を明らかにできます。 5.2.3 短所 ICE 曲線は1つの特徴量のみを意味のある形で表示できます。2つの特徴量を使うと、いくつかの重複した面を描画する必要があるため、このプロットをみても何も理解できないでしょう。 ICE 曲線は、PDP と同様の問題に直面します。 興味のある特徴量が、その他の特徴量と相関している場合、同時分布によって、線の中のいくつかの点は妥当でないデータ点となる可能性があるということです。 多くの ICE 曲線が描かれたとき、プロットは激しく重なり合い、何も発見できません。 解決方法は、線に透明度を追加するか、線のうちのいくつかのみを描画することです。 ICE 曲線の中で、平均をみることは簡単ではないかもしれません。 これに対する単純な解決方法は、ICE 曲線と PDP を組み合わせることです。 5.2.4 ソフトウェアと代替手法 ICE plots は、iml（これらの例で使用）、ICEbox [^ ICEbox]、および pdp の R パッケージで実装されています。 ICE にとても類似しているもう1つのRパッケージは condvisです。 Goldstein, Alex, et al. &quot;Peeking inside the black box: Visualizing statistical learning with plots of individual conditional expectation.&quot; Journal of Computational and Graphical Statistics 24.1 (2015): 44-65.↩ "],["ale.html", "5.3 Accumulated Local Effects (ALE) Plot", " 5.3 Accumulated Local Effects (ALE) Plot Accumulated local effects30 は、特徴量が機械学習モデルの予測に対して、平均的にどの程度影響を与えているか示します。 ALE plot は、partial dependence plot (PDP) と比べて高速で偏りがありません。 PDP と ALE は目的が同じ手法なので、理解のしやすさのために、partial dependence plots の章 を先に読むことをおすすめします。 どちらも、ある特徴量が予測に対して平均的にどの程度の影響を与えるかを説明します。 次のセクションでは、特徴量が相関しているときに、partial dependence plot では深刻な問題があることを紹介します。 5.3.1 モチベーションと直感 もし機械学習モデルの特徴量が相関しているとき、partial dependence plot は信用できません。 他の特徴量と強く相関する特徴量に対する partial dependence plot の計算では、現実的に起こり得ない人工的なインスタンスの予測結果が含まれます。 これが、特徴量の効果を推定するときの大きなバイアスになります。 部屋の数とリビングのサイズによって家の値段を予測する機械学習モデルで partial dependence plot を計算することを考えてみてください。 今、リビングが、予測した値段に与える影響に関心があるとします。 Partial dependence plot の手順は 1) 特徴量を選ぶ 2) グリッドを決める 3) それぞれのグリッドの値に対して、 a) 特徴量をその値に変換し b) 予測を平均化する 4) 曲線を描く でした。 PDP の始めのグリッドの値(例えば 30m^2 )を計算するために、たとえ10部屋ある家でも「全ての」インスタンスに対して、30m^2 と置き換えます。 これは不自然な家です。 Partial dependence plot では、このような不自然な家があったとしても、それは問題がないとして特徴量の効果を推定します。 次の図は、2つの相関した特徴量と、partial dependence plot で現実的でないインスタンスを予測した平均の結果を示しています。 FIGURE 5.10: 強く相関している特徴量 x1 と x2。 x1 が 0.75 の特徴量の影響を計算するために、PDP は全てのインスタンスの x1 を 0.75 に置き換え、x1=0.75 のときの x2 の分布が、全体の x2(縦軸) の分布と同じであると誤った仮定をしている。 これによって不自然な x1 と x2 の組み合わせ(例えば x2=0.2 で x1=0.75)が生まれ、PDP ではこれも平均効果の計算に含まれる。 特徴量同士の相関を保ったまま特徴量の影響の推定を得るためにはどうしたら良いでしょうか。 特徴量の条件付き分布上で周辺化します。つまり、x1 のグリッドの値では、x1 と似たような値を持つインスタンスの予測のみを用いて平均化します。 条件付き分布を使用して、特徴量の影響を計算する方法を Marginal Plot や M-Plot と呼びます(marginal distribution ではなく conditional distribution に基づいているのでややこしい名前です)。 待ってください、ALE plot について説明すると言っていたのではありませんでしたか。 実は、M-Plot は我々が求めている解決策ではありません。 なぜ M-Plot では問題を解決できないのでしょうか。 約 30 m2 の全ての家の予測を平均化してしまうと、相関が原因でリビングや部屋の数の「混合された」影響を推定してしてしまいます。 リビングが予測された家の価値に全く影響を与えないと仮定すると、部屋の数だけが価値に影響を与えます。 M-Plot はリビングのサイズが予測される価値を上げると示すでしょう、なぜなら部屋の数はリビングのサイズにしたがって増えていくからです。 次の図は2つの相関した特徴量に対して M-Plot がどう動くかを示しています。 FIGURE 5.11: 強く相関した特徴量 x1 と x2。 M-Plot は条件付き分布上で平均化する。ここで x1=0.75 のときの x2 の条件付き分布が示されている。局所的な予測を平均化することは両方の特徴量の影響を混ぜることにつながる。 M-Plot は現実的ではないインスタンスの予測を含めることを防ぎますが、ある特徴量の影響を他の相関する全ての特徴量の影響と混ぜてしまいます。 ALE plot では、 -- これも特徴量の条件付き分布に基づいている -- 予測の平均ではなく差分を計算することによってこの問題を解決しています。 30m2 のリビングの影響に対して、ALE はリビングが約 30m2 の家全てを使い、それらの家が31m2 だったとしたときの予測値からそれらの家が29m2 だったとしたときの予測値をひいたモデルの予測値を得ます。 これによって、リビングの広さの純粋な影響が得られ、かつ、他の相関した特徴量の影響とも混ざっていません。 差分を使うことにより他の特徴量の影響を受けないようにできます。 次の図から ALE plot がどのように計算されるか直感的に理解できるでしょう。 FIGURE 5.12: x2 と相関している特徴量 x1 の ALE の計算。はじめに x1 を区間(縦の線)に分ける。ある区間内のデータインスタンス(点)に対して、その特徴量をその区間の上限と下限の値に置き換えたときとの予測値の差分を計算する(横の線)。この差分は後に蓄積され、中心化して ALE 曲線となる。 それぞれの プロット (PDP, M, ALE) が、ある特定のグリッドの値 v での、特徴量の影響をどのように計算するかを以下にまとめます。 Partial Dependence Plots: 「それぞれのインスタンスの特徴量の値を v としたときの、モデルの予測の平均を示します。ただし、特徴量の値を v にしたときのインスタンスが現実的であるかどうかは無視します。」 M-Plots: 「その特徴量が v に近い値を持つインスタンスに対する、モデルの予測の平均を示します。影響はその特徴量に起因しますが、相関する特徴量にも起因してしまいます。」 ALE plots: 「その特徴量が約 v であるインスタンスに対して、小さな&quot;窓&quot;の中でモデルの予測がどう変化するかを示します。」 5.3.2 理論 数学的に PDP, M-plot, ALE plot はどのように異なっているのでしょうか。 これら全てに共通していることは、複雑な予測関数 f を、1つか2つの特徴量に依存する関数に削減します。 3つの方法は全て、他の特徴量の効果を均すことによって関数を削減しますが、予測の平均、または予測の差を計算するかどうか、および平均を周辺分布、もしくは条件付き分布に対して行うかが異なります。 PDP は、周辺分布に対して予測を平均化します。 \\[\\begin{align*}\\hat{f}_{x_S,PDP}(x_S)&amp;=E_{X_C}\\left[\\hat{f}(x_S,X_C)\\right]\\\\&amp;=\\int_{x_C}\\hat{f}(x_S,x_C)\\mathbb{P}(x_C)d{}x_C\\end{align*}\\] これは、\\(x_C\\) の全ての特徴量で平均化された、特徴量 \\(x_S\\) での予測関数 f の値です。 平均化は、集合 C の特徴量に対する周辺化期待値 (marginal expectation) E を計算することを意味しており、これは、確率分布によって重み付けされた予測の積分です。 難しく聞こえますよね、しかし、周辺分布の期待値を計算するためには、単純に、全てのインスタンスを取り出して、これらの、集合 S 内の特徴量を特定のグリッドの値に書き換えます。そして、人工的に作ったデータセットの予測の平均を求めるだけです。 この手順が、特徴量の周辺分布に対して平均化した事を保証しています。 M-plots は条件付き分布に対して予測を平均化します。 \\[\\begin{align*}\\hat{f}_{x_S,M}(x_S)&amp;=E_{X_C|X_S}\\left[\\hat{f}(X_S,X_C)|X_S=x_s\\right]\\\\&amp;=\\int_{x_C}\\hat{f}(x_S,x_C)\\mathbb{P}(x_C|x_S)d{}x_C\\end{align*}\\] PDP と比較した時の唯一の違いは、興味のある特徴量の各グリッドの値で、周辺分布を仮定するのではなく、条件付き分布に対する予測の平均を計算することです。 実際には、これは近傍を定義しなけらばらない事を意味します。 例えば、 予測された家の価値に 30 m2 が与えた影響を計算したい場合、全ての 28 m2 から 32 m2 の間にある家の予測を平均化します。 ALE plot は、予測の変化を平均し、グリッド上で累積します（計算の詳細は後述）。 \\[\\begin{align*}\\hat{f}_{x_S,ALE}(x_S)=&amp;\\int_{z_{0,1}}^{x_S}E_{X_C|X_S}\\left[\\hat{f}^S(X_s,X_c)|X_S=z_S\\right]dz_S-\\text{constant}\\\\=&amp;\\int_{z_{0,1}}^{x_S}\\int_{x_C}\\hat{f}^S(z_s,x_c)\\mathbb{P}(x_C|z_S)d{}x_C{}dz_S-\\text{constant}\\end{align*}\\] この式は M-Plots との3つの違いを明らかにします。 1つ目は、予測値そのものではなく、予測値の変化を平均化することです。 変化は、勾配として定義されます（しかし、後ほど、実際の計算では、区間上の予測の差と置き換えられます)。 \\[\\hat{f}^S(x_s,x_c)=\\frac{\\delta\\hat{f}(x_S,x_C)}{\\delta{}x_S}\\] 2つ目の違いは、z に対するもう1つの積分です。 集合 S の特徴量の範囲にわたって局所勾配を累積することで、予測に対する特徴量の影響を知ることができます。 実際の計算では、 z は予測の変化を計算するために、区間のグリッドに置き換えられます。 直接、予測を平均化する代わりに、ALE では、特徴量 S で条件づけされた予測の差を計算し、効果を推定するために特徴量 S 上の勾配を積分します。 これは、馬鹿げた様に聞こえます。 微分と積分は、通常、互いに打ち消し合います。例えば、最初に引き算をして、同じ数字を足し算する様な物です。 なぜ、これが、ここでは意味をなすのでしょうか。 微分（または、区間の差）は対象の特徴量の効果を分離し、そして、相関した特徴量の影響を遮断するのです。 ALE plots と M-plotsの3つ目の違いは、結果から定数を引く事です。 このステップによって、データに対しての平均的な影響がゼロになるよう ALE plot が中心化されます。 問題が1つ残ります。 全てのモデルで勾配が使用できる訳ではありません。例えば、ランダムフォレストは勾配が計算できません。 しかし、実際の計算では、勾配を使うのではなく、区間を使用します。 もう少し、ALE plots の推定を深く見てみましょう。 5.3.3 予測 最初に、どの様に ALE plots が1つの量的特徴量に対して予測されるのかを説明します。次に、2つの量的特徴量に対して、そして、最後にカテゴリカル特徴量について説明します。 局所的な効果を予測するために、特徴量をいくつかの区間に分割し、そして、予測の差を計算します。 \\[\\hat{\\tilde{f}}_{j,ALE}(x)=\\sum_{k=1}^{k_j(x)}\\frac{1}{n_j(k)}\\sum_{i:x_{j}^{(i)}\\in{}N_j(k)}\\left[f(z_{k,j},x^{(i)}_{\\setminus{}j})-f(z_{k-1,j},x^{(i)}_{\\setminus{}j})\\right]\\] この方程式を右端から読み解いてみましょう。 Accumulated local effect という名前が、この方程式の個々の要素を、とてもよく表しています。 基本的に ALE method は予測の差を計算します。それにより、対象の特徴量をグリッド値 z と置き換えます。 予測の差は、ある区間の中の個々のインスタンスに対して、特徴量が持つ効果となります。 右側の合計は、区間内の全てのインスタンスの効果を足し合わせたものであり、式の中では、近傍 \\(N_j(k)\\) として書かれています。 この区間内の予測の差の平均値を計算するために、合計値を区間内のインスタンス数で割ります。 区間内の平均値は、ALE という名前の Local という用語で表現されます。 左の総和の記号は、全ての区間において、平均効果を累積する事を意味します。 例えば、（中心化されていない）3番目のALEの特徴量は、1番目、2番目そして3番目の区間の効果の合計です。 ALE の Accumulated という用語はこれを表しています。 この効果は、平均がゼロとなるように中心化されます。 \\[\\hat{f}_{j,ALE}(x)=\\hat{\\tilde{f}}_{j,ALE}(x)-\\frac{1}{n}\\sum_{i=1}^{n}\\hat{\\tilde{f}}_{j,ALE}(x^{(i)}_{j})\\] ALEの値は、データの特定の値における予測の平均と比較したときの、特徴量の主な影響として解釈する事が出来ます。 例えば、\\(x_j=3\\) において、ALEの推定が -2 であるとは、j番目の特徴量が 3 のとき、予測は予測の平均よりも 2 下がるということを意味します。 特徴量の分布の分位数は、区間を決めるグリッドとして使用されます。 分位数を使用することで、各区間内に同じ数のインスタンスが存在することを保証できます。 ただし、分位数は、区間の長さがばらつく可能性があるという欠点もあります。 例えば、低い値が多く、高い値が少数のみであるような、対象の特徴量が特に偏っている場合、不自然な ALE plot になる可能性があります。 2つの特徴量の相互作用に対するALEプロット ALEプロットは2つの特徴量の相互作用の影響も表すことができます。 計算方法は単一の特徴量に対するのものと同じですが、2次元における影響を考慮しなければならないため、インターバルの代わりに長方形セル(2次元の区間)を使用します。 また、全体の平均の効果を調整することに加えて、それぞれの特徴量の効果の調整も行います。 これは2つの特徴量に対するALEは、それぞれの特徴量の効果を含まない2次の効果を推定するということです。 言い換えると、2つの特徴量に対するALEは、2つの特徴量の追加の相互作用のみを表すということです。 2次元のALEプロットの式は長くて読みづらいため、掲載を控えます。 計算式に興味がある人は、原論文の式 (13) -- (16) を参照ください。 ここでは、2次のALEの計算は、直感を養うための視覚的な説明にとどめます。 FIGURE 5.13: 2次元ALEの計算。2つの特徴量に渡ってグリッド作成。それぞれのセルにおいて、グリッド内の全てのインスタンスに対して2次の差分を計算する。初めに x1 と x2 の値をセルの角の値で置き換える。a, b, c および d が(図中でラベル付けされているように)人工的なインスタンスの&quot;角&quot;での予測を表すとき、2次の差分は (d - c) - (b - a)となる。各セルにおける2次の差分の平均は、グリッド上のそれぞれのセルで累積されて中心化される。 前の図では、多くのセルが相関によって空となっていました。 ALEプロットでは、これがグレーまたは暗い色のボックスで可視化されます。 その代わりに、空のセルのALEの推定結果を、最も近い空でないセルの ALE の推定結果で置き換えることもできます。 2つの特徴量に対するALEの推定では、2つの特徴量の2次の効果のみを示しているため、解釈には特に注意が必要です。 2次の効果は、特徴量の主な効果を考慮した後の、追加の相互作用による影響です。 2つの特徴量は相互作用しないが、それぞれが予測結果に対して線形の影響を持つと仮定してください。 それぞれの特徴量に対する1次元のALEプロットでは、推定されたALE曲線は直線となるでしょう。 しかし2次元のALEの推定結果をプロットすると、2次の効果は相互効果による追加の影響のみを表すため、それらはゼロに近くなるはずです。 このような場合、ALEプロットとPDPは異なります。 PDPはいつも総合的な影響を可視化しますが、ALEプロットは1次または2次の影響のみ見せます。 それらは根底にある数学には依らない、設計における決まり事です。 純粋な主効果や2次の影響はPDPから低次の効果を差し引くことで計算可能であり、総合的な効果はALEプロットで低次の影響を引くのをやめることで得ることができます。 ALEは、任意の高次の計算 (3つ以上の相互作用) も可能ですが、PDPの章 でも触れたとおり、計算結果自体は意味の解釈が可能であっても可視化ができないので、2つの特徴量までにするべきです。 カテゴリカル特徴量に対するALE ALEプロットは、-- 定義によると -- ある方向の中での影響を累積する手法であるため、特徴量の値の順序関係を要求します。 カテゴリカル特徴量は、自然な順序を持ちません。 カテゴリカル特徴量に対してALEプロットを計算するためには、何らかの方法で順序関係を作り出す、もしくは見出す必要があります。 カテゴリの順序は、ALEの解釈や計算方法に影響を与えます。 1つの解決法は、カテゴリを他の特徴量に対する類似度に従って順序づけることです。 2つのカテゴリ間の距離を、それぞれの特徴の距離の総和とします。 特徴量ごとの距離は、（量的特徴量における）Kolmogorov-Smirnov 距離 や (カテゴリカル特徴量における) 相対頻度表の両方のカテゴリの累積分布を比較します。 一度すべてのカテゴリ間の距離を計算してしまえば、multi-dimensional scaling を使って距離行列を1次元の距離尺度に削減できます。 これにより、カテゴリの類似度を基にした順序を得ることができます。 例を使用して、もう少し詳しくみてみましょう。 2つのカテゴリカル特徴量 &quot;season&quot; 、 &quot;weather&quot;、と 1つの量的特徴量 &quot;temperature&quot; を持つ場合を想定します。 最初のカテゴリカル特徴量 (season) に対して、ALEプロットを計算したいとします。 特徴量はカテゴリ、&quot;spring&quot;, &quot;summer&quot;, &quot;fall&quot;, &quot;winter&quot; を持ちます。 &quot;春&quot; と &quot;夏&quot; のカテゴリ間の距離の計算を始めます。 距離は、気温と天気の特徴量の距離の総和です。 気温に関して、季節が &quot;spring&quot; の全てのインスタンスを取得し、累積分布関数を計算します。 &quot;summer&quot; を持つインスタンスに対しても同様に計算します。そして、それらの距離を Kolmogorov-Smirnov 統計により算出します。 天気に関しても、全ての &quot;spring&quot; のインスタンスに対して各天気ごとの確率を計算し、&quot;summer&quot; のインスタンスに対しても同様の計算を行ます。 その後、確率分布の絶対距離の総和を求めます。 もし、&quot;spring&quot; と &quot;summer&quot; が大きく異なる気温と天気を持つならば、合計のカテゴリの距離は大きくなります。 この処理を他の季節のペアでも繰り返し、得られた距離の行列を multi-dimensional scaling によって、1次元に削減します。 5.3.4 例 ALE プロットを実際に使用してみましょう。 PDP が失敗する状況を準備しました。 この状況では、予測モデルが2つの強く相関する特徴量から構成されています。 予測モデルはほとんど線形回帰モデルですが、インスタンスが観測されていない特徴量の組合わせの部分では奇妙なことになっています。 FIGURE 5.14: 2つの特徴量と予測結果。このモデルは2つの特徴量の和を予測している(影の背景)が、x1&gt;0.7, x2&lt;0.3 の範囲では常に2と予測するという例外があります。この領域はデータの分布(点群)とかけ離れており、モデルの性能には影響しないが、解釈にも影響を与えるべきではない。 これは現実的な状況でしょうか。 モデルを学習するとき、学習アルゴリズムは学習データの存在するインスタンスに対して誤差を最小化します。 奇妙な部分が学習データの分布の外側では起こりえます、なぜならモデルはその領域の奇妙なデータに対してペナルティを与えていないからです。 データ分布から離れることは外挿と呼ばれ、敵対的 サンプルの章で説明されているように、機械学習モデルをだますことにも 使用されます。 この例で、partial dependence plots が ALE plots と比べてどのように振る舞うか見てみましょう。 FIGURE 5.15: PDP(上の段)とALE(下の段)で計算される特徴量の影響の比較。 PDP の推定はデータ分布の外でモデルの奇妙な挙動に影響を受けている (プロットが跳ね上がっている)。ALE plot は機械学習モデルは特徴量と予測の間で関係が線形になっていると正しく認識できており、データがない領域を無視している。 しかし、モデルが x1 &gt; 0.7, x2 &lt; 0.3 の領域での奇妙な振る舞いを知ることは、興味深いと言えないのでしょうか。 これは、どちらとも言えます。 なぜなら、これらのインスタンスは、現実的に起こり得ないのか、それともごく稀に起こりうるかであるので、一般的にはこのようなインスタンスを見るのは意味がありません。 テストデータの分布が多少異なり、いくつかのインスタンスがこの範囲に含まれるのであれば、この領域も特徴量の効果を計算する上で含めることをお勧めします。 ただし、このような、まだ観測がされていないデータの領域を含めるかどうかは意識的に決定できる必要があり、PDP のように手法の副作用となるべきではありません。 もし、モデルが異なる分布のデータで使われる可能性があるのであれば、ALE プロットを使用して可能性のあるデータ分布を用いてシミュレーションしてみることをおすすめします。 次は、現実のデータセットである、レンタル自転車数を天候と曜日に基づいて予測し、ALEプロットが本当に想定した通りに動いているか見てみましょう。 与えられた日のレンタル自転車数を予測するために回帰木を学習し、気温や相対湿度、風速が予測にどの程度影響を与えているか ALEプロットで分析してみます。 ALEプロットの結果を見てみましょう。 FIGURE 5.16: 気温、湿度、風速による予測モデルに対する ALEプロット。気温は予測に大きく影響を与えています。予測の平均は気温の上昇に従って増えていますが摂氏25度を超えると下がる。湿度は負の影響を与えていて、60%以上では、相対湿度が高くなればなるほど、予測結果は小さくなる。風速は予測にはあまり影響を与えていない。 気温、湿度、風速とその他全ての特徴量との相関を見てみましょう。 データはカテゴリカル特徴量を含んでいるため、量的特徴量間でのみ有効なピアソンの相関係数のみを使うことはできません。 代わりに、例えば気温に対して予測する線形モデルを他の特徴量を入力として学習させます。 そして、線形モデルの他の特徴量がどの程度分散を説明するか計算し、平方根を取ります。 もし、他の特徴量が量的特徴量であれば、結果はピアソンの相関係数の絶対値と一致します。 しかし、この &quot;分散による説明&quot; のモデルに基づくアプローチ (ANalysis Of VArianceを表してANOVAとも呼ばれます) は、カテゴリカル特徴量の場合でも有効です。 &quot;分散による説明&quot; の値は、0(無関係)と1(気温を他の特徴量から完全に予測できる) の間の値となります。 気温、湿度、風速と、その他の特徴量との間の因子寄与を計算します。 因子寄与(相関)が高くなればなるほど、PDP では大きな(潜在的な)問題となります。 次の図では、天候がその他の特徴量とどの程度強く相関しているかを可視化しています。 FIGURE 5.17: 季節を特徴量として気温を予測する線形モデルを学習したときの、気温、湿度、風速とその他全ての特徴量との相関の強さを因子寄与で計算。気温については、-- 当然ながら -- 季節や月との高い相関が見られる。湿度は天候状況と相関している。 この相関分析は 特に気温の特徴量に対して、partial dependence plots では、問題に直面するであろうことを明らかにしています。 FIGURE 5.18: 気温、湿度、風速に対する PDP。ALEプロットと比較すると、PDP は高気温または高湿度において予測自転車数が小さな減少を示している。PDPは、高気温の影響を計算するために、たとえそれが&quot;冬&quot;のインスタンスであっても全てのインスタンスを使う。ALEプロットの方が信頼性がある。 次に、カテゴリカル特徴量に対するでの ALE プロットについて見てみましょう。 月は、レンタル自転車数への影響を分析したいカテゴリカル特徴量です。 ほぼ間違いなく、月は一定の並び(1月から12月まで)を既に持っていますが、初めに類似度によってカテゴリを並べ替えて、それから影響を計算したらどうなるか見てみましょう。 月は、気温や休日であるかといった他の特徴量に基づき、各月の日々の類似度によって並べられます。 FIGURE 5.19: カテゴリカル特徴量である月についての ALE プロット。月はその他の特徴量における分布に基づく類似度によって並べられている。他の月と比較して、1月、3月、4月、特に12月と11月は、予測自転車数に低い影響を持つことが見て取れる。 多くの特徴量は天候に関連することから、月の並びには月同士でいかに天候が似ているかが強く反映されています。 全ての寒い月は左側に(2月から4月)、暖かい月は右側に(10月から8月)あります。 ただし、天候以外の特徴量も類似度計算に含まれることに注意してください。例えば、休日の相対頻度は各月間の類似度計算において気温と同じ重みを持っています。 次に、予測自転車数における湿度と気温の2次効果を考えます。 2次効果は2つの特徴量における追加的な相互作用の影響であり、主効果を含まないことを思い出してください。 これは、例えば、高い湿度になると自転車レンタル予測数が平均的に下がるという効果が2次の ALE プロットでは見られなくなるということです。 FIGURE 5.20: 予測自転車数における湿度と気温の2次効果に対する ALE プロット。主効果を考慮した上で、明るい影は平均より上、暗い影は平均より下の予測であることを示す。プロットは気温と湿度の相互作用を明らかにする。暑くて湿った天候は予測数を増加させ、寒くて湿った天候は予測数に対して負の影響を示す。 気温と湿度の両方の主効果は、とても暑くて湿気のある天候では、予測されたバイクの数が減少する事を覚えておいてください。 したがって、暑くて湿気の多い天候において、気温と湿度は主効果の総和ではなく、それより大きいのです。 純粋な二次効果 (私たちが先ほど見た the 2D ALE plot) と全ての効果の間の差を強調するために、PDP を見てみましょう。 PDP は予測の平均、2つの主効果、二次効果 (相互作用) の全ての効果を表しています。 FIGURE 5.21: 予測された自転車数に対する気温と湿度の全ての効果の PDP。プロットは、相互作用のみを示す 2DーALE plots とは対照的に、それぞれの特徴量の主効果と相互作用の影響が組み合わされている。 もし相互作用のみに関心があるなら、主効果が混ざった全ての効果をみるのではなく、2次効果のみを見るべきです。 しかし、特徴量の組み合わさった影響を知りたいのであれば、PDP が示す全ての効果を見るべきです。 例えば、摂氏30度で湿度80%のときの自転車の予測台数を知りたいなら、2D PDP から直接読み取ることができます。 同じものを ALE plots から読み取ろうとすると、3つの plots を見る必要があります。 気温のみ、湿度のみ、気温と湿度の ALE plot から全体の予測平均を知る必要があります。 2つの特徴量の相互作用がない場合、単純な2つの主効果の積であるにもかかわらず、全ての効果は複雑な見た目になる可能性があり、誤解を招く恐れがあります。ただし、二次効果のみを見ることで、ただちに、相互作用がないことが示せるでしょう。 自転車の例はこれぐらいにして、クラス分類のタスクに移りましょう。 リスクの要因から 子宮頸がん の可能性を予測するランダムフォレストを学習します。 特徴量のうちの2つの ALE を可視化しましょう。 {r ale-cervical-1D, fig.cap = &quot;子宮頸がんになる予測確率における年齢とホルモン避妊の年数の影響の ALE プロット。年齢の特徴量に関して、ALE plot は40歳までは平均してがんの確率が低く、それ以降は上昇することを示している。ホルモン避妊の年数が8年以降になると予測されるがんのリスクが高くなる傾向にある。&quot;} data(cervical) cervical.task = makeClassifTask(data = cervical, target = &quot;Biopsy&quot;) mod = mlr::train(mlr::makeLearner(cl = &#39;classif.randomForest&#39;, id = &#39;cervical-rf&#39;, predict.type = &#39;prob&#39;), cervical.task) pred.cervical = Predictor$new(mod, data = cervical, class = &quot;Cancer&quot;) ale1 = FeatureEffect$new(pred.cervical, &quot;Age&quot;, method = &quot;ale&quot;)$plot() ale2 = FeatureEffect$new(pred.cervical, &quot;Hormonal.Contraceptives..years.&quot;, method = &quot;ale&quot;)$plot() + scale_x_continuous(&quot;Years with hormonal contraceptives&quot;) + scale_y_continuous(&quot;&quot;) gridExtra::grid.arrange(ale1, ale2, ncol = 2) 次に、妊娠回数と年齢の相互作用を見ます。 FIGURE 5.22: 妊娠回数と年齢の2次効果に対するALEプロット。プロットの解釈は要領を得ず、過学習が発生している様子。例えば、18-20歳で3回以上の妊娠回数の場合に、プロットは妙なモデルの振る舞いを見せている(癌の確率が5%までの上昇をしている)。この区間の年齢と妊娠回数(点として表示しているデータ)の女性はデータ中で多くないため、学習時にこれらの女性に対して誤ったときの罰則が強く反映されていない。 5.3.5 利点 ALE plots は偏らない、つまり、ALE プロット は特徴量が相関している時でも機能します。 PDP は、相互作用があると、ありそうもない、または、現実的に不可能な特徴量の組み合わせを周辺化により作り出してしまうため、失敗します。 ALE プロットは PDP よりも計算が高速 で、最大の区間の数はインスタンスの数となるため、O(n) となります。PDP は、推定するグリッド点の数の n 倍の計算が必要です。 例えば、20 個のグリッド点の場合、ALE プロットでは、最悪でもインスタンスが含まれている区間の数だけ予測すればいいのに対して、PDPでは、20倍の予測を必要とします。 ALE プロットの解釈は明白です。与えられた値の条件の元、予測においての特徴量の変化の相対的な影響はALE plotsから読み解けます。 ALE プロット0に中心化されています。 ALE 曲線の各点での値は、予測平均との差なので、解釈がしやすくなります。 2D ALE プロットは相互作用のみ示します。 もし、2つの特徴量に相互作用がない場合、プロットは何も表示しません。 特徴量はたいてい、何らかの形で相関しているため、私はPDPよりも、ALE プロットを用いることを好みます。 5.3.6 欠点 ALE プロットは区間数が多いとき、少し不安定(値が上下する)になることがあります。 この場合、区間数を減らすことで、推定結果を安定にできますが、予測モデルの真の複雑さのいくつかを平坦化し隠してしまう恐れがあります。 区間の数を決める完璧な解決法はありません。 もし、インターバルの数が極端に少なけば、ALE プロットは正確ではないかもしれません。 もし、インターバルの数が極端に多いと、曲線はとても不安定になる事があります。 PDP とは違って、ALE プロットには ICE 曲線が付随していません。 PDP とっては、ICE curves は偉大です。なぜなら、特徴量中の不均一性を明らかにできるためであり、これは特徴量の効果がデータの一部で異なって見えるという事を意味します。 ALE プロットに対しては、インスタンス間で効果が異なるかを区間ごとに確認するしかありませんが、各区間には異なるインスタンスがあるため、ICE 曲線と同じではありません。 二次の ALE プロットは、特徴量の空間での安定性が異なっており、どの様な手法でもこれを可視化できません。 この理由としては、区間内の局所効果の推定時に、使用されるインスタンス数が異なっているからです。 結果として、全ての推定は異なる精度を持っています。（ただし、これは、最良の推定です。) この問題は、主効果ALEプロットのそれほど深刻ではないバージョンに存在します。 グリッドとして、分位数を使用しているため、全ての区間で同数のインスタンスが含まれています。しかし、いくつかの領域では、多くの短い区間になり、ALE 曲線はさらに多くの推定値から構成されます。 また、曲線の大部分を占める長い区間においては、インスタンス数は比較的少なくなります。 これは、例えば、年齢が高いときの子宮頸がん予測の ALE プロットで発生しました。 二次のALEプロットは、主効果を常に覚えておかなければいけないため、解釈するのに煩わしく感じるかもしれません。 ヒートマップを2つの特徴量の全体効果として読みたくなりますが、これは単に、相互作用による追加の効果でしかありません。 純粋な二次効果は、相互作用を発見するためには興味深いですが、その効果を理解するためには、主効果もプロットに統合する方が合理的だと考えます。 ALE plots の実装は、PDP に比べて、より複雑で、直感的でもありません。 特徴量が相関している場合、ALE plots はバイアスにかかっていませんが、特徴量が強く相関している場合、解釈は困難なままです。 なぜなら、もし、それらが強い相関を持っていた場合、個々の特徴量をそれぞれ変えていくのではなく、両方を同時に変える方が、効果を分析するためには、理にかなっているからです。 この欠点は ALE plots だけではなく、特徴量が強く相関している場合の一般的な問題です。 もし、特徴量が相関しておらず、計算にかかる時間が問題ではない時、PDPs が少し好まれます、なぜなら、PDP は理解しやすく、そして、ICE 曲線と共にプロットできるからです。 5.3.7 実装と代替手法 partial dependence plots と individual conditional expectation curves が代替手法であることについて説明していましたか？ 私の知る限り、ALEプロットの実装は、R言語で、提唱者によって実装されたALEPlot R packageとiml packageがあります。 Apley, Daniel W. &quot;Visualizing the effects of predictor variables in black box supervised learning models.&quot; arXiv preprint arXiv:1612.08468 (2016).↩ "],["interaction.html", "5.4 特徴量の相互作用", " 5.4 特徴量の相互作用 予測モデルにおいて特徴量の相互作用がある場合、ある特徴量は他の特徴量の値に影響を受けるため、予測は単に特徴量の影響の和では表現できなくなります。 アリストテレスは「全体は部分の総和より勝る」という言葉を残しましたが、これも相互作用の存在により成り立つといえるでしょう。 5.4.1 特徴量の相互作用とは 機械学習モデルが2つの特徴量に基づいて予測する場合、この予測は定数項、1つ目の特徴量、2つ目の特徴量、2つの特徴量の相互作用の4つの項に分解できます。 2つの特徴量の相互作用は個々の特徴量の効果を考慮したのち、特徴量を変化させることによって生じる予測の変化を指します。 例として、モデルが家の大きさ（大きいか小さいか）と家の立地（良いか悪いか）の2つを特徴量として家の価値を予測する場合を考えると、以下のような4つの予測が考えられます。 立地 大きさ 予測 良い 大きい 300,000 良い 小さい 200,000 悪い 大きい 250,000 悪い 小さい 150,000 この予測結果は 定数項 (150,000）、家の大きさ（大きければ+100,000、小さければ+0）、家の立地（よければ+50,000、悪ければ+0） のように分解できます。 この分解はモデルの予測を完璧に説明しています。 モデルの予測は大きさと立地の各特徴量の影響の和と等しくなっているため、相互作用による影響はありません。 小さな家から大きな家に変更した場合、予測は立地に関わらず100,000だけ上がり、同様に立地を悪い家から良い家に変更した場合も、予測は家の大きさに関わらず50,000だけ増加します。 次に、相互作用が存在する場合について考えます。 立地 大きさ 予測 良い 大きい 400,000 良い 小さい 200,000 悪い 大きい 250,000 悪い 小さい 150,000 この予測結果を 定数項 (150,000）、家の大きさ（大きければ+100,000、小さければ+0) 、家の立地 (よければ+50,000、悪ければ+0) のように分解してみます。 ただし、今回は相互作用による項 (家のサイズが大きくかつ立地が良いならば+100,000) を追加で考える必要があります。 この場合、立地によって、家が大きい場合と小さい場合で予測に差が生じるため、家の大きさと立地の間に相互作用があると言えます。 相互作用の大きさを測る1つの方法として、特徴量の相互作用により予測がどの程度変化するかを測る方法があります。 これは H 統計量 と呼ばれ、Friedman と Popescu によって2008年に提案されました31。 5.4.2 Friedman の H統計量の理論 これから、2つのケースについて扱います。 1つ目は、モデル内の2つの特徴が相互作用するかどうかや、その程度を示す双方向相互作用の尺度です。 2つ目は、ある特徴量がモデル内で他の全ての特徴量と相互作用するかどうかやその程度を示す総合的な相互作用の尺度です。 理論的には、任意の数の特徴量間の相互作用を測定できます。しかし、上記の2つが最も興味深いケースです。 もし2つの特徴量が相互作用しない場合、partial dependence function として、以下のように分解できます (ただし、partial dependence function はゼロに中心化されていると仮定) 。 \\[PD_{jk}(x_j,x_k)=PD_j(x_j)+PD_k(x_k)\\] ここで、 \\(PD_{jk}(x_j,x_k)\\) は2つの特徴量に関する2方向の partial dependence function であり、\\(PD_j(x_j)\\) と \\(PD_k(x_k)\\) は1つの特徴量についてのpartial dependence functionです。 同様に、もしも1つの特徴量が他のいかなる特徴量とも相互作用しない場合、予測関数 \\(\\hat{f}(x)\\) を partial dependence function の和として表現できます。ここで、和の第1項は j にのみ依存し、第2項は j 以外に依存します。 \\[\\hat{f}(x)=PD_j(x_j)+PD_{-j}(x_{-j})\\] ただし、\\(PD_{-j}(x_{-j})\\) はj番目を除いた全ての特徴量に依存する partial dependence function です。 この分解は、(特徴量 j と k、もしくは、特徴量 j とその他に) 相互作用のない partial dependence function (もしくは完全予測関数) を表します。 次のステップとして、観測された partial dependence function と相互作用を持たない分解された関数の差を測定します。 2つの特徴間の相互作用を測るための partial dependence function や、ある特徴量とそれ以外の間の相互作用を測るための関数全体の出力のばらつきを計算します。 相互作用 (観測値と理論的な相互作用のない状態のPDの差分)による因子寄与の量が、相互作用の強さを示す統計量として用いられています。 相互作用が何もない場合、統計量は 0 であり、 \\(PD_{jk}\\) もしくは \\(\\hat{f}\\) の全ての分散がpartial dependence functionの合計によって説明される場合は1になります。 相互作用の統計量が 1 のとき、2つの特徴量のそれぞれの PD function は定数で、予測の影響が相互作用のみであることを意味しています。 この H 統計量は 1 よりも大きくなることがあり、解釈がより困難になります。 これは、双方向相互作用の分散が2次元の partial dependence plot の分散よりも大きい場合に起こる可能性があります。 数学的には、特徴量 j, k 間の H 統計量は Friedman と Popescu によって与えられ、以下の式で書き表されます。 \\[H^2_{jk}=\\sum_{i=1}^n\\left[PD_{jk}(x_{j}^{(i)},x_k^{(i)})-PD_j(x_j^{(i)})-PD_k(x_{k}^{(i)})\\right]^2/\\sum_{i=1}^n{PD}^2_{jk}(x_j^{(i)},x_k^{(i)})\\] 特徴量 j とそれ以外の全ての特徴量との相互作用を測るのも同様にして以下の通りに計算できます。 \\[H^2_{j}=\\sum_{i=1}^n\\left[\\hat{f}(x^{(i)})-PD_j(x_j^{(i)})-PD_{-j}(x_{-j}^{(i)})\\right]^2/\\sum_{i=1}^n\\hat{f}^2(x^{(i)})\\] 全てのデータ点で反復し、それぞれのデータ点で partial dependence を評価する必要があるため、H 統計量は評価にはコストがかかります。 最悪のケースでは、双方向H統計量 (j vs. k) を計算するために、2n2 回の機械学習モデルの予測を呼び出す必要があり、全てのH統計量 (j vs. all) を計算するためには、3n2 回必要になります。 計算を高速化するために、n 個のデータ点をサンプリングする方法があります。 この方法は partial dependence の推定のばらつきを高めてしまう欠点があるため、これによって H 統計量は不安定になります。 従って、もしも計算負荷を減らすためにサンプリングを用いている場合は、十分なデータ点が取得できているか注意してください。 Friedman と Popescu は、H統計量が0と有意に異なるかどうかを評価するための検定も提案しています。 この場合の帰無仮説は、「相互作用がない」です。 帰無仮説の下で相互作用統計量を生み出すには、モデルを調整して特徴量 j と k 、もしくは j と他の全ての特徴量との間に相互作用が無いようにする必要がありますが、これは任意のモデルのタイプで行えるわけではありません。 よって、この検定はモデル非依存ではなく、モデル特有のものなのでここでは扱いません。 もしも予測対象が確率である場合、相互作用の強さを測る統計量は分類問題にも適用できます。 5.4.3 例 特徴量の相互作用がどんなものか実際に見てみましょう。 自転車のレンタル数の予測について、天気とカレンダーの特徴量使ってSVMで予測したときの特徴量間の相互作用の強さを計算してみましょう。 FIGURE 5.23: 自転車レンタル数をSVMで予測した時の、ある特徴量から他の特徴量に対する相互作用の強さ（H統計量）。全体的に、特徴量間の相互作用は非常に弱い（いずれも10%未満）。 次は分類問題に対する相互作用を計算してみましょう。 様々なリスク要因が与えられた時の子宮頸がんの予測を学習したランダムフォレストの特徴量間の相互作用を分析してみましょう。 FIGURE 5.24: 子宮頸がんになる確率をランダムフォレストを用いて予測した時の、各特徴量から他の特徴量に対する相互作用の強さ（H統計量）。 相互作用の強さは経口避妊薬の服用年数が1番高く、出産人数が次に高い。 それぞれの特徴量が他の特徴量との相互作用を確認しました。 今度は特定の特徴量に注目して、その変数が他の各々の特徴量とどれだけ相互作用するかの、双方向的な相互作用に掘り下げていきましょう。 FIGURE 5.25: 妊娠回数と他の特徴量における双方向的な相互作用の強さ（H統計量）。妊娠回数と年齢に強い相互作用が認められる。 5.4.4 利点 相互作用の強さを計算するH統計量には、partial dependence decompositionという理論的な背景があります。 H 統計量は意味のある解釈を持ちます。 この相互作用は、相互作用による分散の説明度合いとして定義されます。 H 統計量は無次元なので、特徴量間やモデル間で比較が可能です。 H 統計量は、特定の形式によらず、どのような種類の相互作用でも検出できます。 H 統計量を使うと、3つ以上の特徴量間における相互作用などの、より高次の相互作用も分析できます。 5.4.5 欠点 まず始めに、H特徴量は計算コストが高いため、計算に時間がかかります。 計算には周辺分布の計算が必要です。 この計算は、もし全てのデータ点を使用しないのであれば、推定がばらつきます。 これは、データをサンプリングすると、推定値は実行ごとに異なり結果が不安定になるということです。 安定した結果を得るために、もしデータが十分であれば、数回 H 統計量の計算を繰り返すことをおすすめします。 また、相互作用が 0 より有意に大きいかどうかは不明瞭です。 これには、仮説検定が必要かもしれませんが、このモデル非依存な検定は存在しません。 検定に関して言えば、相互作用が「強い」と考えるに足るH統計量の大きさを決めることも難しいです。 また、H 統計量は 1 より大きくなる場合もあり、この場合解釈が難しくなります。 H 統計量は相互作用の大きさを表しますが、相互作用の性質を説明できません。 そこで partial dependence plots が登場します。 主な作業の流れとしては、まず相互作用の大きさを測ったのち、関心のある相互作用について2次元の partial dependence plots を作成します。 H 統計量は入力がピクセルの場合得られる効果が少なくなります。 このため、この方法は画像の分類に対しては有効ではありません。 相互作用の統計は、特徴量を独立にシャッフルできるという仮定のもとに成り立ちます。 もし特徴量間が強く相関する場合、この仮定は成り立たなくなるため、**実際には起こらないであろう特徴量の組み合わせをつくります。 これは PDP についても同様です。 ただし、これが過大評価に繋がるか過小評価に繋がるのかは一概に言えません。 小規模なシミュレーションでは、期待とは異なる奇妙な振舞いをします。 しかし、これは観測データの不足による不確かさによるものです。 5.4.6 実装 本書で用いた R の iml パッケージは現行版をCRAN から、開発版は Github から入手できます。 他にも特定のモデルを対象にした実装があります。 R パッケージの pre は RuleFit とH統計量を実装しています。 R パッケージの gbm は勾配ブースティングモデルと H 統計量を実装しています。 5.4.7 代替手法 H 統計量が相互作用を定量化する唯一の手法ではありません。 Hooker (2004)32 による Variable Interaction Networks (VIN) は、予測関数を主効果と相互作用に分解する方法です。特徴量間の相互作用はネットワークとして可視化されます。 不運にも、まだVINのソフトウェアは存在しません。 Greenwell ら(2018)33 によって、2つの特徴量の相互作用を定量化するための pertial dependence が提案されました。 この方法は、ある特徴量の重要度（partial dependence functionの分散として定義される）について、他の特徴量の値を固定した上で測ります。 もし分散が大きい場合特徴量は相互作用し、分散が 0 である場合相互作用はありません。 この機能を提供する R パッケージ vip は Githubから利用できます。 このパッケージでは partial dependence plots や feature importance などの機能も含みます。 Friedman, Jerome H, and Bogdan E Popescu. &quot;Predictive learning via rule ensembles.&quot; The Annals of Applied Statistics. JSTOR, 916–54. (2008).↩ Hooker, Giles. &quot;Discovering additive structure in black box functions.&quot; Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining. (2004).↩ Greenwell, Brandon M., Bradley C. Boehmke, and Andrew J. McCarthy. &quot;A simple and effective model-based variable importance measure.&quot; arXiv preprint arXiv:1805.04755 (2018).↩ "],["feature-importance.html", "5.5 Permutation Feature Importance", " 5.5 Permutation Feature Importance Permutation feature importance は、特徴量の値を並び替えることで、特徴量と真の結果との関係性を壊し、これによる予測誤差の増加を測定します。 5.5.1 理論 概念はとても単純です。 特徴量を並び替えたあとのモデルの予測誤差の影響を計算することで、特徴量の重要度を計算します。 特徴量の値を入れ替えるとモデル誤差が増加する場合、モデルは特徴量に依存した予測をしているので、その特徴量は「重要」です。 特徴量の値を入れ替えてもモデル誤差が変わらない場合、特徴量は「重要ではない」と言えます。 permutation feature importance は、Breiman (2001)34によってランダムフォレストのために導入されました。 この考えに基づいて、Fisher, Rudin, Dominici (2018)35は、モデルに依存しない特徴量重要度を提案し、これをモデル信頼度と呼んでいます。 彼らは、それに加えて特徴量の重要度に関する、より高度な考え方、例えば、多くの予測モデルがデータをうまく予測する可能性があることを考慮した（モデル固有の）バージョンも紹介しています。 彼らの論文 The permutation feature importance algorithm based on Fisher, Rudin, and Dominici (2018) は一読の価値があります。 入力: 学習モデル f, 特徴量行列 X, 目標ベクトル y, 誤差関数 L(y,f) 元のモデル誤差 eorig = L(y, f(X)) を推定します。（例: 平均二乗誤差） 各特徴量 j = 1,....,p について データ X の特徴量 j を並べ替えて特徴量行列 Xperm を生成します。これにより、特徴量 j と真の結果 y との間の関連付けが解除されます。 並べ替えられたデータの予測値に基づいて、誤差 eperm = L(Y,f(Xperm)) を推定します。 並べ替えた特徴量の重要度 FIj= eperm/eorig を計算します。あるいは、差分 FIj = eperm - eorig も使用できます。 FIが高い順に特徴量をソートします。 Fisher, Rudin, Dominici (2018)の論文では、特徴量 j を並べ替える代わりに、データセットを半分に分割し、それらの間で特徴量 j の値を入れ替えることを提案しています。 これは考えてみれば、特徴量 j を並べ替えるのと全く同じです。 より正確な推定をしたい場合は、各インスタンスを他のインスタンスの特徴量 j の値とペアにすることで、特徴量 j の並べ替えの誤差を推定できます（自分自身とのペアを除く）。 これにより、並べ替え誤差を推定するためのn(n-1)のサイズのデータセットが得られますが、膨大な計算時間がかかります。 厳密な推定値を得ることを真剣に考えている場合にのみ、この方法を使うことをお勧めします。 5.5.2 特徴量の重要度は、学習データとテストデータのどちらで計算するべきか tl;dr: 明確な答えはありません。 学習データかテストデータかの疑問に答えることは、特徴量重要度とは何かという根本的な問題に首を突っ込むことになります。 学習データとテストデータに基づいた特徴量重要度の違いを理解するためには、「極端な」例が一番適しています。 50個のランダムな特徴量（200のインスタンス）に基づいて、連続でランダムなターゲットを予測するサポートベクタマシンを学習しました。 「ランダム」というのは、ターゲットが 50 個の特徴量とは独立であることを意味しています。 これは最新の宝くじの番号から明日の気温を予測するようなものです。 もしモデルが何かの関係性を「学習した」のならば、それは過学習だといえます。 そして実際、SVMは学習データに過学習してしまいました。 平均絶対誤差 (MAE) は学習データにおいて 0.29 、テストデータにおいて 0.82 であり、これは常に平均値が0 (MAEは 0.78 ) となる結果を予測するような最良のモデルの誤差でもあります。 言い換えれば、このSVMモデルは何の役にも立たないということです。 この過学習したSVMの50個の特徴量の重要度にどんな値が期待できるでしょうか。 まだ見ぬテストデータのパフォーマンスを向上させるような特徴量がないことから 0 となるのでしょうか。 もしくは、学習された関係性がまだ見ぬデータに一般化されているかどうかにかかわらず、重要度はモデルがどれだけそれぞれの特徴量に依存しているかを反映するのでしょうか。 学習データとテストデータの特徴量重要度の分布がどのように異なるのか見ていきましょう。 FIGURE 5.26: データのタイプによる特徴量重要度の分布。SVM は 50 個のランダムな特徴量を持つ 200 のインスタンスからなる回帰データセットで学習された。SVM は過学習しており、学習データに対する特徴量重要度では重要な特徴量がいくつか示されている。一方で、まだ見ぬテストデータに関して計算したところ、特徴量重要度は比率が 1 に近い値（＝重要ではない）となった。 2つの結果のどちらが望ましいかは明確ではありません。 そこで、双方のケースそれぞれについて述べてみますので、どちらがいいのかあなた自身で考えてみてください。 テストデータのケース こちらについては単純です。 学習データに基づくモデル誤差の推定値は役に立たないということから、特徴量の重要度はモデル誤差の推定値に基づいているため、学習データに基づく特徴量重要度は役に立たないと言えます。 これは機械学習において一番最初に学ぶことですが、モデルを学習したときと同じデータでモデル誤差（もしくは性能）を測ると、それは常に楽観的すぎるものになり、実際よりもモデルの性能がよく思えてしまいます。 したがって、permutation feature importance はモデル誤差に基づいているので、まだ見ぬテストデータを用いるべきと言えます。 学習データに基づく特徴量重要度では、実際にはモデルがただ過学習しているだけで全く重要でない特徴量にもかかわらず、予測に重要であるという誤解が生まれてしまいます。 学習データのケース 学習データを使用することに関する議論は形式化するのが難しいですが、私見ではテストデータに関する議論と同じくらい説得力があるように思えます。 違う観点からこの役に立たない SVM についてみていきましょう。 学習データによると、重要な特徴量は X42 でした。 X42 についての Partial Dependence Plot (PDP) について見ていきましょう。 PDP により、モデルの出力が特徴量の違いによってどのように変わるのかを見ることができ、これは汎化誤差に依存しません。 PDP では、学習データかテストデータかは問題ではないのです。 FIGURE 5.27: 学習データに基づく最も重要な特徴量 X42 に関するPDP。このプロットによって、SVMが予測をする際にどのようにこの特徴量を用いているかが示されています。 このプロットによると明らかに SVM は特徴量X42 に依存して学習されていますが、テストデータに基づく特徴量重要度 (1) によると、この特徴量は重要ではありません。 学習データに基づく重要度は1.19であり、これはモデルがこの特徴量を用いるように学習されたことを示しています。 学習データに基づいた特徴量重要度は、予測をする際にそれらに依存しているという意味で、モデルにとって重要な特徴量はどれかを教えてくれます。 学習データを用いるケースの一環として、テストデータを使用することに対する反論を紹介したいと思います。 実際、最終的に可能な限り最良なモデルを得るために、すべてのデータを学習に使いたいと思うでしょう。 これは、特徴量重要度を計算するための未使用のテストデータが残されていないことを意味します。 モデルの汎化誤差を推定するときも同じような問題に直面します。 特徴量重要度を推定するために（ネストされた）交差検証を用いる場合、すべてのデータを学習に用いた最終的なモデルでは特徴量重要度が計算できず、一方でデータの一部を使ったモデルでは異なる振る舞いをしてしまうかもしれない、といった問題に直面するでしょう。 結局、モデルの予測がどの特徴量に頼っているのか（→学習データに基づく特徴量重要度）、もしくはどの特徴量がまだ見ぬデータに対するモデルの性能に寄与しているのか（→テストデータに基づく特徴量重要度）、どちらを知りたいのか決める必要があります。 私の知る限りでは、学習データとテストデータの疑問に関する研究はありません。 「役立たずの SVM」の例よりも徹底的な調査が必要でしょう。 より良い理解を得るためには、これらのツールについてより多くの研究と経験が必要になってくるでしょう。 次に、例をいくつか見ていきましょう。 重要度の計算は、どちらか1つを選ばなければならず、必要となるコードが少なくて済むので、学習データに基づいています。 5.5.3 例と解釈 分類と回帰の例を示します。 子宮頸がん（分類） 子宮頸がんを予測するためにランダムフォレストモデルを学習します。 誤差の増加を 1-AUC（1からROC曲線下面積を引いた値）で測定します。 並べ替えるとモデル誤差が 1 倍に増加する（＝変化なし）特徴量は、子宮頸がんの予測には重要ではありません。 FIGURE 5.28: ランダムフォレストを用いて子宮頸がんを予測するための特徴量重要度。最も重要な特徴量は Hormonal.Contraceptives..years. で、1-AUCは 6.13 倍に増加した。 最も重要度の高い特徴量は Hormonal.Contraceptives..years. であり、誤差は 6.13 増加することがわかりました。 レンタル自転車（回帰） SVM を用いて、気象条件とカレンダーの情報が与えられたときのレンタル自転車の台数を予測します。 誤差の測定には平均絶対誤差を使用します。 FIGURE 5.29: SVM を用いた自転車レンタル台数を予測したときの各特徴量の重要度。最も重要な特徴量は temp で、最も重要でない特徴量は holiday でした。 5.5.4 利点 特徴量重要度が高いということは、特徴量の情報が破壊されたときにモデル誤差が増加するという すばらしい解釈 が可能です。 特徴量重要度は、モデルの振る舞いについて 高度に圧縮されたグローバルな洞察 を提供します。 誤差の差分の代わりに誤差の比を使用することの良い側面は、特徴量重要度の測定値が異なる問題間で比較可能であることです。 重要度の計算は、自動的に他の特徴量とのすべての相互作用を考慮に入れます。 特徴量を並べ替えることは、他の特徴量との相互作用の効果も破壊します。 これは、並べ替えた特徴量の重要度が，モデル性能における特徴量としての主効果と相互作用による効果の両方を考慮に入れることを意味します。 このことは、2つの特徴量間の相互作用の重要度が、両方の特徴量の重要度測定に含まれているため、欠点でもあります。 これは、特徴量重要度が性能の全体的な低下の合計ではなく、総和が大きくなってしまうことを意味します。 線形モデルのように特徴間の相互作用がない場合にのみ、重要度はほぼ総和になります。 Permutation feature importance は モデルの再学習を必要としません。 他の方法の中には，特徴量を削除してモデルを再学習し、モデル誤差を比較するものもあります。 機械学習モデルの再学習には長い時間がかかるので、特徴量を並べ替える &quot;だけ&quot; で済めば多くの時間を節約できます。 特徴量の一部でモデルを再学習する方法は一見すると直感的に見えますが、データを減らしたモデルでは特徴量重要度に意味がありません。 我々が知りたいのは、モデルを固定して得られる特徴量重要度なのです。 削減されたデータセットで再学習すると、我々が興味を持っているモデルとは異なるモデルが得られます。 例えば、スパースな線形モデル（Lassoを使用）を、使用する特徴量の数を固定して重みを学習したとします。 データセットには 100 個の特徴量があり、非ゼロの重みの数を 5 に設定します。 非ゼロの重みを持つ特徴量のうちの 1 つの重要度を分析します。 次に、その特徴量を削除してモデルを再学習します。 その結果、同じように良い特徴量に非ゼロの重みが与えら、モデルの性能が変わらなかったとすると、先ほどの特徴量は重要ではないという結論に至ります。 また、こんなことも起こり得ます。 モデルは決定木で、最初の分割として選ばれた特徴量の重要度を分析します。 その特徴量を削除してモデルを再学習します。 別の特徴量が最初の分割として選択されるので、木全体が全く異なったものになります。つまり、（潜在的に）全く異なる木の誤差率を比較して、その特徴量が元の木にとって、どれだけ重要かを決定することになってしまうのです。 5.5.5 欠点 特徴量重要度を計算するために学習データを用いるべきか、テストデータを用いるべきかは明らかではありません。 Permutation feature importance は モデル誤差と関連しています 。 これは本質的に悪いことではありませんが、場合によっては我々が求めるものと異なります。 ある特徴量がモデルの性能に対しどういう意味を持つかを考えることなく、その特徴量によってモデルの出力がどう変わるかを知りたいケースもあります。 例えば、誰かによって特徴量が改ざんされた場合にモデルの出力がどの程度頑健か知りたい場合です。 この場合には、特徴量が並べ替えられた場合にどの程度モデルの性能が落ちるかではなく、モデルの出力の変化が各特徴量によってどの程度説明されるのかを知りたいはずです。 （特徴量によって説明される）モデルの出力変化と特徴量重要度は、モデルが良く汎化されている（つまり過学習していない）時には、強く相関します。 真の結果にアクセスできる必要があります。 もしモデルとラベル付けされていないデータのみが与えられ、真の結果がなかったならば、並べ替えた特徴量の重要度は計算できません。 permutation feature importance は特徴量のシャッフルに依存しているため、推定結果にランダム性を持ちます。 何度も計算すると、結果が大きく変わるかもしれません。 重要度の測定値を繰り返し平均すれば、推定結果は安定しますが、計算にかかる時間は増大します。 もし特徴量に相関があるのならば、permutation feature importance は現実的ではないインスタンスによって結果が歪められるかもしれません。 これは、Partial Dependence Plots と同じ問題です。 2つ以上の特徴量に相関がある場合、特徴量の並べ替えを行うことで、起こりえないデータを作り出してしまいます。 （人間の体重と身長のように）正の相関があり、その1つをシャッフルするとき、ありえないインスタンス（例えば身長 2m, 体重 30kg）を作り出してしまいますが、そのようなインスタンスも特徴量重要度の計算に使用されてしまいます。 言い換えれば、他の特徴量と相関のある場合の permutation feature importance は、特徴量の入れ替えによって得られた現実に観測しえないようなデータに対してモデルの性能がどの程度落ちたかについて考えていると言えます。 特徴量が強く相関しているかどうかをチェックし、相関がある場合は、特徴量重要度の解釈に注意するようにしましょう。 もう1つ、トリッキーなことがあります。 相関した特徴量を追加すると、両方の特徴量の間で重要度を分割することで、関連する特徴量の重要度が低下する、ということが起こり得ます。 特徴量重要度を「分割する」とはどういうことか、例を挙げてみましょう。 他の相関のない特徴量とともに前日の午前8時の気温を特徴量として使用し、降水確率を予測します。 ランダムフォレストで学習し、気温が最も重要な特徴量だとわかり、うまくいったので夜はよく眠れました。 では、午前8時の気温と強く相関するような午前9時の気温を追加したときを想像してみましょう。 午前8時の気温をすでに知っていたならば、午前9時の気温はそれほど多くの情報を与えません。 でも、特徴量はたくさんあるほうが良いはずです。 ランダムフォレストを2つの気温の特徴量と、他の相関のない特徴量から学習しました。 ランダムフォレストの個々の木は午前8時の気温を選んだり、午前9時の気温を選んだり、あるいは両方選んだりどちらも選ばなかったりしました。 2つの気温の特徴量のどちらもは、1つだけで学習したときよりも少しだけ重要度が高くなっていますが、特徴量を重要な順にリストかすると、どちらの気温も真ん中ぐらいの順位となっていました。 相関する特徴量を入れることによって、最も重要な特徴量をトップから平凡なものへと蹴り落してしまいました。 ある意味では、これは単に機械学習モデル（ここではランダムフォレスト）の動作を反映しているだけなので、問題ありません。 モデルが午前9時の気温にも頼ることができるようになったので、単純に午前8時の気温の重要度が低くなっただけです。 一方で、これによって特徴量重要度の解釈がかなり難しいものになってしまいます。 計測誤差について特徴量をチェックすることを想像してみてください。 チェックは大変なので、重要な特徴量を上位3つだけチェックすることに決めたとしましょう。 最初のケース(午前8時の気温のみ)では、最も重要な特徴量である気温をチェックしますが、2つ目のケース(午前9時の気温も含む) では、重要度がシェアされてしまっているので重要度の上位3つに気温の特徴量は1つも含まれません。 相関のある特徴量がある場合、重要度の値はモデルの動作レベルでは理解しうるものであったとしても、混乱を招く恐れがあります。 5.5.6 ソフトウェアと代替手法 例では、R パッケージの iml を用いました。 R パッケージ DALEX や vip、Pythonライブラリ alibi もまた、モデルに依存しない、permutation feature importance の実装があります。 PIMP と呼ばれるアルゴリズムは、重要度の p値 を提供するために、特徴量重要度アルゴリズムを改良しています。 Breiman, Leo.“Random Forests.” Machine Learning 45 (1). Springer: 5-32 (2001).↩ Fisher, Aaron, Cynthia Rudin, and Francesca Dominici. “Model Class Reliance: Variable importance measures for any machine learning model class, from the ‘Rashomon’ perspective.” http://arxiv.org/abs/1801.01489 (2018).↩ "],["global.html", "5.6 グローバルサロゲート (Global Surrogate)", " 5.6 グローバルサロゲート (Global Surrogate) グローバルサロゲートモデル (global surrogate model) は、ブラックボックスモデルの予測を近似するよう学習された解釈可能なモデルです。 サロゲートモデルを解釈することによって、ブラックボックスモデルについて結論を導き出せます。 機械学習をさらに活用して、機械学習の解釈性を解決するのです。 5.6.1 理論 サロゲートモデルは、工学の分野でも用いられています。 興味のある結果を得るのが高価であったり時間がかかったり、そもそも計測が困難である場合 (複雑なコンピュータシミュレーションに依存するなど) には、代わりに安価で高速なサロゲートモデルの結果で代替されます。 工学の分野で用いられるサロゲートモデルと解釈可能な機械学習で用いられるサロゲートモデルの違いは、モデルはシミュレーションではなく、解釈可能な機械学習モデル (シミュレーションではなく) あるということです。 (解釈可能な) サロゲートモデルの目的は、元のモデルの予測をできるだけ正確に近似し、同時に解釈可能にすることです。 サロゲートモデルのアイデアは様々な名称として見つけることができます。 近似モデル、メタモデル、応答曲面モデル、エミュレータなど。 実は、サロゲートモデルを理解するために必要な理論はあまりありません。 ブラックボックスモデルの予測関数 f を、解釈可能という制約の元、サロゲートモデルの予測関数 g でできるだけ正確に近似したいというだけのことなのです。 関数 g は任意の解釈モデルで利用できます。 -- 例えば、interpretable models chapter -- で紹介されているものが使えます。 線形モデルについて、g は以下のように表せます。 \\[g(x)=\\beta_0+\\beta_1{}x_1{}+\\ldots+\\beta_p{}x_p\\] 決定木について、g は以下のように表せます。 \\[g(x)=\\sum_{m=1}^Mc_m{}I\\{x\\in{}R_m\\}\\] サロゲートモデルの学習は、ブラックボックスモデルの内部の情報を必要とせず、データと予測関数へのアクセスのみアクセスできれば良いので、モデル非依存の手法と言えます。 元の機械学習モデルが他のものと変わったとしても、引き続きサロゲート手法を利用できます。 ブラックボックスモデルの種類とサロゲートモデルの選択は切り離されているのです。 以下の手順を実行することで、サロゲートモデルを得ます。 データセット X を選択します。 これはブラックボックスモデルを学習するのに使用したものと同じデータセット、もしくは、同じ分布の他のデータセット。アプリケーションに応じてデータの部分集合やデータの一部を選択できます。 選択されたデータセット X について、ブラックボックスモデルの予測値を取得します。 解釈可能なモデル(線形モデル、決定木など)を選択します。 データセットXと予測値に基づいて解釈可能なモデルを学習します。 おめでとうございます、これでサロゲートモデルを取得できました。 サロゲートモデルがブラックボックスモデルの予測をどの程度再現できているのかを測定します。 サロゲートモデルを解釈します。 あるサロゲートモデルのアプローチには、追加のステップが存在していたり、多少の差はあったりするかもしれませんが、一般的な考え方はここで説明されている通りです。 サロゲートモデルがブラックボックスモデルをどの程度再現しているのかを計測する方法の1つに R2 スコアがあります。 \\[R^2=1-\\frac{SSE}{SST}=1-\\frac{\\sum_{i=1}^n(\\hat{y}_*^{(i)}-\\hat{y}^{(i)})^2}{\\sum_{i=1}^n(\\hat{y}^{(i)}-\\bar{\\hat{y}})^2}\\] ここで、\\(\\hat{y}_*^{(i)}\\) はサロゲートモデルのi番目のインスタンスの予測値で、\\(\\hat{y}^{(i)}\\) はブラックボックスモデルの予測値、\\(\\bar{\\hat{y}}\\) はブラックボックスモデルの予測値の平均です。 SSE は二乗誤差の総和 (Sum of Squares Error)、SST は二乗の総和 (Sum of Squares Total)です。 R2 の尺度はサロゲートモデルによって捉えられた分散の割合として解釈できます。 もし、R2 が 1 に近い (SSE が低い) 時、解釈可能モデルはブラックボックスモデルの挙動を非常によく近似できているということになります。 したがって、R2 が 1 に近い場合 (SSE が低い)、複雑なモデルを解釈可能なモデルに置き換えたほうが良いかもしれません。 また、R2 が0に近い (SSE が高い)時、解釈可能なモデルはブラックボックスモデルの説明に失敗しているということになります。 ここで、ブラックボックスモデルの性能自体、つまり実際の結果とブラックボックスモデルの予測の良し悪し には言及していないことに注意してください。 ブラックボックスモデルの性能は、サロゲートモデルの学習の成否と関係がありません。 サロゲートモデルの解釈は、現実世界に対してではなくモデルについて解釈するため有効です。 しかし、ブラックボックスモデルの性能が悪ければブラックボックスモデル自体が無意味になるので、当然サロゲートモデルの解釈は無意味になります。 また、元のデータの一部や、インスタンスに重み付けしたものに対してもサロゲートモデルを作ることも有効です。 その場合、サロゲートモデルの入力の分布は変化しているので、どこに注目して解釈するのかが変わります (なので、グローバルではなくなります) 。 データの特定のインスタンスに従って局所的に重み付けすると (あるインスタンスが選択されたインスタンスに近いほど重みが大きいなど)、個々のインスタンスの予測を説明できる局所的なサロゲートモデルを得られます。 ローカルなモデルについては次の章で詳しく解説しています。 5.6.2 例 サロゲートモデルを実演するために回帰と分類の例について考えてみましょう。 最初に、天気とカレンダーの情報が与えられたときの 1日の自転車のレンタル数を SVM で予測するため学習します。 SVMは解釈性が高くないので、決定木 (CART) を解釈可能なサロゲートモデルとして学習し、SVM の挙動を近似します。 FIGURE 5.30: 自転車のレンタルデータセットで学習した SVM の予測を近似したサロゲートモデルの木の終端ノード。ノード内の分布を確認すると、2年後以降 (カットポイントは435日) の気温が13度を超える時にサロゲートモデルが自転車のレンタル数が多くなると予測している。 このサロゲートモデルの R2 スコア (因子寄与) は 0.77 であり、これは元のブラックボックスの挙動を非常に良く近似していることを示していますが、ただし完全ではありません。 もし、これが完璧なら、SVM を捨てて、代わりに決定木を使うことができます。 2つ目の例として、ランダムフォレストを用いて子宮頸がんの確率を予測します。 ここでも元のデータセットを用いて決定木を学習しますが、データからの実際のクラス (健康なクラスとがんのクラス) の代わりに、ランダムフォレストの予測を結果として用います。 FIGURE 5.31: 子宮頸がんのデータセットで学習されたランダムフォレストの予測を近似するサロゲートモデルの木の終端ノード。ノード内のカウントはノード内でのブラックボックスモデルの分類の頻度を表す。 このサロゲートモデルの R2 スコア(因子寄与)は 0.19 であり、これはランダムフォレストをうまく近似していないことを示しています。そのため、複雑なモデルについての結論を出す時にこの木の学習結果を使用すべきではありません。 5.6.3 長所 サロゲートモデルの手法は柔軟です。 解釈可能なモデルの章のモデルならどのようなモデルでも使用できます。 これは、また、解釈可能なモデルだけでなく、ブラックボックスモデルでも使用できることを意味します。 あなたは、いくつかの複雑なモデルを作って、それをあなたの会社の異なるチームに説明することを想定してください。 あるチームは線形モデルに精通し、もう一方は決定木が理解できるとしましょう。 あなたは、元のブラックボックスモデルに対して、2つのサロゲートモデルを学習し、2種類の説明を提示できます。 もし、より性能の良いブラックボックスモデルを見つけたとしても、同じクラスのサロゲートモデルを使うことができるので解釈手法を変更する必要はありません。 私はこの手法が、とても直感的でストレートだと考えています。 つまり、実装しやすいことだけでなくデータサイエンスや機械学習に関わりのない人にも説明しやすいということです。 R2 スコアを用いると、サロゲートモデルがブラックボックスモデルの予測をどの程度、近似できているか簡単に計測できます。 5.6.4 短所 サロゲートモデルは実際の結果を見ていないので、 データではなくモデルについての結論を出していることに気をつける必要があります。 サロゲートモデルがブラックボックスモデルを十分近似しているかを示す最良のR2 スコアの閾値は明らかではありません(ばらつきの 80% が説明されることでしょうか？それとも 50% や 99% でしょうか？)。 サロゲートモデルがブラックボックスモデルにどれだけ近いかを計測できます。 近すぎないが、十分近くにいると仮定してみましょう。 データセットの一部では非常に近いが別の部分に対しては大きく乖離しているということが起こり得ます。 この場合、単純なモデルの解釈は全てのデータ点に対して等しく優れているとは言えません。 解釈可能なモデルとして選んだサロゲートモデルには長所と短所が存在します。 一般的に、本質的に解釈可能なモデル(線形モデルや決定木も含む)は存在せず、解釈可能であるという幻想を抱くことは危険ですらあると主張する人もいます。 あなたがこの意見に共感しているのであれば、当然この手法はあなたの求めるものではありません。 5.6.5 ソフトウェア 例では R パッケージの iml を使用しました。 機械学習モデルを学習できるのであれば、自分自身でサロゲートモデルを実装できるはずです。 単に、ブラックボックスモデルの予測値を予測する解釈可能なモデルを学習するだけです。 "],["lime.html", "5.7 Local Surrogate (LIME)", " 5.7 Local Surrogate (LIME) ローカルサロゲートモデルは解釈可能なモデルであり、ブラックボックスな機械学習モデルの個々の予測を説明するために用いられます。 Local interpretable model-agnostic explanations (LIME)36という論文の中で、具体的に局所的なサロゲートモデルの実装が提案されました。 サロゲートモデルは根底にあるブラックボックスモデルの予測を近似するように学習されます。 グローバルなサロゲートモデルを学習する代わりに、LIME は個々の予測を説明するためにローカルサロゲートモデルを学習することに焦点を当てています。 アイデアはとても直感的です。 最初に、学習データのことは忘れて、データを入れると予測値を返すブラックボックスモデルを持っているだけの状態を想像してください。 何度でもボックスを調べることができます。 目的は機械学習モデルがなぜ特定の予測を返すか理解することです。 LIME は、機械学習モデルの入力データに変動を加えた時、予測にどのような変化が起こるかを検証します。 LIME は、サンプルの特徴量の値を置き換えて得られた新しいデータセットを作成し、ブラックボックスモデルで予測します。 そして、この新しいデータセットに基づいて、解釈可能なモデルを学習します。このモデルは、サンプルされたインスタンスと関心のあるインスタンスの近接性によって重み付けされます。 解釈可能なモデルには interpretable models の章に記載されている Lasso や decision tree などのどのモデルも使用できます。 学習されたモデルは局所的には機械学習モデルの予測を近似していますが、大域的にはよい近似にグローバルになるとは言えません。 この種の正確性は局所忠実性 (local fidelity) とも呼ばれます。 数学的には、解釈可能な制約を加えたローカルサロゲートモデルは次のように表現されます。 \\[\\text{explanation}(x)=\\arg\\min_{g\\in{}G}L(f,g,\\pi_x)+\\Omega(g)\\] インスタンス x に対する説明モデルは、元のモデル f (例: xgboost モデル)の予測をどの程度説明できるかを表す損失 L (例: 二乗和誤差)を最小化するようなモデル g（例: 線形回帰モデル）となり、このモデルの複雑さ \\(\\Omega(g)\\) は小さい (例: より少ない特徴量)必要があります。 G は可能な説明の族で、例えばすべての可能な線形回帰モデル、などです。 近似度 (proximity measure) \\(\\pi_x\\) はインスタンス x を説明するために考慮する近傍をどの程度大きくするか定義します。 実際、LIME は損失関数を最適化しているだけです。 ユーザーは、線形回帰モデルが使用する特徴量の数の最大を選択することなどにより、複雑性を決定する必要があります。 ローカルサロゲートモデルは、次のように学習されます。 ブラックボックスな予測を説明したいインスタンスを選びます。 データセットを摂動させ、新たなデータ点に対するブラックボックスな予測を得ます。 新しいサンプルを、関心のあるインスタンスへの近さに応じて重み付けします。 ばらつきを加えて作成したデータセットで重み付きの解釈可能なモデルを学習します。 解釈可能な局所的なモデルによって予測を説明します。 例えば、現在の R と Python での実装では、線形回帰を解釈可能な代理モデルとして選択できます。 事前に、解釈可能なモデルが持つ特徴量の数 K を選択しなければなりません。 K が低ければ低いほど、モデルは解釈しやすくなります。 K が高ければ高いほど、より忠実なモデルとなります。 ちょうど K 個の特徴量を学習する方法はいくつかあります。 Lasso は良い選択と言えます。 正則化パラメータ \\(\\lambda\\) が大きい Lasso は何の特徴量もないモデルとなります。 ゆっくりと \\(\\lambda\\) を減少させていきながら Lasso を再学習していくと、非ゼロの重みをもつ特徴量が現れます。 モデルが K 個の特徴量を持つならば、希望する特徴量の数にたどり着いたことになります。 他の戦略は、特徴量の前方選択または後方選択です。 これは、完全なモデル（= すべての特徴量を含む）もしくは切片のみのモデルから始めて、どの特徴量が追加または削除されたときに最も改善がみられるかを、モデルの特徴量が K 個になるまで繰り返しテストします。 どうやってデータにばらつきを持たせるのでしょうか。 これは、データのタイプ（テキスト、画像、表形式など）に依存します。 テキストや画像では、1つの単語もしくはスーパーピクセルのオン・オフを切り替えるという方法があります。 表形式のデータでは、LIMEはそれぞれの特徴量をその平均と標準偏差を持つ正規分布に基づいて摂動させることによって新たなサンプルを作り出します。 5.7.1 表形式データにおける LIME 表形式のデータは、各行がインスタンスを表し、各列が特徴量を表しています。 LIMEのサンプルは、対象のインスタンスの周辺から取得されるのではなく、学習データの重心から取得され、これが問題になります。 しかし、これにより、サンプル点のいくつかが興味の対象のデータ点と異なる予測値を持つ可能性が高くなります。そのため、LIME が少なくとも何らかの説明を学習できる可能性も高くなります。 どのように、サンプリングや局所的なモデルの学習をしているかは視覚的に説明するのがベストです。 FIGURE 5.32: 表形式データのLIMEアルゴリズム。 A) 特徴量 x1, x2 が与えられたときのランダムフォレストの予測。予測されたクラス: 1（暗い領域）または 0（明るい領域）。 B) 対象のインスタンス（大きい点）および正規分布から選ばれたデータ（小さい点）。 C) 対象のインスタンスの周辺に、より大きい重みを割り当てる。 D) グリッドの記号は重みづけされたサンプルから局所的に学習されたモデルの分類を示している。白い線は決定境界 (P(class=1) = 0.5) を示している。 いつものように、悪魔は細部に潜んでいます。 データ点の周りに意味のある近傍を定義することは難しいです。 LIMEは現在、近傍を定義するのに指数平滑化カーネル (exponential smoothing kernal) を使用しています。 平滑化カーネルは、2つのデータのインスタンスを受け取り、近接度を返す関数です。 カーネルの幅は近傍の大きさを定義します。 カーネル幅が小さいということは、ローカルモデルに影響を与えるにはインスタンスが非常に近くなければならないことを意味し、カーネル幅が大きいということは、遠くにあるインスタンスもモデルに影響を与えることを意味します。 LIME の Python 実装 (file lime/lime_tabular.py) を見ると、（正規化されたデータ上で）指数平滑化カーネルが用いられていることがわかります。 また、カーネル幅は、学習データの列の数の平方根の 0.75 倍になっていることもわかります。 一見、罪のないコードに見えますが、見て見ぬ振りをしている点があります。 重大な問題は、カーネルやその幅を決める良い方法が無いことです。 一体どこから 0.75 という数字がきているのでしょうか。 以下の図のように、カーネル幅を変えることによって説明を簡単にねじ曲げることができます。 FIGURE 5.33: x = 1.6 という予測に対しての説明。1つの特徴量に依存しているブラックボックスモデルの予測値は太線で示し、データの分布は底部の棒線で示されている。異なるカーネル幅の3つのローカルサロゲートモデルが計算。線形回帰モデルの結果をみると、特徴量は x = 1.6 に対して負の効果があるか、正の効果があるか、あるいは全く効果がないかは、カーネル幅に依存していることがわかる。 例では1つの特徴量しか取り上げませんでした。 より高次元な特徴量空間ではもっと悪くなります。 また、全ての特徴量を同じ距離尺度で扱っていいものかは非常に不明確です。 特徴量 x1 と特徴量 x2 で 1 だけ変化したとき、同じ距離だけ変化したと扱っていいのでしょうか。 距離尺度は極めて恣意的で、異なるの次元（別の特徴量）とは全く互換性がないかもしれません。 5.7.1.1 例 具体例を見てみましょう。 レンタル自転車のデータに戻って、予測問題をクラス分類の問題にします。 時間が経つにつれてレンタル自転車の人気が高まっているという傾向を考慮した上で、ある日のレンタサイクルの台数がトレンドラインより多いのか、少ないのかを知りたいとします。 「多い」とは平均のレンタル数を上回っているかどうかと解釈できますが、しかし、トレンドに対して調整されています。 まず、分類タスクに対して 100 個の決定木を用いたランダムフォレストを学習します。 天気やカレンダーの情報を元に、何日にレンタル自転車が平均 (trend-free) を上回るでしょうか。 説明は 2 個の特徴量から作成されます。 学習されたスパース局所線形モデルによって予測クラスの異なる2つのインスタンスを説明すると、次のようになります。 FIGURE 5.34: レンタル自転車データセットの2つのインスタンスに対する LIME の説明。暖かく天気が良いと予測に正の影響を与える。x 軸は特徴量の効果を示しており、実際の特徴量の値と重みの積に相当する。 図から、量的特徴量よりもカテゴリカル特徴量の方が解釈しやすいことがわかりました。 1つの解決策は量的特徴量を度数によってカテゴライズすることです。 5.7.2 テキストデータに対するLIME テキストデータに対する LIME は表形式データに対する LIME とは異なります。 データのバリエーションは異なる方法で生成されます。 元のテキストから始めて、新たなテキストは元のテキストからランダムに単語を削除することによって作成されます。 データセットは各単語に対するバイナリ特徴量で表現されます。 対応する単語が含まれている場合は 1、削除されている場合は 0 です。 5.7.2.1 例 この例では、YouTubeのコメント をスパムか通常のコメントかで分類します。 ブラックボックスモデルとして、文書の単語行列上で学習した深い決定木を使用します。 この行列では、各コメントは1つの文書（= 1行）であり、各列は特定の単語の出現回数を表します。 短い決定木は容易に理解できますが、今回は非常に深い木を扱います。 この決定木の代わりに、単語の埋め込み（抽象的なベクトル）に対して学習された再帰ニューラルネットワークや SVM を用いることもできます。 このデータセットから2つのコメントと対応するクラス (スパムなら1、通常のコメントなら0) を見てみましょう。 CONTENT CLASS 267 PSY is a good guy 0 173 For Christmas Song visit my channel! ;) 1 次のステップでは、局所モデルで使用される、データセットのバリエーションを作成します。 例えば、1つのコメントに対するバリエーションは次のようになります。 For Christmas Song visit my channel! ;) prob weight 2 1 0 1 1 0 0 1 0.17 0.57 3 0 1 1 1 1 0 1 0.17 0.71 4 1 0 0 1 1 1 1 0.99 0.71 5 1 0 1 1 1 1 1 0.99 0.86 6 0 1 1 1 0 0 1 0.17 0.57 各列は文中の1つの単語に対応します。 各行が1つのバリエーションを表しており、1 は単語がこのバリエーションの一部であることを意味し、0 はその単語が削除されたことを意味します。 バリエーションの1つに対応する文は、&quot;Christmas Song visit my ;)&quot;です。 &quot;prob&quot; の列は各バリエーションがスパムである予測確率を表します。 &quot;weight&quot;の列は元の文へのバリエーションの近接度を表しており、1 から削除された単語の割合を引いたものとして計算されます。 例えば、7語のうち1語が削除された場合、近接度は 1 - 1/7 = 0.86 となります。 2つの文（一方はスパム、もう一方はスパムでない）とLIMEのアルゴリズムによって推定された重みを見てみましょう。 case label_prob feature feature_weight 1 0.1701170 is 0.000000 1 0.1701170 good 0.000000 1 0.1701170 a 0.000000 2 0.9939024 channel! 6.180747 2 0.9939024 Christmas 0.000000 2 0.9939024 Song 0.000000 &quot;チャンネル&quot; という単語がスパムである確率が高いことを示しています。 スパムでないコメントの場合、どの単語が削除されてもクラス予測に影響がないため、ゼロ以外の重みは推定されませんでした。 5.7.3 画像データに対するLIME このセクションは Verena Haunschmid が執筆しました。 画像に対する LIME は表形式データやテキストデータに対する LIME とは動作が異なります。 直感的には、複数のピクセルが1つのクラスに寄与するため、ひとつひとつのピクセルを摂動させることは意味がありません。 なので、ひとつひとつのピクセルをランダムに変更しても、予測結果はあまり変わらないでしょう。 そこで、画像を「スーパーピクセル」にセグメント化し、スーパーピクセルをオフ・オンに切り替えることで画像のバリエーションを作ります。 スーパーピクセルとは類似した色を相互接続したピクセルであり、それぞれのピクセルをグレーなどのユーザ定義の色に置き換えることでオフにできます。 また、ユーザは、それぞれの置換においてスーパーピクセルをオフにする確率を指定できます。 5.7.3.1 例 この例では、Inception V3 ニューラルネットワークによるクラス分類を見ていきます。 使用したのは、ボウルに入ったパンの画像です。 画像ごとに複数の予測ラベルを持つことができるので（確率でソートされている）、上位のラベルを説明できます。 1番目の予測は確率 77% で「ベーグル」であり、次に 4% で「ストロベリー」が続きます。 下の画像は、「ベーグル」と「ストロベリー」の LIME の説明を示しています。 説明はサンプル画像上に直接表示できます。 緑は画像のこの部分がラベルの確率を増加させることを意味し、赤は減少させることを意味します。 FIGURE 5.35: 左：ボウルに入ったのパンの画像。中央と右: Google の Inception V3 ニューラルネットワークによる画像分類の上位2クラス（ベーグル、ストロベリー）に対する LIME の説明。 「ベーグル」の予測と説明は、たとえ予測が間違っていたとしても、非常に合理的です。 実際には、中央に穴がないため、この画像はベーグルではありません。 5.7.4 長所 元の機械学習モデルを置き換えたとしても、説明にはそのまま同じ局所的な解釈可能モデルを使うことができます。 説明を受ける人は決定木を十分に理解していると仮定します。 ローカルサロゲートモデルを使用するため、実際の予測では決定木を使用していなくても、決定木を説明として使用できます。 例えば、予測モデルとして SVM を使用できます。 そして、xgboost モデルの方がうまく機能することがわかった場合は、SVM を xgboost モデルに置き換え、予測を説明する決定木はそのまま使用できます。 ローカルサロゲートモデルは、解釈可能なモデルの学習や解釈の文献や経験から恩恵を受けています。 Lasso や短い木を使用する場合、結果として得られる説明は短く（= 選択的）、対照的なものになる可能性があります。 したがって、人間に優しい説明になります。 これが、説明を受ける側が専門家でなかったり時間のない人であるようなアプリケーションで LIME がよく用いられる理由です。 ただし、完全な帰属に対しては十分ではないので、完全な予測の説明が法的に求められる場合などでは LIME が使われることはありません。 また、機械学習モデルのデバッグをする際、少しの理由ではなく全ての理由があると便利です。 LIME は表形式データ、テキストデータ、画像データ全てで有効な数少ない手法の1つです。 忠実度（解釈可能なモデルがブラックボックスの予測をどの程度近似しているか）は、対象の入力データ近傍におけるブラックボックスの予測を説明する際に、解釈可能なモデルがどれほど信頼できるかについて良いアイデアを与えてくれます。 LIME は Python (lime ライブラリ) や R (lime package と iml package) で実装されており、非常に簡単に使用できます。 局所代理モデルで作成された説明は、元のモデルが学習に使用したもの以外の特徴量を使用できます。 もちろん、これらの解釈可能な特徴量は入力データから得られるものでなければなりません。 テキスト分類器は特徴量として抽象的な単語の埋め込みを使用できますが、その説明は文内の単語の有無に基づいて行なうことができます。 回帰モデルが特徴量への解釈不可能な変換を用いて学習されたとしても、その説明は元の特徴量を使用して作成できます。 例えば、回帰モデルがアンケート回答の主成分分析 (PCA) の結果から学習されたとしても、LIME では元のアンケートの質問に対して学習できます。 LIME に解釈可能な特徴量を使用することは、特にモデルが解釈不可能な特徴量で学習されている場合に、他の手法に比べて大きな利点があります。 5.7.5 短所 表形式のデータに対して LIME を使用する際、近傍の正しい定義を与えることが非常に大きな未解決の問題となっています。 私はこれが LIME の最大の問題点であると考えており、LIME を慎重に使用することを推奨しています。 アプリケーションごとに異なるカーネルの設定を試して、説明が意味をなしているかを自分で確認する必要があります。 残念ながら、これが適切なカーネル幅を見つけるために私ができる最大限のアドバイスです。 サンプリングは LIME の現在の実装から改善される可能性があります。 データ点は、特徴量間の相関を無視して、ガウス分布からサンプリングされます。 これにより、実際には発生しがたいデータ点が局所的な説明モデルを学習する際に使用される可能性があります。 説明モデルの複雑さをあらかじめ定義する必要があります。 これに関しては、些細な不満です。 なぜなら、最終的には常にユーザが忠実度とスパース性の間の妥協点を定義する必要があるからです。 もう1つの非常に大きな問題点は、説明の不安定さです。 記事 37 の中で、著者はあるシミュレーションにおいて非常に近い2点の説明が大きく異なることを示しました。 また、私の経験上でも、サンプリングプロセスを繰り返すと得られる説明が異なってしまう場合があります。 不安定さは説明が信頼しがたいものであることを意味しており、これは致命的です。 結論として、具体的な実装として LIME を使用したローカルサロゲートモデルは非常に将来有望です。 しかし、この方法はまだ発展途上段階であり、安全に使用するには多くの問題を解決する必要があります。 Ribeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. &quot;Why should I trust you?: Explaining the predictions of any classifier.&quot; Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining. ACM (2016).↩ Alvarez-Melis, David, and Tommi S. Jaakkola. &quot;On the robustness of interpretability methods.&quot; arXiv preprint arXiv:1806.08049 (2018).↩ "],["anchors.html", "5.8 Scoped Rules (Anchors)", " 5.8 Scoped Rules (Anchors) 著者: Tobias Goerke &amp; Magdalena Lang This chapter is currently only available in this web version. ebook and print will follow. Anchor は、予測を &quot;固定&quot; するのに十分な決定規則を見つけることにより、ブラックボックスの分類モデルの個々の予測結果を説明します。 他の特徴量の値が変わっても予測に影響がない場合、ルールにより予測は固定されます。 Anchor は、強化学習とグラフ探索アルゴリズムを組み合わせて、モデルの呼び出し回数を最小限に抑えながら（これにより実行時間も抑えられます）、局所最適解に陥るのを回避できます。 このアルゴリズムは、2018年に Ribeiro、Singh、Guestrinら38により提案されました (LIMEアルゴリズムを導入したのも彼らです)。 LIME と同様に、Anchor は、ブラックボックスな機械学習モデルの予測に対して 局所的な 説明をするためにデータに 摂動 を与える方法を採用しています。 ただし、LIME が説明のためにサロゲートモデルを用いるのに対し、Anchor では Anchor と呼ばれる、より理解しやすい IF-THEN ルールが用いられます。 これらのルールはスコープ化されているので再利用が可能です。つまり、Anchor はカバレッジの概念が含まれており、それが他のインスタンス、もしくはまだ見ぬインスタンスに適用されるかを正確に示しています。 Anchor を見つけることは、強化学習の分野に由来する探索または多腕バンディット問題を伴います。 そのため、説明される各インスタンスに対して、近傍データ（摂動）を作成し、評価する必要があります。 そうすることで、ブラックボックスの構造とその内部パラメータを無視するアプローチが可能になり、これらが参照されず、変更されないままになります。 したがって、このアルゴリズムは モデル非依存 であり、任意の クラスのモデルに適用できます。 彼らの論文では、LIME と Anchor の両方のアルゴリズムを比較し、結果を導出するためにインスタンスの近傍を参照する方法がどのように異なるかを視覚化しています。 次の図は、2つのインスタンスを使用して、LIME と Anchor の両方が複雑な2値分類モデル（- または + のいずれかを予測）を局所的に説明している様子を示しています。 LIMEは、摂動空間 \\(D\\) が与えられたモデルを局所的に最もよく近似する線形決定境界を学習するだけなので、LIMEの結果がどれだけ忠実であるかを示すものではありません。 同じ摂動空間を与えられると、Anchor は、モデルの振る舞いに適応したカバレッジを持つ説明を構築し、その境界を明確に表現します。 したがって、Anchor は設計上忠実であり、どのインスタンスに対して有効であるかを正確に示します。 この特性により、Anchor は特に直感的で理解しやすくなります。 FIGURE 5.36: LIME vs. Anchor -- 簡単な可視化。図はRibeiro, Singh, and Guestrin (2018)より 前述したように、アルゴリズムの結果や説明は、Anchor と呼ばれるルールの形で得られます。 例えば、タイタニック号の事故で乗客が生き残るかどうかを予測する二変量のブラックボックスモデルが与えられたとします。 ここで私たちは、モデルがある特定の個人の生存を予測している 理由 を知りたいと思います。 Anchor アルゴリズムは、以下に示すような説明を提供します。 表 ある個人とモデルの予測の例。 Feature Value Age 20 Sex female Class first TicketPrice 300$ More attributes ... Survived true そして、対応するAnchorsの説明は以下のようになります。 IF SEX = female AND Class = first THEN PREDICT Survived = true WITH PRECISION 97% AND COVERAGE 15% この例は、Anchor がモデルの予測についての本質的な洞察とその基礎となる根拠を提供できることを示しています。 結果は、モデルがどの属性を考慮に入れたかを示しており、この例では女性でありファーストクラスであることが考慮されていました。 人は正確さを最優先するため、このルールを使ってモデルの振る舞いを検証できます。 Anchor によると、このルールは摂動空間のインスタンスの 15% に適用できます。 その場合の説明は 97％ の確率で正確であり、表示された述部が予測された結果のほぼ唯一の原因であることを意味しています。 Anchor \\(A\\) は正式には次のように定義されます。 \\[\\mathbb{E}_{\\mathcal{D}_x(z|A)}[1_{f(x)=f(z)}]\\geq\\tau,A(x)=1\\] \\(x\\) は説明されるインスタンス（例えば、表形式のデータセットの1行）を表します。 \\(A\\) は述部の集合、すなわち結果のルールまたは Anchor を示します。すべての \\(x\\) の特徴量の値が \\(A\\) によって定義された特徴量の述部に対応するとき、\\(A(x)=1\\) となります。 \\(f\\) は、説明する分類モデル（例えば、ニューラルネットワークモデル）を表します。これは、\\(x\\) とその摂動に対するラベルを予測できる必要があります。 \\(D_x (\\cdot|A)\\) は、\\(A\\) に一致する \\(x\\) の近傍データの分布を示しています。 \\(0 \\leq \\tau \\leq 1\\) は精度の閾値を指定します。少なくとも \\(\\tau\\) 以上の局所的な忠実性 (local fidelity)を達成したルールのみが有効な結果とみなされます。 形式的な説明は威圧的かもしれないので、言葉にしてみましょう。 説明されるインスタンス \\(x\\) が与えられると、\\(x\\) に適用されるようなルールまたは Anchor \\(A\\) を見つけます。このとき、同じ \\(A\\) を適用できる \\(x\\) の近傍データのうち、少なくとも \\(\\tau\\) の割合 については \\(x\\) と同じクラスであると予測されるようにします。ルールの精度は、与えられた機械学習モデル（指標関数 \\(1_{f(x) = f(z)}\\) で示される）を用いて、近傍データまたは摂動（\\(D_x (z|A)\\) に続く）を評価することで得られます。 5.8.1 Anchor の発見 Anchor の数学的な説明は明確で分かりやすいように見えますが、特定のルールを作成することは実行不可能です。 すべての連続空間や大規模な入力空間では不可能な \\(z \\in \\mathcal{D}_x(\\cdot|A)\\) で \\(1_{f(x) = f(z)}\\) を評価する必要があります。 それゆえ、著者らは \\(0 \\leq \\delta \\leq 1\\) のパラメータを使用して、確率的な定義をします。 このようにして、サンプルは統計的に信頼のある精度が出るまで引き出されます。 確率的な定義は以下のようになります。 \\[P(prec(A)\\geq\\tau)\\geq{}1-\\delta\\quad\\textrm{with}\\quad{}prec(A)=\\mathbb{E}_{\\mathcal{D}_x(z|A)}[1_{f(x)=f(z)}]\\] 前述の2つの概念はカバレッジの概念により結合・拡張されます。 その理論的根拠はモデルの入力空間の大部分に適用できるルールを見つけることから構成されます。 カバレッジは近傍(摂動空間)に適応できる Anchor の確率として定義されます。 \\[cov(A)=\\mathbb{E}_{\\mathcal{D}_{(z)}[A(z)]}\\] この要素を含めるとカバレッジ最大化を考慮できる Anchor の最終的な定義が可能となります。 \\[\\underset{A\\:\\textrm{s.t.}\\;P(prec(A)\\geq\\tau)\\geq{}1-\\delta}{\\textrm{max}}cov(A)\\] したがって、この手続きではすべての適格なルール（確率的な定義の閾値を満たす全てのルール）の中で最大のカバレッジをもつルールを目指しています。 モデルの大部分を説明するために、これらのルールはより重要なものと考えられます。 ただし、より多くのルールを持つ述部は、ルールが少ないときと比べて精度が高くなることに注意してください。 特に、\\(x\\) のそれぞれの特徴量を固定するようなルールは、評価する近傍を同一のインスタンスのみに減らします。 それゆえ、モデルは全ての近傍を同様に分類し、規則の精度は \\(1\\) になります。 同時に、多くの特徴を固定するルールは、特定のしすぎであり、少数のインスタンスにしか適用できません。 それゆえ、 精度とカバレッジにはトレードオフ があります。 Anchor のアプローチは説明を見つけるため、下図のように、4つの要素を使用します。 候補の生成: 説明の候補を生成します。 最初の段階では、\\(x\\) のそれぞれの特徴量に対して1つの候補が作成され、可能な摂動の値でそれぞれの値を固定します。 その他の段階では、前の段階での最良な候補に、まだ含まれていない述語を1つ追加することで拡張されます。 最良の候補の特定: 候補となるルールのうち、どのルールが \\(x\\) を最も説明できているか比較できる必要があります。 この目的を達成するために、現在観測されているルールに合う摂動が作成され、モデルを呼び出すことで評価されます。 しかしながら、これらの呼び出しは計算負荷を少なくするために最小限にする必要があります。 そのため、この要素の中核には純粋な探索問題 Multi-Armed-Bandit（MAB; 正確には KL-LUCB39）があります。 MAB は逐次選択を使用して、様々な戦略 (スロットマシーンに倣ってアームと呼ばれる) を効率的に探索するために使用されます。 与えられたルールの中では、それぞれの候補となるルールはアームに見立てられます。 アームが引っ張られるたびに、それぞれの近傍が評価され、候補のルールの報酬についてより多くの情報（Anchor の場合の精度）を得ることができます。 精度はルールがどの程度インスタンスを説明できているかを示します。 候補の精度検証: 候補の精度が閾値 \\(\\tau\\) を超えているかどうか統計的に信頼性がない時はさらに多くのサンプルを取ります。 修正されたビームサーチ: 上記全ての要素はグラフ探索アルゴリズムで幅優先アルゴリズムの派生であるビームサーチによって組み立てられます。 ビームサーチは各ラウンドの \\(B\\) 個の最良の候補を次のラウンドに運びます (\\(B\\) は ビーム幅 と呼ばれます)。 これらの \\(B\\) 個の最良のルールは次の新しいルールを作るために使用されます。 ビームサーチは各特徴量が含まれるのは1回だけなので最大で \\(featureCount(x)\\) 回行います。 それゆえ、それぞれのラウンド \\(i\\) で、それぞれ \\(i\\) 個の述語をもつ候補を生成してその中で最良の \\(B\\) 個を選択します。 したがって、\\(B\\) を高く設定するとアルゴリズムは局所最適解を回避しやすくなります。 一方で、モデルの呼び出し回数は多くなるため、計算負荷が増加します。 FIGURE 5.37: Anchor アルゴリズムの構成要素とその相互関係（簡易版） このアプローチは一見すると、どのようなシステムでもインスタンスを分類した理由について、統計的に健全な情報を効率的に導き出すことができる完璧なレシピのように見えます。 このアプローチは、モデルの入力を機械的に実験し、それぞれの出力を観察することで結論づけます。 モデルの呼び出し回数を削減するために、研究・確立された機械学習の手法（MAB）に依存します。 これによりアルゴリズムの時間を短縮できます。 5.8.2 複雑性と実行時間 Anchor アプローチの漸近的な実行時間のふるまいを知ることは、特定の問題に対してどのくらい良いパフォーマンスが期待されるかを評価することに役立ちます。 \\(B\\) はビーム幅、\\(p\\) は特徴量の数とします。 すると、Anchor アルゴリズムは次のようになります。 \\[\\mathcal{O}(B\\cdot{}p^2+p^2\\cdot\\mathcal{O}_{\\textrm{MAB}\\lbrack{}B\\cdot{}p,B\\rbrack})\\] この境界は統計的な信頼度 \\(\\delta\\) のような、問題に依存しないハイパーパラメータを抽象化したものです。 ハイパーパラメータを無視することで、境界の複雑さを軽減できます (詳細は元の論文を参照してください)。 MABは各ラウンドで \\(B \\cdot p\\) の中から最良の候補 \\(B\\) を取り出すので、ほとんどのMABとその実行時間は、他のパラメータに係数 \\(p^2\\) がかけられます。 ですからこれは明らかなことですが、特徴量が大きくなると、アルゴリズムの効率は低下します。 5.8.3 表形式データの例 表形式のデータは表によってあらわされた構造化データで、列は特徴量を表し、行はインスタンスを表しています。 Anchor アプローチのポテンシャルを示すために、例として、レンタル自転車のデータを使って、選択されたインスタンスの機械学習の予測の説明をしてみましょう。 ここでは、回帰ではなく分類問題として扱い、ブラックボックスモデルとしてランダムフォレストで学習しています。 レンタル自転車の数がトレンド曲線よりも上にあるか下にあるかを分類しています。 Anchor による説明を生成する前に、摂動関数を定義する必要があります。 簡単な方法としては、例えば学習データからサンプリングして作られた直感的な摂動空間を用いることです。 インスタンスを摂動させるとき、このデフォルトのアプローチでは、Anchor の述語の対象となる特徴量の値を維持し、固定されていない特徴量を、特定の確率でランダムにサンプリングされた別のインスタンスの値で置き換えます。 このプロセスでは、説明されるインスタンスと似ているが、他のランダムなインスタンスの値もいくつかもつような新たなインスタンスを生成します。 このようなインスタンスは、近傍と言えます。 FIGURE 5.38: レンタル自転車のデータセットの6つのインスタンスを説明する Anchor。各列は1つの説明もしくはAnchor、各バーはそれに含まれた特徴量の述部を表している。x軸はルールの精度を表し、バーの厚みは範囲に対応している。'base' ルールは述部を含んでない。これらの Anchor によってモデルは予測の際に主に温度を考慮に入れていることが示されている。 この結果は直感的に解釈できるもので、モデルの予測に関してどの特徴量が最も重要であるかがそれぞれのインスタンスについて示されています。 さらに、Anchorは少ない述部しか持たないので、広い範囲をカバーし他のケースにも適用できます。 上に示されるルールは \\(\\tau = 0.9\\) で作成されました。 つまり、評価された摂動が \\(90\\%\\) の精度でラベルを忠実にサポートするような Anchor が求められます。 また、量的特徴量の表現力や適応性を上げるために離散化も行っています。 これまでのすべてのルールは、モデルが少ない特徴量に基づいて自信を持って決定できるインスタンスに対して 作られました。 しかし、他のインスタンスはより多くの特徴量が重要であるため、モデルによって明確に分類されていません。 そのような場合には、Anchor はより具体的になり、より多くの特徴量を持ち、より少ないインスタンスにのみ適用されます。 FIGURE 5.39: 決定境界周辺のインスタンスを説明するには、より多くの特徴量の述部とより低いカバレッジからなる特殊なルールが必要。空のルール、すなわち基本特徴量は重要性が低くなる。これは、インスタンスが不安定な近傍の中に位置するので、決定境界のシグナルとして解釈できる。 デフォルトの摂動空間を選ぶことは適した選択ですが、一方でアルゴリズムに大きな影響を与え、結果が偏ってしまう可能性もあります。 例えば、学習データが不均衡である（それぞれのクラスのインスタンスの数が均等でない）とき、摂動空間も同様になります。 さらにこの状況はルールの発見や結果の精度にも影響します。 子宮頸がんのデータセットは Anchor アルゴリズムが以下のような状況につながる典型的な例です。 healthy とラベル付けされたインスタンスはすべての近傍が healthy と評価されるので空のルールが作り出されます。 摂動空間はほとんどが healthy データの値でカバーされているため、cancer とラベル付けされたインスタンスに対する説明は過度に具体的であり、述部は多くの特徴量を持っています。 FIGURE 5.40: 不均衡な摂動空間中で Anchor を構築すると、予期しない結果となる。 この結果は、求めていたものとは異なるでしょうが、いくつかの方法でアプローチできます。 例えば、不均衡なデータセットや正規分布から異なるサンプリング方法で得られる別の摂動空間を定義できます。 しかし、これには副作用があります。それは、サンプリングされた近傍は代表的なものでなく、カバレッジのスコープが変わってしまいます。 あるいは、MAB の信頼度 \\(\\delta\\) と誤差パラメータの値 \\(\\epsilon\\) を修正するという方法もあります。 これによって MAB はより多くのサンプルを引き出し、最終的にマイノリティのサンプルが、よりサンプリングされやすくなります。 例えば、多くのケースが cancer とラベル付けされているように子宮頸がんデータの一部のみを使用します。 そして、そこから対応する摂動空間を作るようなフレームワークを使用します。 摂動は、予測を変化させる可能性が高くなり、Anchor アルゴリズムは重要な特徴量を特定できるようになります。 しかし、カバレッジの定義を考慮に入れる必要があり、カバレッジは摂動空間の中でのみ定義されています。 前の例では、摂動空間の基底として学習データを用いました。 ここではデータセットの一部を用いているので、カバレッジが高いことは必ずしもグローバルにルールの重要性が高いことを示しているわけではありません。 FIGURE 5.41: Anchor を構築する前にデータセットを均一にすることで、マイノリティのケースにおけるモデルの判断根拠がわかる。 5.8.4 長所 Anchor のアプローチは、LIME に比べて複数の長所があります。 まず第一に、ルールが 解釈しやすい ため（素人であっても）、アルゴリズムの出力を容易に理解できます。 さらに、Anchor はサブセット化可能 であり、カバレッジの概念を含めることで重要度の尺度を示します。 第二に、Anchor のアプローチは入力の近傍において モデルによる予測が非線形や複雑な場合でも機能します。 このアプローチはサロゲートモデルをフィッティングする代わりに強化学習の手法を使用するため、モデルのアンダーフィットは起こりにくいです。 それとは別に、アルゴリズムがモデルに依存しないため、どのモデルに対しても適用できます。 さらに、アルゴリズムはバッチサンプリングをサポートしているMAB（BatchSARなど）を利用することで並列化できるので、非常に効率的です。 5.8.5 短所 このアルゴリズムは、ほとんどの摂動ベースの説明手法と同様に、高度に変更可能で影響力のある設定に悩まされます。 意味のある結果を得るために、ビーム幅や精度の閾値などのハイパーパラメータを調整する必要があるだけでなく、摂動関数を1つのドメイン／ユースケース用に設計する必要もあります。 表形式のデータをどのように摂動させるか考えてみてください。 同様の摂動を画像データに適用するにはどうしたらいいでしょうか（ヒント: 適用できません）。 幸いなことに、一部のドメイン（表形式など）ではデフォルトのアプローチが使用され、説明の初期設定が楽になります。 また、多くの場合では離散化が必要です。 そうしないと、結果が具体的すぎたり、カバレッジが低すぎたり、モデルの理解に貢献しないものになってしまうからです。 離散化は役立つ場合もありますが、不用意に使用すると決定境界が曖昧になる可能性があり、全く逆の効果をもたらしてしまいます。 最良の離散化手法は存在しないため、悪い結果が得られないように、ユーザはデータの離散化手法を決定する前にデータを把握する必要があります。 Anchor の構築には、全ての摂動ベースの説明手法と同様に、機械学習モデルを何度も呼び出す 必要があります。 アルゴリズムは呼び出し回数を最小限に抑えるために MAB を使用していますが、それでも実行時間はモデルのパフォーマンスに大きく依存するため非常に変わりやすいです。 最後に、一部のドメインではカバレッジの概念が定義されていません。 例えば、ある画像のスーパーピクセルと他の画像のスーパーピクセルをどのように比較するのか、その明確なまたは普遍的な定義はありません。 5.8.6 ソフトウェアと代替手法 現在、Python パッケージの anchor（Alibiにも統合されています）と Java implementation の2つの実装が利用可能です。 前者は Anchor アルゴリズムの著者のリファレンスで、後者は高い性能を示す R のインターフェイスを持つ実装であり、anchorsと呼ばれています。 この章の例ではこれを用いました。 今のところ、Anchor は表形式のデータのみをサポートしています。 ただし、理論的には任意のドメインまたはデータのタイプに対しても使うことができます。 Marco Tulio Ribeiro, Sameer Singh, Carlos Guestrin. &quot;Anchors: High-Precision Model-Agnostic Explanations.&quot; AAAI Conference on Artificial Intelligence (AAAI), 2018↩ Emilie Kaufmann and Shivaram Kalyanakrishnan. “Information Complexity in Bandit Subset Selection”. Proceedings of Machine Learning Research (2013).↩ "],["shapley.html", "5.9 シャープレイ値 (Shapley Values)", " 5.9 シャープレイ値 (Shapley Values) 予測は、インスタンスの特徴量の値が &quot;プレイヤー&quot;で、予測が報酬であるようなゲームを想定して説明できます。 シャープレイ値（協力ゲーム理論の手法）は、特徴量の間で &quot;報酬&quot; を公平に分配する方法を教えてくれます。 5.9.1 一般的なアイデア 次のシナリオを想定してみましょう。 あなたは、アパートの価格を予測するための機械学習モデルを学習しました。 あるアパートは €300,000 と予測されており、この予測を説明する必要があります。 そのアパートの広さは 50m2 で、二階にあり、近くに駐車場があり、猫を飼うことは禁止されています。 FIGURE 5.42: 駐車場が近く猫が禁止されている 50m2 の二階のアパートの予測価格は €300,000。目的は、これらの特徴量がそれぞれどのように予測値に寄与したのかを説明すること。 全アパートの平均予測値は €310,000 です。 平均予測値と比較して、それぞれの特徴量の値は予測にどの程度寄与しているでしょうか。 線形回帰モデルの場合、答えは簡単です。 各特徴量の効果は、特徴量の重みと値を掛け合わせたものだからです。 ただしこれは、モデルが線形なので上手くいっているだけです。 より複雑なモデルに対しては、異なる解決策が必要です。 例えば、LIME は、効果を見積もるために局所的なモデルを提案します。 もう1つの解決策は、協力ゲーム理論に由来します。 シャープレイ値は、Shapley (1953)40によって提案された方法であり、総報酬額に対する貢献度に応じて参加者に支払額を割り当てる方法です。 参加者は、連合とよばれるグループ内で互いに協力し、この協力を通じて一定の利益を受け取ります。 参加者、ゲーム、報酬、これらは、機械学習の予測や解釈可能性に対して、一体何の関係があるのでしょうか。 &quot;ゲーム&quot; とは、データセットの中の1つのインスタンスに対する予測タスクを意味します。 &quot;報酬&quot; は、このインスタンスの実際の予測から全てのインスタンスの予測平均を引いたものです。 &quot;プレイヤー&quot; とは、報酬を受け取るために協力するインスタンスの各特徴量の値です。 アパートの例では、park-nearby、cat-banned、area-50 そして floor-2nd という特徴量の値が互いに連携することで €300,000 の予測を達成したということになります。 ここで我々の目標は、実際の予測 (€300,000) と平均予測 (€310,000) の違い、つまり -€10,000 の違いを説明することです。 この答えとしては次のようなものが考えられます。 park-nearby は €30,000 の寄与、size-50 は €10,000 の寄与、floor-2nd は €0 の寄与、cat-banned は -€50,000 の寄与。 寄与の合計は -€10,000 になり、最終予測から平均予測のアパート価格を差し引いたものになります。 １つの特徴量のシャープレイ値をどのように計算するのでしょうか。 シャープレイ値は、考えうる全ての組み合わせ（連合）にわたる平均周辺寄与 (average marginal contribution) です。 これで理解できるしょうか。 次の図では、cat-banned を park-nearby と size-50 の連合に追加するときの寄与を評価します。 データから別のアパートをランダム抽出してその値を基準にすることで、park-nearby、cat-banned および size-50 のみが連合をなしている場合をシミュレートします。 floor-2nd がランダム抽出された floor-1st に置き換えられたとします。 そしてこの組み合わせでアパート価格を予測した結果は €310,000 です。 次のステップでは、cat-banned を連合から取り除いて、それをランダム抽出したアパートから、猫の禁止/許可という値と置き換えます。 この例では cat-allowed ですが、cat-banned が再び選ばれるかもしれません。 park-nearby と size-50 の連合のアパート価格を予測すると€320,000です。 その結果、cat-banned の寄与は €310,000 - €320,000 = -€10.000 となりました。 この見積は、ランダム抽出されたアパート (猫と階の特徴量の値を提供した &quot;ドナー&quot; ) の価格に依存しています。 そのため、サンプリングを繰り返して、寄与の平均を計算することで、よりよい推定を行ます。 FIGURE 5.43: park-nearby と area-50 の連合に cat-banned を追加する場合の予測に対する寄与を推定するための反復 全ての可能な連合に対して、この計算を繰り返します。 シャープレイ値は考えうる全ての連合に対する全ての周辺寄与の平均です。 計算時間は、特徴量の数とともに指数関数的に増加します。 計算時間を現実的にするための1つの解決策は、連合の少数のサンプルから寄与を計算することです。 次の図は、cat-banned のシャープレイ値を決定するために必要となる特徴量の全ての連合を示しています。 最初の行は、特徴量の値がない連合です。 2行目、3行目、4行目は、&quot;|&quot; で区切られた、サイズが増加した様々な連合です。 全体としては、以下の連合が可能です。 No feature values park-nearby size-50 floor-2nd park-nearby+size-50 park-nearby+floor-2nd size-50+floor-2nd park-nearby+size-50+floor-2nd. これらの連合のそれぞれについて、cat-banned という特徴量の値がある場合とない場合のアパート価格を予測して、その差を計算することで周辺寄与を求めます。 シャープレイ値は、周辺寄与の（加重）平均です。 連合に入っていない特徴量の値を、アパートデータセットから取得したランダムな特徴量の値と置き換えて、機械学習モデルで予測値を計算します。 FIGURE 5.44: cat-banned の正確なシャープレイ値を計算するのに必要な8つの連合すべて。 全ての特徴量値に対してシャープレイ値を推定すると、特徴量間の予測の完全な分布（平均を差し引いたもの）が得られます。 5.9.2 例と解釈 特徴量 j のシャープレイ値の解釈は次のようになります。 j 番目の特徴量の値は、データセットの平均予測と比較して、この特定のインスタンスの予測に \\(\\phi_j\\) 寄与しました。 シャープレイ値は分類（確率を扱う場合）と回帰の両方に有効です。 シャープレイ値を使ってランダムフォレストモデルを使用した子宮頸がんの予測を分析します。 FIGURE 5.45: 子宮頚がんデータセットのシャープレイ値。0.57の予測では、この女性の癌の確率が平均予測0.03より、0.54だけ高いことを示している。STDsと診断された回数が多いほど確率がもっとも増加する。寄与度の合計は実際の予測と平均予測の差 (0.54) を示す。 自転車レンタルデータセットに対しても、与えられた天気やカレンダー情報から一日のレンタル数を予測するランダムフォレストを学習させます。 そして、特定の日のランダムフォレストの予測を説明します。 FIGURE 5.46: 285日目のシャープレイ値。レンタル自転車の予測は 2409 となり、この日は平均予測 4518 より -2108 だけ低い。天候と湿度が最も大きい負の寄与となり、気温は正の寄与があった。シャープレイ値の合計は実際の予測値と予測平均の差 (-2108) になっている。 シャープレイ値を正しく解釈するために、注意すべきことがあります。 シャープレイ値は異なる連合の中で、特徴量の値が予測に与える平均寄与であり、モデルから特徴量を削除したときの予測の差ではありません。 5.9.3 シャープレイ値の詳細 この章では好奇心旺盛な読者のためにシャープレイ値の定義と計算について深掘りします。 もし、技術的な詳細に興味がなければこの章をスキップして、&quot;長所と短所&quot; に飛んでください。 各特徴量がデータ点の予測にどれだけ影響があるかに興味があります。 線形モデルでは各々の効果を計算するのは簡単です。 1つのデータインスタンスに対する線形モデルの予測は次のようになります。 \\[\\hat{f}(x)=\\beta_0+\\beta_{1}x_{1}+\\ldots+\\beta_{p}x_{p}\\] x を寄与を計算したいインスタンスとします。 \\(x_j\\) (j = 1, ..., p) はそれぞれ特徴量 j の値とします。 \\(\\beta_j\\) は特徴量 j に対応する重みです。 j 番目特徴量の予測 \\(\\hat{f}(x)\\) に対する寄与 \\(\\phi_j\\) は次のようになります。 \\[\\phi_j(\\hat{f})=\\beta_{j}x_j-E(\\beta_{j}X_{j})=\\beta_{j}x_j-\\beta_{j}E(X_{j})\\] ただし、\\(E(\\beta_jX_{j})\\) は特徴量 j の平均効果です。 寄与度は特徴量の効果から平均効果を引いた値です。 すばらしい！ これで各特徴量がどれだけ予測に影響を及ぼしたかが分かります。 1つのインスタンスに対するすべての特徴量の寄与度を合計すると、結果は以下のようになります。 \\[\\begin{align*}\\sum_{j=1}^{p}\\phi_j(\\hat{f})=&amp;\\sum_{j=1}^p(\\beta_{j}x_j-E(\\beta_{j}X_{j}))\\\\=&amp;(\\beta_0+\\sum_{j=1}^p\\beta_{j}x_j)-(\\beta_0+\\sum_{j=1}^{p}E(\\beta_{j}X_{j}))\\\\=&amp;\\hat{f}(x)-E(\\hat{f}(X))\\end{align*}\\] これはデータ点 x の予測値から平均予測値を引いたものです。 特徴量の寄与度は負になることもあります。 どんなモデルに対しても同じようにいくでしょうか？ モデルに依存しないツールとしてこれがあるといいですね。 他のタイプのモデルは同様の重みを持たないことから、異なる解決策が必要です。 助けは意外なところからやってきます。 それこそが、協力ゲーム理論に由来するシャープレイ値です。 シャープレイ値は任意の機械学習モデルの単一の予測に対する特徴量の寄与度を計算する方法です。 5.9.3.1 シャープレイ値 シャープレイ値は S の中のプレイヤーの値の関数 val と定義されます。 特徴量の値のシャープレイ値は報酬への寄与度であり、特徴量の値の全ての組み合わせに対して、加重和をとったのものです。 \\[\\phi_j(val)=\\sum_{S\\subseteq\\{x_{1},\\ldots,x_{p}\\}\\setminus\\{x_j\\}}\\frac{|S|!\\left(p-|S|-1\\right)!}{p!}\\left(val\\left(S\\cup\\{x_j\\}\\right)-val(S)\\right)\\] ただし、S はモデルで使用されている特徴量の部分集合、x は説明されるインスタンスの特徴量ベクトル、p は特徴量の数を表します。 \\(val_x(S)\\) は集合 S に含まれていない特徴量で周辺化された集合 S の特徴量の値に対する予測です。 \\[val_{x}(S)=\\int\\hat{f}(x_{1},\\ldots,x_{p})d\\mathbb{P}_{x\\notin{}S}-E_X(\\hat{f}(X))\\] 実際には、S に含まれていないそれぞれの特徴量に対して多重積分します。 具体例として、4つの特徴量 x1、x2、x3、x4 に対する機械学習モデルがあったときに、特徴量 x1 と x3 からなる連合 S の予測値を評価します。 \\[val_{x}(S)=val_{x}(\\{x_{1},x_{3}\\})=\\int_{\\mathbb{R}}\\int_{\\mathbb{R}}\\hat{f}(x_{1},X_{2},x_{3},X_{4})d\\mathbb{P}_{X_2X_4}-E_X(\\hat{f}(X))\\] これは線形モデルの特徴量の寄与度に似ています。 &quot;値&quot; という言葉の多用に惑わされないでください。 特徴量の値は、インスタンスや特徴の数値(numerical or categorical)です。 シャープレイ値は予測に対する特徴量の寄与度です。 値関数 (value function) は、プレイヤー（特徴量の値）の連合に対する報酬関数です。 シャープレイ値は効率性、対称性、ダミー、加法性を満たす唯一の方法で、これらを組み合わせることにより、公平な報酬の定義を考えることができます。 効率性: 特徴量の寄与度の合計は x の予測と平均との差である必要があります。 \\[\\sum\\nolimits_{j=1}^p\\phi_j=\\hat{f}(x)-E_X(\\hat{f}(X))\\] 対称性: 2つの特徴量の値 j と k の寄与度は、全ての連合に等しく寄与する場合、同じである必要があります。 すなわち、全ての \\[S\\subseteq\\{x_{1},\\ldots,x_{p}\\}\\setminus\\{x_j,x_k\\}\\] について、\\[val(S\\cup\\{x_j\\})=val(S\\cup\\{x_k\\})\\] ならば、 \\[\\phi_j=\\phi_{k}\\] が成立します。 ダミー: 予測値に影響のない特徴量 j は、追加される連合に関係なく、シャープレイ値が 0 になる必要があります。 すなわち、すべての \\[S\\subseteq\\{x_{1},\\ldots,x_{p}\\}\\] について \\[val(S\\cup\\{x_j\\})=val(S)\\] ならば、\\[\\phi_j=0\\] が成立します。 加法性: 結合した報酬 val+val+ をもつゲームに対して、それぞれのシャープレイ値は次のようになります。 \\[\\phi_j+\\phi_j^{+}\\] ランダムフォレストを学習したとすると、予測は多くの決定木の平均となります。 加法性は各々の決定木のシャープレイ値を計算し、平均を取るとランダムフォレスト全体のシャープレイ値を得られることを保証します。 5.9.3.2 直感 シャープレイ値は次のように直感的に理解できます。 特徴量がランダムな順序で部屋に入ります。 部屋にいる全ての特徴量はゲームに参加します（= 予測に寄与します）。 ある特徴量のシャープレイ値とは、既に部屋にいた特徴量が受ける予測と、その特徴量が加わった時の予測の変化の平均です。 5.9.3.3 シャープレイ値の推定 シャープレイ値を正確に計算するには、特徴量のすべての可能な連合（集合）を \\(j\\) 番目の特徴量がある場合とない場合とで評価する必要があります。 可能な連合の数は特徴量が追加されると指数的に増加するため、この問題の正確な解を得るのは困難になります。 Strumbelj et al. (2014)41 はモンテカルロサンプリングによる近似を提案しています。 \\[\\hat{\\phi}_{j}=\\frac{1}{M}\\sum_{m=1}^M\\left(\\hat{f}(x^{m}_{+j})-\\hat{f}(x^{m}_{-j})\\right)\\] ここで、\\(\\hat{f}(x^{m}_{+j})\\) は x に対する予測ですが、\\(j\\) 番目の特徴量の各値を除いて、ランダムな数の特徴量がランダムなデータ点 \\(z\\) からの特徴量に置き換えられています。 ベクトル \\(x^{m}_{-j}\\) はほとんど \\(x^{m}_{+j}\\) と同じですが、\\(x_j^{m}\\) もサンプリングされた \\(z\\) から得られます。 これらの \\(M\\) 個の新しいインスタンスはそれぞれ、2つのインスタンスから組み立てられた一種の「フランケンシュタインの怪物」です。 1つの特徴量値に対するシャープレイ値の概算 出力: j 番目の特徴量の値に対するシャープレイ値 入力: 反復回数 M、関心のあるインスタンス x、特徴量のインデックス j、データ行列 X、および機械学習モデル f すべての m = 1,...,M について データ行列 X からランダムなインスタンス z を抽出する 特徴量のランダムな並べ替え o を選択する o に従ってインスタンス x を並べる: \\(x_o=(x_{(1)},\\ldots,x_{(j)},\\ldots,x_{(p)})\\) o に従ってインスタンス z を並べる: \\(z_o=(z_{(1)},\\ldots,z_{(j)},\\ldots,z_{(p)})\\) 2つの新しいインスタンスを構築する j 番目の特徴量がある場合: \\(x_{+j}=(x_{(1)},\\ldots,x_{(j-1)},x_{(j)},z_{(j+1)},\\ldots,z_{(p)})\\) j 番目の特徴量がない場合: \\(x_{-j}=(x_{(1)},\\ldots,x_{(j-1)},z_{(j)},z_{(j+1)},\\ldots,z_{(p)})\\) 周辺寄与を算出する: \\(\\phi_j^{m}=\\hat{f}(x_{+j})-\\hat{f}(x_{-j})\\) 平均値としてシャープレイ値を計算する: \\(\\phi_j(x)=\\frac{1}{M}\\sum_{m=1}^M\\phi_j^{m}\\) まず、関心のあるインスタンス x、特徴量 j、反復回数 M を選択します。 繰り返しごとに、データからランダムなインスタンス z を選択し、特徴量のランダムな順序を生成します。 関心のあるインスタンス x とサンプル z の値を組み合わせて、2つの新しいインスタンスが生成されます。 インスタンス \\(x_{+j}\\) が関心のあるインスタンスですが、特徴量 j より前の順序の値はすべてサンプル z からの特徴量で置き換えられます。 インスタンス \\(x_{-j}\\) は \\(x_{+j}\\) と同じですが、さらに特徴量 j の値がサンプル z からの特徴量 j の値で置き換えられています。 そして次のようにブラックボックスからの予測値の差が計算されます。 \\[\\phi_j^{m}=\\hat{f}(x^m_{+j})-\\hat{f}(x^m_{-j})\\] これらの差をすべて平均すると、シャープレイ値が得られます。 \\[\\phi_j(x)=\\frac{1}{M}\\sum_{m=1}^M\\phi_j^{m}\\] 平均することによって、暗黙のうちにXの確率分布によってサンプルに重みづけされます。 すべての特徴量についてのシャープレイ値を得るため、この手順を各特徴量に対して繰り返す必要があります。 5.9.4 長所 予測と予測の平均の差分は、シャープレイ値の効率性という性質から、インスタンスの特徴量間で公平に分散されます。 この性質により、シャープレイ値は LIME などの他の手法と区別されます。 LIME は予測が特徴量間で公平に分散されることを保証しません。 EU の「説明を求める権利」のように、法が説明を求める状況下において、シャープレイ値は確かな理論に基づき、予測を公平に分配しているため、法に準拠した唯一の手法であるかもしれません。 ただし、筆者は法律家ではないので、これは法の要請に対する著者の直感でしかありません。 シャープレイ値は対照的な説明を可能にします。 予測をデータセット全体の予測の平均と比較する代わりに、データの部分集合や単一のデータとも比較できます。 この性質もまた、LIME のような局所モデルにはないものです。 シャープレイ値は確かな理論に基づいた唯一の説明手法です。 効率性、対称性、ダミー、加法性の原理により説明に合理的な根拠が与えられます。 LIME のような手法は機械学習モデルが局所的には線形に振る舞うことを仮定しますが、これがうまくいく理由についての理論はないのです。 特徴量によってプレイされるゲームとして予測を説明するというのは驚くべきことです。 5.9.5 短所 シャープレイ値は、多くの計算時間 を必要とします。 実世界の問題の99.9%では、近似解しか求めることができません。 シャープレイ値の厳密な計算は、特徴量の 2k の可能な連合があり、特徴量の &quot;不在&quot; はランダムなインスタンスを描くことでシミュレートしなければならず、シャープレイ値の推定値の分散が大きくなるため、計算量が多くなります。 連合の指数関数的な数は、連合をサンプリングし、反復回数 M を制限することで対処します。 M を減らすと計算時間は短縮されますが、シャープレイ値の分散が大きくなります。 反復回数 M には良い経験則はありません。 Mは、シャープレイ値を正確に推定するのに十分な大きさでなければなりませんが、妥当な時間で計算を完了するのに十分な小ささである必要もあります。 チェルノフ・バウンズに基づいて M を選択できますが、機械学習予測のためのシャープレイ値についてこれを行った論文を見たことはありません。 シャープレイ値は誤解される可能性があります。 特徴量のシャープレイ値は、モデル学習から特徴量を除去した後の予測値の差ではありません。 シャープレイ値は、「現在の特徴量の値の集合が与えられたときの、実際の予測値と平均予測値の差に対する特徴量の寄与度」であると解釈してください。 スパースな説明（特徴量をほとんど含まない説明）を求める場合、シャープレイ値は不適切な説明方法です。 シャープレイ値は常にすべての特徴を使う説明を作成します。 人はLIMEのような選択的な説明を好みます。 一般の人が扱わなければならない説明には、LIMEの方が適しているかもしれません。 もう1つの解決策として、Lundberg and Lee (2016)42が紹介しているSHAPがあります。 これはシャープレイ値をベースにしていますが、特徴量の少ない説明も可能です。 シャープレイ値は特徴量あたりの単純な値を返しますが、LIMEのような予測モデルはありません。 つまり、「年収が300ユーロ増えると、私のクレジットスコアは5ポイント上昇します。」というような入力の変化に対する予測の変化についての記述には使えません。 もう1つの欠点は、新しいインスタンスのシャープレイ値を計算したい場合、データへのアクセスが必要になることです。 対象のインスタンスの一部をランダムに選ばれたインスタンスからの値に置き換えるデータが必要なため、予測関数にアクセスできるだけでは十分ではありません。 これは、実際のインスタンスのように見えるがそうではないデータを、学習データから作成できる場合にのみ回避できます。 他の多くの並べ替えに基づく解釈法と同様に、シャープレイ値による手法も、特徴量に相関がある場合、非現実的なデータインスタンスが含まれているという問題があります。 連合から特徴値が欠落していることをシミュレートするために、特徴量を周辺化します。 これは、特徴量の周辺分布から値をサンプリングすることで達成されます。 特徴量が独立であれば問題ありませんが、特徴量が依存関係にある場合、このインスタンスに対して意味をなさない特徴量の値をサンプリングするかもしれません。 しかし、特徴量のシャープレイ値を計算するためにそれらの値を利用します。 これは、相関のある特徴量を一緒に変化させて、それらの特徴量のために1つの相互シャープレイ値を得ることで解決できるかもしれません。 もう1つの解決策は，条件付きサンプリングです。 特徴量は、すでに連合にある特徴量に条件をつけてサンプリングされます。 条件付きサンプリングにより、非現実的なデータ点の問題は修正できますが、新たな問題があります。 Sundararajan et.al. (2019)43によって発見され、さらにJanzing et.al. (2020)44によって議論されているように、結果として得られる値は対称性の公理に反しているので、我々のゲームにとってのシャープレイ値ではありません。 5.9.6 ソフトウェアと代替手法 R では、シャープレイ値は iml と fastshap のパッケージで実装されています。 シャープレイ値の代替推定手法である SHAP は次の章で紹介されています。 他には、breakDown と呼ばれる手法があり、Rのパッケージ breakDown45 で実装されています。 breakDown もまた、各特徴量の予測への寄与を示しますが、breakDown ではそれを段階的に計算します。 ゲームの例えを再び用いましょう。 空のチームからスタートし、予測に最も寄与している特徴量を追加し、これを全ての特徴量が追加されるまで繰り返します。 各特徴量がどのくらい予測に貢献するのかは、すでに「チーム」に属しているそれぞれの特徴量に依存しています。 これが breakDown メソッドの大きな欠点です。 breakDown はシャープレイ値法よりも高速であり、相互作用のないモデルの場合には同じ結果を与えます。 Shapley, Lloyd S. &quot;A value for n-person games.&quot; Contributions to the Theory of Games 2.28 (1953): 307-317.↩ Štrumbelj, Erik, and Igor Kononenko. &quot;Explaining prediction models and individual predictions with feature contributions.&quot; Knowledge and information systems 41.3 (2014): 647-665.↩ Lundberg, Scott M., and Su-In Lee. &quot;A unified approach to interpreting model predictions.&quot; Advances in Neural Information Processing Systems. 2017.↩ Sundararajan, Mukund, and Amir Najmi. &quot;The many Shapley values for model explanation.&quot; arXiv preprint arXiv:1908.08474 (2019).↩ Janzing, Dominik, Lenon Minorics, and Patrick Blöbaum. &quot;Feature relevance quantification in explainable AI: A causal problem.&quot; International Conference on Artificial Intelligence and Statistics. PMLR, 2020.↩ Staniak, Mateusz, and Przemyslaw Biecek. &quot;Explanations of model predictions with live and breakDown packages.&quot; arXiv preprint arXiv:1804.01955 (2018).↩ "],["shap.html", "5.10 SHAP (SHapley Additive exPlanations)", " 5.10 SHAP (SHapley Additive exPlanations) This chapter is currently only available in this web version. ebook and print will follow. Lundberg and Lee (2016)46による SHAP (SHapley Additive exPlanations)は、個々の予測を説明する手法です。 SHAP はゲーム理論的に最適な シャープレイ値 に基づいています。 SHAP が シャープレイ値 中の一節ではなく単独の章となっている理由は2つあります。 1つ目は、SHAP の作者らは KernelSHAP を提案したことです。これはローカルサロゲートモデル(local surrogate models) から着想を得たカーネルベースのシャープレイ値の代替的な推定手法です。 そして、 彼らはツリーベースのモデルに対する効率的な推定手法である TreeSHAP を提案しました。 2つ目は、SHAP にはシャープレイ値の集合に基づいた多くの大域的解釈モデルが付随することです。 この章では新たな推定手法と大域的な解釈の両方を説明します。 シャープレイ値 と 局所的モデル(LIME) の章を先に読んでおくことをおすすめします。 5.10.1 定義 SHAP の目標は、それぞれの特徴量の予測への貢献度を計算することで、あるインスタンス x に対する予測を説明することです。 SHAP による説明では、協力ゲーム理論によるシャープレイ値を計算します。 インスタンスの特徴量の値は、協力するプレイヤーの一員として振る舞います。 シャープレイ値は、&quot;報酬&quot; (=予測) を特徴量間で公平に分配するにはどうしたら良いかを教えてくれます。 各プレイヤーは、例えば表形式データでは、個別の特徴量の値となります。 プレイヤーは特徴量の値の組の可能性もあります。 画像を説明する例では、画素はスーパーピクセルとしてグループ化され、予測はそれらの間で分配されるでしょう。 SHAP が生んだ革新の1つは、シャープレイ値による説明が、線形モデルのような特徴量の効果の総和として表されることです。 この観点は、LIMEとシャープレイ値を結びつけます。 SHAPは、説明を次のように記述します。 \\[g(z&#39;)=\\phi_0+\\sum_{j=1}^M\\phi_jz_j&#39;\\] ここで、g は説明モデル、\\(z&#39;\\in\\{0,1\\}^M\\) は連合ベクトル、 M は連合サイズの最大値、そして \\(\\phi_j\\in\\mathbb{R}\\) は特徴量 j についての特徴量の属性であり、シャープレイ値です。 私が &quot;連合ベクトル&quot; と呼んでいるものは、SHAP の論文では &quot;simplified features&quot; と呼ばれています。 この名前が選ばれたのは、例えば画像データでは、画像は画素レベルではなく、スーパーピクセルに集約されたレベルで表されるためであると私は考えています。 私は z が連合を表すものであると考えるのは有用であると信じています。 連合ベクトルにおいて、要素が 1 のとき対応する特徴量が &quot;存在&quot; することを意味し、0 は &quot;不在&quot; であることを表します。 あなたがシャープレイ値について知っているならば、これは馴染み深く感じるでしょう。 シャープレイ値を計算するために、いくつかの特徴量だけがゲームに参加 (&quot;存在&quot;) し、その他は参加しない (&quot;不在&quot;) としてシミュレーションします。 連合の線形モデルとする表現は、\\(\\phi\\) を計算するためのトリックです。 興味があるインスタンス x に対し、連合ベクトル x' は全てが1であるベクトル、つまり、全ての特徴量が&quot;存在&quot;となります。 このとき、式は次のように単純になります。 \\[g(x&#39;)=\\phi_0+\\sum_{j=1}^M\\phi_j\\] この式はシャープレイ値での表記に似ています。 実際の推定については後ほど触れます。 推定の詳細の前に、まずは \\(\\phi\\) の性質について紹介します。 シャープレイ値は効率性 (Efficiency)、対称性 (Symmetry)、ダミー性 (Dummy)、加法性 (Additivity) を満たす唯一の解決案です。 シャープレイ値を計算するため、SHAP もこれらの性質を満たします。 SHAPの論文では、SHAPの性質とシャープレイ値の性質間の食い違いに気づくでしょう。 SHAPは次の望ましい3つの性質を説明します。 1) 局所正確性 (Local accuracy) \\[f(x)=g(x&#39;)=\\phi_0+\\sum_{j=1}^M\\phi_jx_j&#39;\\] もし \\(\\phi_0=E_X(\\hat{f}(x))\\) と定義し、全ての \\(x_j&#39;\\) を 1 とするとき、これは、シャープレイの効率性となります。 名前が異なり、連合ベクトルを使っているだけです。 \\[f(x)=\\phi_0+\\sum_{j=1}^M\\phi_jx_j&#39;=E_X(\\hat{f}(X))+\\sum_{j=1}^M\\phi_j\\] 2)欠損 (Missingness) \\[x_j&#39;=0\\Rightarrow\\phi_j=0\\] 欠損は、欠損している特徴量の属性がゼロになることを意味しています。 \\(x_j&#39;\\) は連合を指し、値が 0 のとき、特徴量が不在を示していることに注意してください。 連合の表記では、説明されるインスタンスの全ての特徴量 \\(x_j&#39;\\) は 1 である必要があります。 0 の存在は、注目しているインスタンスの特徴量の値が欠損していることを意味します。 この性質は、&quot;普通の&quot;シャープレイ値の性質には含まれていません。 では、なぜこれがSHAPのために必要なのでしょうか。 Lundberg は&quot;minor book-keeping property&quot; と呼んでいます。 欠損している特徴量は、理論的に、 \\(x_j&#39;=0\\) と掛け合わされるため、局所的な精度を損なうことなく、任意のシャープレイ値を持つ事ができます。 欠損の性質は、欠損している特徴量は 0 のシャープレイ値を持つようにします。 実際には、これは定数の特徴量にだけ関係します。 3) 一貫性 (Consistency) \\(f_x(z&#39;)=f(h_x(z&#39;))\\) と \\(z_{\\setminus{}j&#39;}\\) は \\(z_j&#39;=0\\) を表すとします。 任意の2つのモデル f と f' に対して、任意の入力 \\(z&#39;\\in\\{0,1\\}^M\\) が、\\[f_x&#39;(z&#39;)-f_x&#39;(z_{\\setminus{}j}&#39;)\\geq{}f_x(z&#39;)-f_x(z_{\\setminus{}j}&#39;)\\]を満たすならば、\\[\\phi_j(f&#39;,x)\\geq\\phi_j(f,x)\\]が成り立ちます。 一貫性は、もし特徴量の値の周辺寄与が（他の特徴量に関わらず）増加または同じままモデルが変化すると、シャープレイ値もまた、増加または同じままになるということを言っています。 Lundberg と Leeの付録で説明されている通り、一貫性から線形性、ダミー性、対称性が導かれます。 5.10.2 KernelSHAP KernelSHAP はインスタンス x の予測に対するそれぞれの特徴量の値の寄与を推定します。 KernelSHAP は以下の5つのステップで構成されています。 連合 \\(z_k&#39;\\in\\{0,1\\}^M,\\quad{}k\\in\\{1,\\ldots,K\\}\\) (1 = 連合内に特徴量が存在する, 0 =特徴量が存在しない) をサンプリングする。 最初に、\\(z_k&#39;\\) を元の特徴空間に変換し、モデル f: \\(f(h_x(z_k&#39;))\\) を適用しそれぞれの \\(z_k&#39;\\) を予測する。 SHAP カーネルを使って各 \\(z_k&#39;\\) の重みを計算する。 重み付きの線形モデルで学習する。 シャープレイ値 \\(\\phi_k\\) と 線形モデルの係数を返す。 0 と 1 の連鎖ができるまでコイントスを繰り返す事によってランダムな連合を作成できます。 例えば、(1,0,1,0) のベクトルは、1番目と3番目の特徴量の連合を意味します。 K 個のサンプルされた連合は線形モデルのためのデータセットになります。 回帰モデルの目標は連合に対する予測です。 このモデルは、バイナリの連合ベクトルに対して学習されていないため予測はできないと思うかもしれません。 特徴量の連合から有効なデータインスタンスを取得するため、関数 \\(h_x(z&#39;)=z\\) (ただし、\\(h_x:\\{0,1\\}^M\\rightarrow\\mathbb{R}^p\\)) が必要です。 関数 \\(h_x\\) は 1 を説明したいインスタンス x に対する値に割り当てます。 表形式のデータの場合、0 をサンプリングした他のインスタンスの値に割り当てます。 これは、&quot;特徴量の値が不在&quot;であるということと、&quot;特徴量の値は、データからランダムで選ばれた特徴量で置き換えられる&quot;ということが等価であることを意味しています。 表形式データの場合、以下の図は連合から特徴の値に変換する方法を可視化しています。 FIGURE 5.47: 関数 \\(h_x\\) は、連合から有効なインスタンスへの写像。特徴量が存在 (1) するとき、\\(h_x\\) は x の特徴量の値に変換する。特徴量が不在 (0) のとき、\\(h_x\\) はランダムにサンプリングされたデータインスタンスの値に変換する。 表形式データに対する \\(h_x\\) は、\\(X_C\\) と \\(X_S\\) は独立として扱い、周辺分布上で積分します。 \\[f(h_x(z&#39;))=E_{X_C}[f(x)]\\] 周辺分布からのサンプリングは、存在する特徴量と存在しない特徴量の間の依存関係を無視することを意味します。 それゆえに、KernelSHAP は permutation ベースの解釈手法と同じ問題があります。 推定は、現実的に起こりそうもないインスタンスに過剰な重みを与えます。 結果は信頼できないものになります。 しかし、周辺分布からサンプリングすることは必要です。 もし、存在しない特徴量が条件付き分布からサンプルされた場合、結果の値はもはやシャープレイ値ではありません。 結果の値は、結果に寄与しない特徴量のシャープレイ値は 0 であるという、ダミーに関するシャープレイの公理に反します。 画像の場合、以下の図が、可能なマッピング関数を説明します。 FIGURE 5.48: 関数 \\(h_x\\) はスーパーピクセル(sp)の連合を画像にマッピングします。存在する特徴量(1)について、\\(h_x\\) は元の画像の相当する部分を返します。不在の特徴量(0)については、\\(h_x\\) は相当する部分をグレーアウトします。周りのピクセルの平均の色を割り当てるか似たような色で割り当てるかは任意です。 LIME との大きな違いは回帰モデルの中のインスタンスの重みです。 LIME は、元のインスタンスにどのくらい近いかによってインスタンスの重みを決定します。 連合ベクトルの中の 0 が多いほど、LIME の重みは小さくなります。 SHAP は、連合がシャープレイ値の推定で得るであろう重みに従って、サンプリングされたインスタンスに重み付けをします。 小さな連合（1が少ない）と大きな連合（1が多い）が最も大きな重みをとります。 この理由は以下の通りです。 もし個々の特徴量の影響を個別に調べることができるのであれば、個々の特徴量のほとんどを知ることになります。 もし連合が単一の特徴量で構成されている場合、予測に対する特徴量の個々の主な影響を知ることができます。 もし連合が全て1つの特徴量を基に構成されているなら、特徴量の全体効果(主要な効果+特徴量の相互作用)を知ることができます。 もし連合が特徴量の半分で構成されている場合、半分の特徴量となりうる連合が多くあるため、個々の特徴量についてほどんど知ることができません。 シャープレイ準拠の重みを達成するために、Lundbergらは、SHAP カーネルを提案しました。 \\[\\pi_{x}(z&#39;)=\\frac{(M-1)}{\\binom{M}{|z&#39;|}|z&#39;|(M-|z&#39;|)}\\] ここで、M は連合の最大サイズ、 \\(|z&#39;|\\) はインスタンス z' 内に存在する特徴量の数です。 Lundberg と Lee はこのカーネルの重みを使った線形回帰でシャープレイ値が得られることを示しています。 もし連合データにおいてSHAPカーネルをLIMEで使用する場合、LIMEもまたシャープレイ値を推定します。 連合のサンプリングに関してもう少し賢い方法があります。 最も小さい連合と大きい連合が重みのほとんどを奪います。 盲目的にサンプリングをする代わりに、サンプリング予算 K の一部使ってこれらの高い重みの連合を含めることで、よりよいシャープレイ値の推定を得ることができます。 そこで、1個 と Mー1個の特徴量を持つ可能な全ての連合(全部で 2M 個の連合)からスタートします。 十分な予算が残っている時(現在の予算は K - 2M)、2個の特徴量の連合や、M-2個の特徴量の連合などを含めます。 残りのサイズの連合からサンプリングを行い、重みを再調整します。 ターゲットと重みのデータを持っています。 重み月線形回帰モデルを構築する全ては以下の通りです。 \\[g(z&#39;)=\\phi_0+\\sum_{j=1}^M\\phi_jz_j&#39;\\] 線形モデル g を損失関数 L を最適化することによって学習します。 \\[L(f,g,\\pi_{x})=\\sum_{z&#39;\\in{}Z}[f(h_x(z&#39;))-g(z&#39;)]^2\\pi_{x}(z&#39;)\\] ただし、Z は学習用データです。 これは、線形モデルを最適化するための、古き良き、退屈な二乗誤差の合計です。 予測されたモデルの係数 \\(\\phi_j\\) がシャープレイ値です。 線形回帰の設定なので、回帰のための標準ツールも使用できます。 例えば、正則化項を追加し、モデルをスパース化できます。 もし、私たちが損失 L に L1 ペナルティを追加することで、スパースな説明ができます。 （ただし、スパース化によって、有効なシャープレイ値のままであるかどうか確かでありません。） 5.10.3 TreeSHAP Lundberg ら(2018)47 は、決定木やランダムフォレスト、GBT(gradient boosted trees)などといった木構造ベースの機械学習モデルのための SHAP のバリエーションとして、TreeSHAP を提案しました。 TreeSHAP は高速でモデルに特化した KernelSHAP の代替として導入されましたが、直感的でない 特徴量の属性を生成しうることが判明しました。 TreeSHAP は、周辺分布の期待値の代わりに条件付き期待値 \\(E_{X_S|X_C}(f(x)|x_S)\\) を使って関数の値を定義します。 条件付き期待値の問題点は、予測関数 f に影響を与えない特徴量が、非ゼロの TreeSHAP 推定値を取得しうることです。4849 非ゼロの推定値は、特徴量が実際に予測に影響する他の特徴量と相関を持つときに起こります。 TreeSHAPはどれほど高速なのでしょうか。 厳密なKernelSHAPと比較して、T を木の数、L を木の中の最大の葉の数、D を木の中の最大の深さとしたとき、計算量を \\(O(TL2^M)\\) から \\(O(TLD^2)\\) にまで削減できます。 TreeSHAPは影響を推定するために条件付き期待値 \\(E_{X_S|X_C}(f(x)|x_S)\\) を使用します。 あるインスタンス x と特徴量の部分集合 S に対して、1つの木の期待される予測値がどのように計算されるのか、直感的に理解してみましょう。 全ての特徴量で条件付けされているとき -- S は全ての特徴量の集合 -- 、期待される予測は、インスタンス x が含まれるノードの予測値になります。 どの特徴量でも条件付けされていないなら -- S は空集合 -- 、全ての終端ノードの予測の重み付き平均が期待される予測になります。 いくつかの特徴量を S が含んでいる場合(全てではない)、到達できないノードの予測は無視します。 ちなみに、到達できないとは、ノードへの決定経路(decision path)が、\\(x_S\\) の値と矛盾することを意味します。 残った終端ノードから、ノードサイズ（ノード内の学習サンプル数）によって重みづけられた予測値の平均を計算します。 この結果が S が与えられた時の x に対する期待される予測値となります。 問題点は、この手順をありうる特徴量の部分集合 S のそれぞれに対して適用しなければならないことです。 TreeSHAP は指数時間ではなく多項式時間で計算します。 基本的な考えは、全てのあり得る部分集合 S を同時に木へ押し込むことです。 それぞれの決定ノードに対し、部分集合の数を追跡する必要があります。 これは、親ノードの部分集合と特徴量の分割に依存します。 例えば、ある木における最初の分割が x3 とすると、全ての x3 を含む部分集合が1つのノード（xの行先）へ行きます。 x3 を含まない部分集合は、削減された重みと共に両方のノードへ行きます。 不運にも、異なるサイズの部分集合は異なる重みを持ちます。 アルゴリズムはそれぞれのノードにおける部分集合の重み全体を追跡しなければなりません。 これがアルゴリズムを複雑にしています。 TreeSHAP の詳細について、原論文を参照してください。 この計算は複数の木に対しても拡張されます。 シャープレイ値の加法性のおかげで、アンサンブル木のシャープレイ値は、個別の木のシャープレイ値の (加重) 平均となります。 次に、SHAP による説明を見てみましょう。 5.10.4 例 子宮頸がんのリスクを予測するために100の木を持つランダムフォレスト分類器を学習させました。 それぞれの予測を説明するために SHAP を使います。ランダムフォレストは木のアンサンブルであるため、Kernel SHAP の代わりに高速な TreeSHAP が使用できます。 しかし、この例では条件付き分布の代わりに周辺分布を用いています。これはパッケージで説明されており、元の論文では説明されていません。 Python の TreeSHAP 関数は周辺分布では低速ですが KernelSHAP よりは高速です、なぜならデータの行に比例して増加していくからです。 ここでは周辺分布を使っているので、説明はシャープレイ値の章と同じです。 しかし、Python の shap パッケージは異なる可視化になります。 特徴量の属性をシャープレイ値のような &quot;力(forces)&quot; として可視化できます。 それぞれの特徴量の値は予測値を増加させるか減少させるかの力を持ちます。 予測値は基準値から始まります。 シャープレイ値の基準値はすべての予測値の平均になります。 プロットでは、シャープレイ値は予測値を増加させる方向(正の値)か減少させる方向(負の値)を指す矢印となります。 この影響はデータインスタンスの実際の予測値で互いに釣り合っています。 次の図は、子宮頸がんデータセットからの2人の女性の SHAP による説明の様子を示しています。 FIGURE 5.49: 2人の個人の予測される癌の確率を説明するためのSHAP。 ベースライン（平均予測確率）は 0.066 であり、 最初の女性の予測リスクは 0.06 と低くなっている。性感染症（STD）などのリスク増加効果は、年齢などの減少効果によって相殺される。2人目の女性の予測リスクは 0.71 と高くなっている。51歳と34歳の喫煙は、癌の予測リスクを高める。 これらは、個々の予測に対する説明でした。 シャープレイ値は、グローバルな説明のために組み合わせることができます。 すべてのインスタンスに対して SHAP を実行すると、シャープレイ値の行列が得られます。 この行列は、データインスタンスごとに1つの行があり、特徴量ごとに1つの列があります。 この行列のシャープレイ値を分析することで、モデル全体を解釈できます。 SHAPを使った特徴量重要度から始めます。 5.10.5 SHAP 特徴量重要度 (SHAP Feature Importance) SHAP 特徴量重要度の背後にあるアイデアは単純です。 シャープレイ値の絶対値が大きい特徴量は重要です。 大域的な重要度を求めたいため、特徴量ごとのシャープレイ値の絶対値のデータ内での平均をとります。 \\[I_j=\\sum_{i=1}^n{}|\\phi_j^{(i)}|\\] 次に、重要度の降順に特徴量を並べ替え、それらをプロットします。 次の図は、以前子宮頸がんの予測のために学習したランダムフォレストの SHAP 特徴量重要度です。 FIGURE 5.50: シャープレイ値の絶対値の平均として計算された SHAP 特徴量重要度。ホルモン避妊薬の使用年数は最も重要な特徴量であり、予測される癌の可能性の絶対値を2.4%変動させます (x軸での0.024). SHAP 特徴量重要度は permutation feature importance の代替手法です。 ただし、これらの重要度の計測には大きな違いがあります。 Permutation feature importance はモデルの性能低下に基づきます。 一方で、SHAP は特徴量の帰属の大きさに基づいています。 特徴量重要度プロットは有用ですが、重要度以上の情報は含んでいません。 より有益なプロットとして、次の summary plot があります。 5.10.6 SHAP Summary Plot この summary plot は、特徴量重要度と特徴量の影響を結びつけます。 Summary plot の各点はあるインスタンスの特徴量のシャープレイ値です。 y軸方向の位置は特徴量によって、x軸方向の位置はシャープレイ値によって決まります。 色は特徴量の値の大小を表します。 特徴量ごとのシャープレイ値の分布を知ることができるように、重複点はy軸方向にずらされています。 特徴量は重要度に従って並べられます。 FIGURE 5.51: SHAP summary plot. ホルモン避妊薬使用年数が低いと予測される癌のリスクは低減し、高いとリスクが増加する。ただし、あらゆる効果はモデルの振る舞いを説明するものであり、必ずしも実世界での要因とは限らないことを注意。 Summary plotでは、特徴量の値と予測への重要度の関係性を示しています。 しかし、関係性の正確な姿を見るには、SHAP dependence plot を見る必要があります。 5.10.7 SHAP Dependence Plot SHAP feature dependence は、最も単純な大域的な解釈のためのプロットかもしれません。 1) 特徴量を選ぶ。 2) それぞれのインスタンスに対して、x軸に特徴量の値を、y軸に対応するシャープレイ値をプロットします。 3) おわり。 数学的には、プロットは次の点を含みます。 \\(\\{(x_j^{(i)},\\phi_j^{(i)})\\}_{i=1}^n\\) ホルモン避妊薬使用年数の SHAP feature dependence を以下に示します。 FIGURE 5.52: ホルモン避妊薬使用年数の SHAP feature dependence。 0年と比較して、低い年数のときは予測される確率を下げ、高い年数は予測される確率を増加させている。 SHAP dependence plot は、partial dependence plots や accumulated local effects の代替手法です。 PDP や ALE plot が平均効果を示すのに対して、SHAP dependence はy軸方向のばらつきも示せます。 特に相互作用がある場合には、SHAP dependence plot はy軸方向に更にばらつくでしょう。 SHAP dependence plot はそれらの特徴量の相互作用を強調表示することで、改善されます。 5.10.8 SHAP 相互作用値 (SHAP Interaction Values) 相互作用効果は、個々の特徴量の影響を考慮した後の追加の複合的な特徴量の効果です。 ゲーム理論から、シャープレイ相互作用は下記のように定義できます。 \\[\\phi_{i,j}=\\sum_{S\\subseteq\\setminus\\{i,j\\}}\\frac{|S|!(M-|S|-2)!}{2(M-1)!}\\delta_{ij}(S)\\] \\(i\\neq{}j\\) のときであり、\\[\\delta_{ij}(S)=f_x(S\\cup\\{i,j\\})-f_x(S\\cup\\{i\\})-f_x(S\\cup\\{j\\})+f_x(S)\\] です。 この方程式は特徴量の主効果を差し引くので、個々の効果を考慮した後の純粋な相互作用効果を得ることができます。 シャープレイ値の計算と同様に、私たちは、考えられる全ての特徴量連合 S の値を平均化します。 全ての特徴量のSHAP相互作用値を計算した時、インスタンスごとに、サイズが M x M の行列が得られます。ここで M は特徴量の数を表しています。 どのように相互作用インデックスを使用するのでしょうか？ 例えば、相互作用の強さで自動的に SHAP feature dependence plot をつけます。 FIGURE 5.53: SHAP feature dependence plot による相互作用の可視化。ホルモン避妊薬の使用年数は性感染症と相互作用する。0年に近づく、STDの発生がある場合は予測される癌のリスクが増加する。避妊薬を数年に渡り使用した場合、STD の発生は予測されたリスクを減少させる。繰り返しになりなるが、これは、因果のモデルではない。この影響は交絡による可能性がある。（例えば、STDsと癌リスクの低下は、より多くの医師の診察と相関している可能性がある。 5.10.9 Clustering SHAP values シャープレイ値を使って、データをクラスタリングできます。 クラスタリングの目的は似たようなインスタンスのグループをみつけることです。 クラスタリングは普通、特徴量を基に行われますが、特徴量が異なるスケールを持つことがよくあります。 例えば、身長はメートルで計測されていたり、色の強弱は 0 から 100 の数字であったり、センサー出力が -1 から 1 の間ということもあります。 そのような異なる比較できない特徴量を持つインスタンスの間の距離を計算することは困難です。 SHAP clustering は各インスタンスのシャープレイ値を使ってクラスタ化します。 これは、説明の類似性によってインスタンスをクラスタ化することを意味します。 全てのSHAP値は同じ単位 -- (予測空間の範囲) -- を持っています。 任意のクラスタ手法が使用可能です。 次の例では、階層的クラスタリングを使ってインスタンスを順序付けています。 プロットは多くの force plot で構成されており、それぞれがインスタンスの予測を説明します。 force plotを垂直に回転させ、クラスタリングの類似性にしたがって並べて配置しています。 FIGURE 5.54: 説明の類似性によってクラスタリングされた Stacked SHAP explanations。それぞれのx軸の位置はデータのインスタンス。赤の SHAP 値は予測を増加させ、青の値は予測を減少させる。右にある目立つクラスタは、癌のリスクが高いと予測されたグループです。 5.10.10 長所 SHAPがシャープレイ値を計算する事から、シャープレイ値の全ての長所がSHAPにも反映されます。 SHAPは、ゲーム理論において &quot;確かな理論的根拠&quot; を持ちます。 予測は特徴量の中で公平に分配されています。 平均の予測と、個々の予測を比較することで 対照的な説明 ができます。 SHAPは、LIME と シャープレイ値をつなげます。 これは、両方の手法をより理解するためにとても有用です。 また、解釈可能な機械学習の分野を統一するために有用です。 SHAPは、決定木ベースのモデルに対して高速な実装を持っています。 私はこれが SHAP の人気の鍵だと信じています。 なぜなら、シャープレイ値の実装の一番の障害はその計算速度の遅さだからです。 早い計算速度は、大域的なモデルの解釈に必要な多くのシャープレイ値の計算をすることを可能にします。 大域的な解釈の手法は、特徴量重要度(feature importance)、feature dependence、interactions, clustering や summary plots があります。 SHAPを使用すると、シャープレイ値が大域的な解釈の &quot;原子単位(atomic unit)&quot; であるため、大域的な解釈と局所的な解釈が一致します。 もしあなたが LIME を局所的な説明に使い、partial dependence plot、permutation feature importance を大域的な説明のために使用した場合、共通の基盤が欠けています。 5.10.11 短所 KernelSHAPは計算に時間がかかります。 多くのインスタンスに対してシャープレイ値を計算したい時に、KernelSHAP は実用的ではありません。 また、SHAP feature importance などの、全ての大域的な SHAP の手法は多くのインスタンスのシャープレイ値を計算することを必要とします。 KernelSHAPは特徴量の依存関係を無視します。 ほとんどの他の permutation ベースの解釈手法も同じ問題を抱えています。 特徴量を、ランダムなインスタンスからの値に入れ替えることによって、簡単に周辺分布からのランダムサンプリングを実現できます。 しかしながら、もし、特徴量に依存がある場合(例えば、相関関係がある)、これは、ありそうもないデータ点に過剰な重みを与える事につながります。 TreeSHAP は、条件付きの予測を明示的にモデリングする事で、この問題を解決しています。 TreeSHAP は直感的ではない特徴量の属性を作り出す可能性があります。 TreeSHAP が現実的ではないデータ点を作り出してしまう問題を解決できる一方で、別の問題を作り出してしまいます。 TreeSHAP では、value function は条件付き期待予測に従うように変更されています。 そのため、予測に影響を与えない特徴量に対しても、TreeSHAP は非ゼロの値を持つ可能性があります。 シャープレイ値の欠点もまた、SHAPに適用されます。 シャープレイ値は誤解する恐れがあり、新しいデータに対してシャープレイ値を計算するためには、データにアクセスできる必要があります(TreeSHAPを除いて)。 5.10.12 ソフトウェア 著者は SHAP の Python パッケージのshap を実装しました。 この実装は Python のための機械学習ライブラリである scikit-learn の決定木ベースのモデルに対して機能します。 このパッケージはこの章の例でも使用されました。 SHAP は tree boosting のフレームワークである xgboost と LightGBMに統合されています。 R では、shapper と fastshap のパッケージがあります。 SHAP は R のxgboost のパッケージにも含まれています。 Lundberg, Scott M., and Su-In Lee. &quot;A unified approach to interpreting model predictions.&quot; Advances in Neural Information Processing Systems. 2017.↩ Lundberg, Scott M., Gabriel G. Erion, and Su-In Lee. &quot;Consistent individualized feature attribution for tree ensembles.&quot; arXiv preprint arXiv:1802.03888 (2018).↩ Sundararajan, Mukund, and Amir Najmi. &quot;The many Shapley values for model explanation.&quot; arXiv preprint arXiv:1908.08474 (2019).↩ Janzing, Dominik, Lenon Minorics, and Patrick Blöbaum. &quot;Feature relevance quantification in explainable AI: A causality problem.&quot; arXiv preprint arXiv:1910.13413 (2019).↩ "],["example-based.html", "Chapter 6 例示に基づいた説明手法", " Chapter 6 例示に基づいた説明手法 例示に基づいた説明手法では、データセットにおける特定のインスタンスを用いることで、機械学習モデルの振る舞いや、データの分布を説明できます。 例示に基づいた説明手法の大部分は、任意の機械学習モデルをさらに解釈可能なものにできるため、モデル非依存の手法と言えます。 モデル非依存の手法 (model-agnostic methods) で紹介したものと異なる部分は、例示による方法は、feature importance や partial dependence のように特徴量の要約をするものではなく、データセットにおけるいくつかのインスタンスを選択することでモデルを説明するという点です。 例示に基づいた説明手法は、データのインスタンスを人間が理解できる方法で表現できる場合のみ効果があります。 この手法は、直接表示できるため、画像に対して有効です。 一般的には、例示に基づいた手法は画像や文章のように、特徴量がより多くの文脈を持つ場合、すなわちデータが何らかの構造を持つ場合、有効な手段となります。 非常に多くの、かつ構造を持たない特徴量から構成されるような表形式のデータを表現することはより難しくなります。 1つの要素を表現するのに全ての特徴量を用いることは多くの場合効果的な手段ではありません。 この手法は、データを表現する特徴量が少ししかない場合や、要素を何らかの方法で要約できる場合に上手く機能します。 例示に基づいた説明手法は、人間が機械学習モデル及び機械学習モデルが学習するデータを想像するのに役立ちます。 特に、複雑なデータ分布を理解するときに有益です。 しかし、例示に基づいた説明手法というのは具体的にはどのようなことなのでしょうか。 実は、私たちはこれらの手法を日々の仕事や生活の中でよく使っています。 まず例をいくつか考えてみましょう。50 ある医師が普通ではない咳と微熱の症状のある患者を診察している場合を考えましょう。 患者の症状から、医師は数年前診察した同様の症状をもつ患者を思い出しました。 そこで、医師は目の前にいる患者が同じ病気であることを疑い、この病気を検査するための血液検査をしました。 次に、あるデータサイエンティストが自身の顧客の新しいプロジェクトに取り組む場合を考えます。 そのプロジェクトは、キーボードを生産する機器の誤作動に繋がるリスク要因を分析するというものです。 このデータサイエンティストは似たようなプロジェクトに取り組んだことがあることを覚えており、同様の分析を顧客が望んでいると考えて前のプロジェクトのプログラムを一部再利用しました。 最後に、とある子猫が燃えさかる無人の家の窓枠に座っている状況を考えましょう。 消防隊は既に到着しており、そのうちの一人が子猫を助けるため危険を犯して建物に入るべきだろうかと考えをめぐらせました。 しかし、この消防士は今までの経験で似たような状況に出会ったことがあったことを思い出します。 古い木造の家はしばらくの間ゆっくりと燃えた後、多くの場合不安定になり最終的に倒壊してしまっていたのです。 これらの状況を照らし合わせ、この消防士は家が崩れる危険性が高すぎると考え、家に入らないことにしました。 幸運にも子猫は窓から飛び出してきて上手く着地し、この火事により被害を受けた人はいませんでした。 ハッピーエンドです。 これらの例は人間がどのように例やアナロジーを考えるかを示しています。 例示に基づいた説明手法は、基本的に「BはAと似ており、AはYの原因であるから、Bも同様にYを引き起こすと予測する」という考えを用いています。 暗黙のうちに、いくつかの機械学習の手法は、これに基づいて動作しています。 決定木は予測のために重要な特徴量の類似性に基づいて、データをノードに分割していきます。 決定木は、新しいインスタンスに対して、似たインスタンス(同じ終端ノードに属する)を発見し、それらの平均値を予測値として返すという動作をします。 また、k近傍法（knn）は、まさに、例示に基づく予測方法です。 新しいインスタンスにたいして、knn は k 個の最近傍を求め (例えば、k=3 のインスタンス)、それらの平均を予測値として返すように動作します。 knn の予測は、k 近傍を返すことで説明可能です。 ただし、個々のインスタンスを表現する良い方法がある場合のみ、有効な手段になります。 この章では、以下の例示に基づく説明手法について説明します。 反事実的説明(Counterfactual explanations)では、あるインスタンスの予測を大きく変化させるためには、どのように変更する必要があるかを示します。反事実的なインスタンスを作成することによって、モデルがどのように予測するかが分かり、個々の予測を説明できるようになります。 敵対的サンプル(Adversarial examples)は、機械学習モデルを騙すために用いられる反事実です。 これは、予測を説明することが目的ではなく、間違った予測をさせることが目的となります。 プロトタイプ(Prototypes)は、データから選択された代表的なインスタンスの集合であり、クリティシズム(criticism)は、それらのプロトタイプではうまく表現できないインスタンスのことを言います。51 Influential instancesとは、予測モデルのパラメータや予測そのものに最も影響を与えた学習データのインスタンスを指します。Influential instance は、データそのものの問題の発見や、モデルのデバッグ、モデルの振る舞いをより解釈可能なものとするために役立ちます。 k近傍モデル：例示に基づいた（解釈可能な）機械学習モデル Aamodt, Agnar, and Enric Plaza. &quot;Case-based reasoning: Foundational issues, methodological variations, and system approaches.&quot; AI communications 7.1 (1994): 39-59.↩ Kim, Been, Rajiv Khanna, and Oluwasanmi O. Koyejo. &quot;Examples are not enough, learn to criticize! Criticism for interpretability.&quot; Advances in Neural Information Processing Systems (2016).↩ "],["反事実的.html", "6.1 反事実的説明 (Counterfactual Explanations)", " 6.1 反事実的説明 (Counterfactual Explanations) 反事実的説明は、「もしXが起こらなかったら、Yも起こらなかっただろう」のような因果的状況で述べられます。 「このホットコーヒーを一口飲まなければ、舌を火傷しなかっただろう」という例では、 イベント Y は舌を火傷したことであり、原因 X はホットコーヒーを飲んだこととなります。 反事実を考えることは、観察された事実と矛盾する（例えば、ホットコーヒーを飲まなかったという世界のような）仮説的な現実を想像する必要があります。それ故に、「反事実的」という名称がつけられているのです。 反事実を考える能力によって、我々人類は他の動物と比較してとても賢くなりました。 解釈可能な機械学習において、反事実的説明は個々の事象に対する予測を説明する際に使用されます。 「出来事」はあるインスタンスの予測結果であり、「原因」は、モデルに入力されたモデルに入力された予測の要因となるインスタンスのある特徴量の値です。 グラフが示すように、入力と予測の関係性はとても単純で、特徴量の値が予測の原因となっています。 FIGURE 6.1: 機械学習モデルの入力と予測の因果関係。モデルは単にブラックボックスモデルとみなされる。入力が予測の原因となっている（ただし、データの現実の因果を反映している必要はない） たとえ入力と予測される結果の関係性が現実には因果関係でなかったとしても、モデルの入力を予測の原因みなすことができます。 この単純なグラフが与えられたとき、機械学習モデルの予測に対して、反事実を想定する方法は簡単です。 予測をする前にインスタンスの特徴量の値を変化させ、どのように予測が変化するかを分析します。 予測されたクラスの反転 (例: クレジットカード申請の可否の反転) や、予測がある閾値に到達する (例: がんの確率が 10% に到達する) など、関連する方法で予測が変化するシナリオに関心があります。 予測の反事実的な説明は、最初に定義された出力に予測を変化させるような特徴量の最小の変更方法を示します。 反事実的説明はモデルの入力と出力のみを用いるため、モデル非依存の手法です。 この手法が生み出す解釈は特徴量の違いを要約したもの（特徴量 A と特徴量 B を変更することで予測を変えることができる）とも捉えることができるため、この手法はモデル非依存の手法（model-agnostic methods）の章で取り上げることもできたでしょう。 しかし、反事実的説明はそれ自体が新しいインスタンスであるため、この章で取り上げています（インスタンス X から始め、A と B を変更することで反事実的説明を得る）。 プロトタイプ（prototypes)とは異なり、反事実は学習データに実際に含まれるインスタンスである必要はありませんが、特徴量の新しい組み合わせである可能性はあります。 反事実の作成方法を紹介する前に、いくつかの反事実の使用例と良い反事実的説明がどのようなものなのかということについて説明します。 まず最初の例では、ある男性が、申請した融資を（機械学習を利用している）銀行のソフトウェアに却下された、という状況を考えます。 彼はなぜ自分の申請が却下され、またどのようにしたら融資を得られる見込みが高くなるかということについて不思議に思いました。 この「なぜ」という疑問は、反事実を用いることによって明確にできます。 予測を却下から承認に変えるための特徴量（収入、クレジットカードの枚数、年齢など）の最小の変化はなんでしょうか。 この問いに対する答えには例えば次のようなものが考えられます。 この男性が年間 10,000 ユーロ以上を稼ぐならば、彼は融資を受けられたでしょう。 もしくは、この男性の持つクレジットカードがより少なく、5年前に不履行に陥っていなかったならば、融資を受けられたでしょう。 この銀行は透明性に関して全く関心を持っていなかったため男性が融資を断られた理由を知ることはありませんでしたが、それはまた別の話です。 次の例では、連続的な結果を予測するモデルを反事実を用いて説明しようと思います。 ある女性が所有するアパートを貸し出したいと考えているが、家賃をどの程度にすればわからないため、家賃を予測するために機械学習モデルを学習することにした、という状況を考えます。 もちろん、この女性はデータサイエンティストであるため、これこそが彼女が問題を解決する手段です。 家の大きさ、立地、ペット可かどうかなどの詳細を全て入力すると、モデルは家賃を 900 ユーロと予測しました。 彼女は家賃は 1000 ユーロ以上になると考えていましたが、自分のモデルを信頼し、アパートの価値をどのようにしたら高めることができるかを見るために特徴量を変化させてみることにしました。 そして彼女は部屋が 15m2 よりも大きかった場合、1000 ユーロ以上で貸し出せる可能性があることを見つけました。 興味深い結果ですが、アパートを大きくできないため、これは実現不可能な知識です。 最終的には、彼女は自分でも改善できる特徴量の値（ビルトインキッチンがあるか、ペット可かどうか、床のタイプなど）を微調整することによって、ペットを許可し、より良い断熱窓を設置すれば家賃を1000ユーロ以上にできることを突き止めます。 彼女は直感的に反事実を利用することにより結果を変えたのです。 反事実は現在のインスタンスとは対照的であり、また選択的である、すなわち通常は少ない特徴量の変化に焦点をあてるため、人間に優しい説明です。 しかし反事実は「羅生門効果」の影響を大きく受けます。 羅生門とは侍の殺人事件が複数人の視点から語られる日本の映画のことです。 それぞれの話は結果を等しく説明しますが、それらは互いには全く異なっているのです。 たいていは、複数の異なった反事実的説明が存在するため、同じことが反事実的説明にも言えます。 それぞれの反事実は、どのように特定の結果に到達するかの異なる「ストーリー」を語っています。 ある反事実は特徴量 A を変化させると言い、一方で、他の反事実は特徴量 A はそのままで、特徴量 B を変化させると言うかもしれませんが、これは矛盾しています。 この複数の真実が存在する問題は、全ての反事実を報告するか、反事実を評価する基準を持ち最良のものを選ぶかによって解決できます。 では、反事実的説明の良し悪しはどのように定めれば良いのでしょうか。 まず、反事実的説明ではユーザーがインスタンス（現実の代替）の予測の変化を定義することから、第一に反事実的なインスタンスの予測が、事前に定義された予測を可能な限り忠実に再現していることが必要です。 定義通りの予測を正確に出力することは常に可能であるとは限りません。 滅多に発生しないクラスと発生する頻度の高いクラスを持つような分類問題では、モデルは常にインスタンスを頻繁に起こるクラスに分類してしまうかもしれません。 このような場合、特徴量を変更することで予測されるラベルを普遍的なクラスから希少なクラスに変えることは不可能かもしれません。 このため、反事実に対する予測は定義された結果に正確に一致していなければならないという条件をもう少し緩めてみましょう。 先の分類問題を例に取ると、希少なクラスが予測される確率を現在の 2% から 10% に増加するような反事実をあげることができます。 このときの質問は、予測される確率を 2% から 10% （もしくは 10% に近い値）に変えるような特徴量に対する最小限の変更は何か、となります。 もう1つの品質に対する基準は反事実は元のインスタンスの特徴量と可能な限り似ている必要があるということです。 これには2つのインスタンス間の距離を測る必要があります。 また、反事実には元のインスタンスに近いだけでなく、変更された特徴量の数が少ないことも必要です。 これはマンハッタン距離などの適切な尺度を選ぶことにより実現できます。 最後の条件は、反事実的インスタンスはもっともらしい特徴量の値を持つことが必要である、ということです。 アパートのサイズが負の値であったり、部屋の数が200もあるようなアパートの家賃に対しては、反事実的説明は意味をなさないでしょう。 反事実がデータの同時分布に従っていると、より良いです。例えば、20m2 のサイズの部屋が 10 部屋あるアパートは、反事実的説明に用いるべきではありません。 6.1.1 反事実的説明の生成 反事実的説明を生成するためのシンプルかつ素朴な方法は試行錯誤を繰り返しながら探索することです。 この例としては、先のアパートの例で家賃を高くできるアパートの条件を見つけようとしたように、関心のあるインスタンスの特徴量の値をランダムに変え、望む結果が得られたらやめる、といった方法があります。 しかし、試行錯誤して探すよりも良い方法があります。 まず、関心のあるインスタンス、それに対する反事実、そして望む（反事実的な）結果を入力とする損失関数を定義します。 この損失関数は反事実から予測された結果が事前に定義された結果からどの程度離れているか、また反事実そのものがどの程度関心のあるインスタンスから離れているかを測ります。 この損失関数は、直接最適化アルゴリズムを用いるか、&quot;Growing Spheres&quot; という手法で提案されているように、インスタンスの周囲を探索することで最適化できます（詳しくはソフトウェア及びその代替手法を参照）。 ここでは、2017年、Wacherらにより提案されたアプローチを紹介します。 このアプローチでは次の損失を最小化することを提案しています。 \\[L(x,x^\\prime,y^\\prime,\\lambda)=\\lambda\\cdot(\\hat{f}(x^\\prime)-y^\\prime)^2+d(x,x^\\prime)\\] この式の最初の項では反事実 x' のモデルの予測と、事前にユーザが定義した望ましい結果 y' の二乗距離を計算しています。 第2項は説明されるインスタンス x と反事実 x' の距離を表します。 この項についての詳細は後ほどします。 パラメータ \\(\\lambda\\) は第1項目の予測に対する距離と第2項目の特徴量に対する距離のバランスを決定します。 この損失関数は、与えられた \\(\\lambda\\) の値に対して、反事実 x' を求めます。 \\(\\lambda\\) の値が大きいとき、得られる反事実が求めたい結果 y' により近くなる結果が得られ、\\(\\lambda\\) の値が小さいとき、得られる反事実の特徴量が x と近い結果が得られます。 \\(\\lambda\\) が非常に大きい値である場合、x からの距離に関わらず y' に最も近い予測をもつインスタンスが選択されます。 最終的には、ユーザは反事実の予測が、どの程度求める結果と一致するかという点と、反事実がどの程度 x と一致するかという点に関して、バランスを決定する必要があります。 この手法の著者は、\\(\\lambda\\) の値を決定する代わりに、どれだけ反事実インスタンスの予測が y' と離れていても良いかを表す許容量 \\(\\epsilon\\) を決定することを提案しています。 この条件は以下のように記述できます。 \\[|\\hat{f}(x^\\prime)-y^\\prime|\\leq\\epsilon\\] この損失関数を最小化するには、Nelder-Mead法といった多くの最適化アルゴリズムを用いることができます。 もし機械学習モデルの勾配にアクセスできる場合、ADAM などの勾配ベースの手法を用いることもできます。 説明されるインスタンス x 、求めたい結果 y' 、許容量 \\(\\epsilon\\) は事前に定めておく必要があります。 \\(\\lambda\\) の値を増加させながら（許容パラメータ内において）十分に近い解を探索することで、損失関数は x' について最小化され、（局所的に）最適な反事実 x' が返されます。 \\[\\arg\\min_{x^\\prime}\\max_{\\lambda}L(x,x^\\prime,y^\\prime,\\lambda)\\] インスタンス x と反事実 x' の距離を測る関数 d は、中央絶対偏差（median absolute deviation, MAD）の逆数によって特徴量ごとにスケーリングされたマンハッタン距離で表されます。 \\[d(x,x^\\prime)=\\sum_{j=1}^p\\frac{|x_j-x^\\prime_j|}{MAD_j}\\] 距離の合計値は全ての p 個の特徴量間の距離の総和、すなわちインスタンス x と反事実 x' の特徴量の差の絶対値を表します。 特徴量ごとの距離は、データセット内の特徴量 j の MAD の逆数によってスケーリングされます。 \\[MAD_j=\\text{median}_{i\\in{}\\{1,\\ldots,n\\}}(|x_{i,j}-\\text{median}_{l\\in{}\\{1,\\ldots,n\\}}(x_{l,j})|)\\] ベクトルの中央値はベクトル内の値の半分がこれより大きくなり、残りの半分がこれより小さくなる値を指します。 MAD は特徴量の分散に相当しますが、平均を中心として距離の二乗和をとる代わりに、中央値を中心として距離の絶対値の和を用います。 ユークリッド距離と比較すると、提案された距離関数はスパース性が導入されているという利点があります。 これは、異なる特徴量の数が少ないとき、2点がより近くなることを意味します。 また、外れ値に対してよりロバストになります。 MAD によりスケーリングすることは全ての特徴量を同じスケールにするためことに必要です。 アパートの大きさをメートルで測るかフィートで測るかは結果に影響すべきではありません。 反事実を生成する手順はシンプルです。 説明したいインスタンス x、望ましい結果 y'、許容量 \\(\\epsilon\\)、\\(\\lambda\\) の初期値（小さな値）を選択する。 最初に用いる反事実としてランダムにインスタンスをサンプリングする。 最初に選んだ反事実を出発点として損失関数を最適化する。 \\(|\\hat{f}(x^\\prime)-y^\\prime|&gt;\\epsilon\\) を満たす間、 \\(\\lambda\\) の値を増加させる。 現在の反事実を出発点として損失を最適化する。 損失を最小化する反事実を返す。 ステップ 2-4 を繰り返し、反事実のリストもしくは損失を最小化するものを返す。 6.1.2 例 どちらの例も Wachter らの論文 (2017)より引用しています。 最初の例では、著者は3層の全結合型ニューラルネットを学習し、法科大学入学前の成績（GPA）、人種、入学試験の得点から、入学1年目の学生の成績を予測しようとしています。 目標は次の問いを満たす反事実的説明を各学生に対して求めることです。 予測スコアが 0 となるにはどのように入力特徴量を変更する必要があるでしょうか。 スコアは事前に標準化されているため、スコアが 0 というのはその学生が平均的であることを意味します。 また、スコアが負であることは平均以下、スコアが正であることは平均以上であることを意味します。 次の表は学習された反事実を表しています。 点数 GPA 入学試験の点数 人種 GPA x' 入学試験の点数 x' 人種 x' 0.17 3.1 39.0 0 3.1 34.0 0 0.54 3.7 48.0 0 3.7 32.4 0 -0.77 3.3 28.0 1 3.3 33.5 0 -0.83 2.4 28.5 1 2.4 35.8 0 -0.57 2.7 18.3 0 2.7 34.9 0 最初の列は予測した点数、次の3列は特徴量の元々の値、そして最後の3列が点数が0に近い値となるような反事実の特徴量の値を表しています。 また、最初の2行は平均以上の予測となった学生、残りの3行は平均以下となった学生を表しています。 ここで、最初の2行の反事実は、予測値が下がるには各生徒の特徴量がどのように変更される必要があるのか、残りの3行では予測値が上がるためには各生徒の特徴量がどのように変更される必要があるのかを示しています。 点数が上がる反事実では常に人種が黒人（1の値）から白人（0の値）となっており、モデルが人種に対するバイアスを持っていることがわかります。 この時、GPAは変化していませんが、入学試験の点数が変化していることがわかります。 2つ目の例では糖尿病のリスクの予測に対する反事実的説明を表しています。 3層全結合ニューラルネットを訓練し、年齢、BMI、妊娠の回数などからピマ族の血を引く女性が糖尿病にかかるリスクを予測しています。 反事実を次の問いとして、糖尿病リスクのスコアを0.5に増加させる、もしくは減少させるにはどの特徴量が変更される必要があるかを考えます。 1人目：2時間の血中インスリン測定の結果が 154.3 ならば、スコアが0.51となる 2人目：2時間の血中インスリン測定の結果が 169.5 ならば、スコアが0.51となる 3人目：血糖値が 158.3 かつ2時間の血中インスリン測定の結果が 160.5 ならば、スコアが0.51となる 6.1.3 長所 反事実的説明は単純明快です。 反事実に従ってあるインスタンスの特徴量を改変すると、予測結果が予め設定した値に変わります。 背景には他の仮定も魔法もありません。 従って、LIMEのように局所的なモデルをどれだけ外挿していいか不明な手法と比べると、危険性が低いです。 反事実的説明は新たなインスタンスを生成しますが、反事実がどの特徴量を変更したものか要約できます。 結果をまとめるにあたって2つの選択肢が生じます。 反事実インスタンスを報告するか、説明したいインスタンスとその反事実インスタンスとの間でどの特徴量が異なるかを示すか選べます。 反事実的手法はデータやモデルへのアクセスが不要です。 モデルの予測関数にさえアクセスできればよく、web APIなどによる運用も可能です。 この性質は、サードパーティの監査を受ける企業やデータやモデルを非公開にしたままユーザーに説明性を提供したい企業にとって魅力的です。 会社は企業秘密やデータ保護の観点から、モデルやデータの秘匿に関心を寄せています。 反事実による説明は、モデルの予測を説明することと、モデルの所有者の利益を守ることのバランスを取ることができます。 反事実的手法は機械学習を用いないシステムにも応用可能です。 入出力を持つ任意のシステムに対して反事実は作れます。 アパートの賃料予測は手書きのルールによる構築も可能ですが、反事実的説明は依然として機能します。 反事実的説明は比較的簡単に実装できます。 というのも、反事実的説明は標準的な最適化ライブラリによって最適化可能な損失関数にすぎないからです。 ただし、特徴量の範囲の妥当性などいくつか追加で考慮すべき点があります（例えば 、アパートの広さは正の値しか取りません）。 6.1.4 短所 通常、各インスタンスは複数の反事実的説明を持ちます（羅生門効果）。 これは便利ではありません。多くの人は現実世界の複雑さよりも簡単な説明の方を好みます。 また、これは実用面での課題でもあります。 例として1つのインスタンスに対して23個の反事実的説明が生成されたとしましょう。 これらを全て報告すべきでしょうか、それとも最も良いものだけにするべきでしょうか。 もしそれらが全て比較的「良い」もので、中身が異なるものであったならどうでしょうか。 これらの課題はプロジェクトごとに新たに答えを出さなくてはいけません。 過去の経験に合うものを選択できるという理由から、複数の反事実的説明を持つことが有利になることもありえます。 与えられた許容量 \\(\\epsilon\\) に対して、反事実的説明が見つかるとは限りません。 これは手法による過失ではなく、データに依存しています。 提案された手法では異なるレベルを持つカテゴリカル特徴量を上手く扱うことができません。 この手法の著者はカテゴリカル特徴量の組み合わせごとにこの方法を実行することを提案していますが、これも多くの値を持つ複数のカテゴリカル特徴量を持つ場合には組合せ爆発を起こしてしまいます。 例えば10個の固有の値を持つカテゴリカル特徴量が6個あった場合、100万回もの実行が必要となります。 カテゴリカル特徴量のみを扱う際の解決策としては、Martensらが提案した手法(2014)52 が存在します。 また、量的変数とカテゴリカル変数を両方扱い、カテゴリカル変数に対して原理的に摂動を生成する手法が Python パッケージ Alibi に実装されています。 6.1.5 ソフトウェアと代替手法 反事実的説明のPython実装は Alibi パッケージです。パッケージの作者は 単純な反事実的手法 と その拡張 としてプロトタイプクラスを用いて解釈しやすくし、アルゴリズムの収束性を改善したものを実装しています。 Martens らは文書の分類問題に対して同様の手法を提案しています(2014)。 彼らはある文書がどうして特定のクラスに分類されなかったのか説明することに注力しています。 本章で説明した手法と異なり、Martens らは文書の分類に注目しているので、入力は単語の出現頻度です。 他にも反事実を探す方法として、Laugel らによるGrowing Spheresアルゴリズム (2017)53があります。 この手法は、注目したい点の周囲に球を描き、その球に含まれる点を抽出し、抽出した点の予測結果が期待通りか確認しながら、（スパースな）反事実を発見し提示するまで球を拡大していきます。 彼らは論文中で反事実 (counterfactual) という用語を用いていませんが、手法としてはよく似ています。 彼らはさらに特徴量を最小限に改変して反事実を得るための損失関数を定義しています。 損失関数を直接最適化する代わりに、彼らは上述の球を用いた探索を提案しています。 Ribeiro らによるAnchor (2018)54は反事実とは対極的な存在です。 これについては、Scoped Rules (Anchors)の章をご覧下さい。 Martens, David, and Foster Provost. &quot;Explaining data-driven document classifications.&quot; (2014).↩ Laugel, Thibault, et al. &quot;Inverse classification for comparison-based interpretability in machine learning.&quot; arXiv preprint arXiv:1712.08443 (2017).↩ Ribeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. &quot;Anchors: High-precision model-agnostic explanations.&quot; AAAI Conference on Artificial Intelligence (2018).↩ "],["adversarial.html", "6.2 敵対的サンプル (Adversarial Examples)", " 6.2 敵対的サンプル (Adversarial Examples) 敵対的サンプルとは、機械学習モデルに間違った予測をさせる、意図的に小さな摂動を持たせたサンプルのことをいいます。 これは概念的にとても似ているため、反事実的説明についてを先に読むことをおすすめします。 敵対的サンプルは、モデルを解釈するためのものではなく、モデルを騙すことを目的とした反事実的説明だといえます。 なぜ私たちは敵対的サンプルについて学ぶ必要があるのでしょうか。これらは実用性のない、機械学習のおもしろい副産物なのでしょうか。答えは「NO」です。以下のような場合、機械学習のモデルは敵対的サンプルにより脆弱性を持つことになってしまいます。 自動運転の車が「止まれ」の標識を無視したために他の車にぶつかってしまいました。この前、誰かが標識の上に、人間には標識が少し汚れていると感じさせる程度の写真を置いていきました。しかし、車に搭載されていた標識を識別するシステムに対しては「駐車禁止」の標識と判断させるよう作られたものでした。 迷惑メールの検出器が、迷惑メールを分類することに失敗しました。迷惑メールはあたかも普通のメールに見えるよう書かれていましたが、受け取った人を騙すような内容が含まれていました。 機械学習の機能を搭載したスキャナが空港でスーツケースに入った武器の検出に用いられていました。このシステムに傘だと思わせることで検知を防ぐナイフが開発されてしまいました。 それでは敵対的サンプルを生成するいくつかの方法を見ていきましょう。 6.2.1 手法及び例 敵対的サンプルを生成する手法はいくつか存在します。 多くの手法では、予測が（敵対的な）求める値となり、かつ敵対的サンプルと元のサンプルの距離が最小化されるよう操作します。 手法には、ニューラルネットなど勾配を用いるモデルに対しこの勾配にアクセスする必要があるものや、また、予測関数にアクセスできればよく、モデルに依存せず用いることができるものなどが存在します。 この章では既に研究が多くされており、敵対画像を可視化することで多くの情報が得られる、ニューラルネットワークによる画像分類における手法に焦点を当てて説明します。 画像の敵対的サンプルでは画像の画素に意図的に特定の値を加えることで、実用段階のモデルを騙すことを目的とします。 これらの例からは、物体検知する深層ニューラルネットが人間には何の変哲もないように見える画像からいかに簡単に騙されてしまうかを見てとることができます。 これらの例を見たことがない人は、予測の変化があまりに理解できないものであることに驚くかもしれません。 敵対的サンプルは、機械にとっての目の錯覚のようなものなのです。 私の犬の何かがおかしい Szegedyら（2013）55は &quot;Intriguing Properties of Neural Networks&quot; という論文の中で、勾配ベースの最適化手法を用いて深層ニューラルネットの敵対的サンプルを見つけようとしました。 FIGURE 6.2: Szegedyらの生成したAlexNetにおける敵対的サンプル（2013）。左側の列の画像は全て正しく分類される。真ん中の列は、右側の画像を生成するために画像に加えられた誤差を拡大したものを示しており、右側の画像はすべて「ダチョウ」として間違って分類されてしまう。 （&quot;Intriguing properties of neural networks&quot;, Figure 5 by Szegedy et. al. CC-BY 3.0.） これらの敵対的サンプルは次の関数を変数 r に関して最小化することで生成されます。 \\[loss(\\hat{f}(x+r),l)+c\\cdot|r|\\] この式では、x は画像を表す画素値のベクトル、r は敵対画像を生成するための画素値の差分（x+r が新しい画像となる）、l は求める結果のクラス、c は画像間及び予測結果間の距離のバランスをとるためのパラメータを表します。 1項目は敵対的サンプルの予測結果と望ましいクラス l との距離であり、2項目は元の画像と敵対的サンプルの画像の距離を示しています。 この定式化は 反事実的説明 を生成するため用いられた損失関数とほぼ同じです。 ただし、こちらでは画素値が0から1の値であるための制約が加えられます。 著者らは、矩形拘束条件を持つ L-BFGS 法という、勾配ベースの最適化アルゴリズムを用いてこの最適化問題を解くことを提案しています。 Disturbed panda: Fast gradient sign method。 Goodfellowら（201456は gradient sign method と呼ばれる敵対的画像を生成するアルゴリズムを開発しました。 この手法は元となるモデルの勾配を用いることで敵対的サンプルを作成します。 元の画像 x は各ピクセルに微小な誤差 \\(\\epsilon\\) を加えることで操作され、この誤差 \\(\\epsilon\\) の符号は対応するピクセルの勾配の符号により決定されます。 勾配方向に対して誤差を加えることで、モデルが画像を誤認識するよう画像を意図的に変更を加えているのです。 fast gradient sign method の主な考え方は以下の式のように表されます。 \\[x^\\prime=x+\\epsilon\\cdot{}sign(\\bigtriangledown_x{}J(\\theta,x,y))\\] ここで、\\(\\bigtriangledown_x{}\\) は元の入力ベクトル x に対するモデルの損失関数の勾配、y は x のラベルを表すベクトル、\\(\\theta\\) はモデルのパラメータのベクトルを表します。 勾配ベクトルは入力画像のベクトルと同じ長さであり、ここではこの符号のみを扱います。 勾配の符号が正ならば、ピクセル値を増加させることでモデルの損失も増加し、負ならばピクセル値を下げることでモデルの損失は増加します。 この脆弱性はニューラルネットワークが入力のピクセル値とクラススコアの関係性を線形に扱う際に起こります。特に、LSTM や maxout network、ReLU を活性化関数とするネットワーク、ロジスティック回帰など、線形性を含む構造はこの手法に対して脆弱性を持ちます。 この攻撃は外挿法により実行されます。 入力画像のピクセル値とクラススコア間の線形性は外れ値に対して脆弱であり、ピクセル値をデータの分布の領域外の値とすることでモデルは騙される場合があります。 これらの敵対的サンプルはニューラルネットワークの構造に対し極めて特異的だと考えられてきましたが、この手法により同じタスクに対し訓練されたネットワークならば敵対的サンプルを使い回すことができることが明らかになりました。 Goodfellowら（2014）は敵対的サンプルを学習データに加えることでモデルをより頑健なものとすることを提案しました。 くらげ？バスタブ？: 1-pixel attack。 2014年にGoodfellowとその同僚らにより提案された手法では多くのピクセルの値を僅かに変更する必要がありました。しかし1つのピクセル値しか変えられないとしたらどうでしょうか。モデルを欺くことができるでしょうか。 Suら(2019)57 は、1つの画素値を変更するだけで画像分類モデルを欺くことが可能であることを示しました。 FIGURE 6.3: 故意に1つのピクセル値を変更することで ImageNet データセットに対して訓練されたニューラルネットは元のクラスではなく誤ったクラスを予測するようになってしまう。 反事実的説明と同様に、1-pixel attack は元画像 x に近いサンプル x’ を探し変更を加えますが、予測を敵対的な結果へと導きます。ただし、距離の近さの定義は異なり、1ピクセルのみ変えることが許可されます。1-pixel attack は差分進化を用いることで、どのピクセルをどのように変更すべきかを探索します。差分進化は生物学における種の進化を元に作られた手法で、候補解と呼ばれる個体群を反復的に組み替えて行くことで解を探索します。それぞれの候補解はピクセル値の変更を表現したものであり、xy座標及び赤、青、緑のrgb値の5つの要素により表されます。探索は、例として400の候補解（ピクセル値の変更案）から始まり、次の式を用いて新しい世代の候補解が作成されます。 \\[x_{i}(g+1)=x_{r1}(g)+F\\cdot(x_{r2}(g)-x_{r3}(g))\\] ここで、それぞれの \\(x_i\\) は候補解の要素（x座標、y座標、赤、緑、青）、g は現在の世代、F はスケーリング用パラメータ（0.5に設定）、r1、r2、r3 は異なる乱数を表します。 それぞれの新しい子の候補解は順に位置及び色の5つの特性をもつ1つのピクセルであり、これらの特性はランダムに選ばれた3つ親ピクセルの特性を混ぜ合わせたものとなります。 1つの候補解が敵対的サンプルとして、すなわち誤ったクラスに分類された場合、もしくは反復回数がユーザが指定する最大値に達した場合、子の生成は終了します。 全部トースター: 敵対的パッチ。 私が最も好きな手法の１つに、敵対的サンプルを現実世界へと拡張させるものがあります。ブラウンらは2017年、物体の横に貼り付けることで画像分類器にトースターと認識させる、印刷可能なラベルを開発しました。 FIGURE 6.4: ImageNet 上で訓練された VGG16 分類器にバナナの画像をトースターと分類させるステッカー。Brown et. al (2017) この手法では、敵対的サンプルは元の画像とほぼ同じものである必要があるという制約が取り除かれている点で、これまでに紹介した敵対的サンプルの例とは異なっています。その代わり、この手法では画像の一部が任意の形を取ることのできるパッチによって完全に置き換えられます。パッチは多くの状況で機能するよう、画像に対するパッチの位置を変えながら、また時には大きさを変えたり回転させながら、異なる背景画像に対し最適化されます。最終的には最適化された画像は印刷され、様々な画像分類器を誤認識させるのに用いることができるようになります。 3Dプリンタで作った亀を銃撃戦に持ち込むな。たとえコンピュータがそれを良いアイディアだと思ったとしても: 頑健な敵対的サンプル 次の方法は文字通り、トースターに別の次元を足すものです。 Athalye ら(2017)58はディープニューラルネットワークにとってどう見てもライフルとしか認識できない亀を3Dプリンタを用いて作成しました。 人にとって亀に見える物質がコンピューターにとってはライフルに見えてしまうのです。 FIGURE 6.5: Athalye ら(2017)は、TensorFlow の標準的な訓練済み InceptionV3 分類器にとってはライフルにしか見えない亀を3Dプリンタで作成した。 著者らが用意した3次元的な敵対的サンプルは、画像に回転や拡大など、どんな変形を加えても、2次元画像の分類器を欺くことに成功しました。 Fast gradient法など、他の方法は画像の回転や角度の変更に対応できません。 Athalye et. al (2017)は画像の変形に頑健な Expectation Over Transformation (EOT)アルゴリズムを提案しました。 EOTはさまざまな変形に対して敵対的サンプルを最適化するというアイディアに基いています。 オリジナル画像と敵対的サンプルの距離を最小化する代わりに、EOTは様々な方法でオリジナル画像を変形した時に距離の期待値が閾値以下になるよう調整します。 変形に伴う距離の期待値は以下のように記述できます。 \\[\\mathbb{E}_{t\\sim{}T}[d(t(x^\\prime),t(x))]\\] ここで、\\(x\\) はオリジナル画像、\\(t(x)\\) は変形した画像（たとえば回転）、\\(x&#39;\\) は敵対的サンプル、\\(t(x&#39;)\\) は変形した敵対的サンプルです。 様々な変形に加えて、EOT 法は敵対的サンプルの探索を最適化問題として扱うために、familiar pattern of framing the search を用います。 次の確率を最大化することで、変形に対して、 例えばライフルなどの選択したクラス \\(y_t\\) と見なす敵対的サンプル \\(x&#39;\\) を見つけます。 \\[\\arg\\max_{x^\\prime}\\mathbb{E}_{t\\sim{}T}[log{}P(y_t|t(x^\\prime))]\\] さらに、オリジナル画像 x と敵対的サンプル x' の距離の期待値が次のようにある閾値以下になるよう制約をつけます。 \\[\\mathbb{E}_{t\\sim{}T}[d(t(x^\\prime),t(x))]&lt;\\epsilon\\quad\\text{and}\\quad{}x\\in[0,1]^d\\] この方法の実現可能性について少し考えておきましょう。 他の手法はデジタル画像を操作します。 一方で、3次元に出力した頑健な敵対的サンプルは現実に登場し、コンピューターによる物体認識を阻害します。 言い方を変えてみましょう。 誰かが亀にしか見えないライフルを作ったらどうしますか。 目隠しされた敵：ブラックボックスアタック。 以下のような状況を考えてみましょう。 あなたは Web API を通して優れた画像分類器に対してアクセスできるようになりました。 ただしモデルからの予測を得ることはできますが、モデルのパラメータに対してはアクセスできません。 ソファに寝そべっていても、データを送信すればこのサービスはそのデータに対応する分類結果を送り返してくれます。 多くの敵対的手法では、敵対的サンプルの生成に元となる深層ニューラルネットワークの勾配を用いるため、このような状況には用いることができません。 Papernot と彼の同僚59は2017年、モデルに関する情報や学習データを用いることなく敵対的サンプルを生成できることを示しました。この種の、（ほぼ）前提知識を必要としない攻撃手法はブラックボックスアタックと呼ばれます。 どのように動くかを以下に示します。 学習データと同じ分野に属する画像を何枚か用意します。例として攻撃する分類器が数字の分類するものである場合、数字の画像を用います。その分野への知識は必要ですが、学習データへアクセスする必要はありません。 ブラックボックスから準備した画像に対する予測結果を取得します。 これらの画像に対するサロゲートモデル（ニューラルネットワークなど）を訓練します。 現在の画像セットについて、モデルの出力の分散が大きくなるようなピクセル値の操作を探索するヒューリスティックを用い、新しく合成画像のデータセットを作成します。 事前に定めたエポック数だけ2から4のステップを繰り返します。 Fast Gradient Method などを用いてサロゲートモデルに対して敵対的サンプルを生成します。 これらの敵対的サンプルを用いて元のモデルを攻撃します。 サロゲートモデルの目的はブラックボックスモデルの決定境界を推定することであり、同等の精度を求める必要はありません。 著者はクラウド上の機械学習サービス上で学習させた画像分類器に対し攻撃を仕掛けることでこの手法をテストしました。これらの画像分類器はユーザがアップロードした画像とラベルを用いて学習をします。ソフトウェアは、時にはユーザの知らないアルゴリズムを用いてモデルを自動的に学習させ、デプロイします。その後分類器はアップロードされる画像に対し予測を出力しますが、モデル自体は中身を見たりダウンロードできません。著者らは多くのプロバイダに対して敵対的サンプルを生成することに成功し、最大 84% もの敵対的サンプルが誤認識されました。 この手法は攻撃するモデルがニューラルネットワーク出ない場合でも動作します。これには決定木のような勾配を用いない学習モデルも含まれます。 6.2.2 サイバーセキュリティーの観点 機械学習は既知の分布から未知のデータ点を予測するという既知の未知を扱うものです。 攻撃に対する防御は未知の未知を扱います。敵対的な入力の未知の分布からの未知のデータを頑健に予測します。 機械学習が自動運転車や医療機器などの多くのシステムに適用されるにつれて、それらはサイバー攻撃の入り口にもなりつつあります。 テストデータセットでの機械学習モデルの予測が 100% 正しい場合でさえ、モデルを騙すための敵対的な例を見つけることができます。 サイバー攻撃から機械学習モデルを守ることは、新しいサイバーセキュリティーの一分野となっています。 Biggio ら(2018)60は、この10年の敵対的機械学習に関する研究の優れたレビューをしています。 サイバーセキュリティーは攻撃者と防御者が何度も互いを出し抜く軍拡競争です。 サイバーセキュリティーには3つの黄金則があります。1)敵を知れ 2)プロアクティブであれ 3)自分自身を守れ です。 分野が違えば、敵も異なります。 お金目当てに電子メールを使って詐欺する人々は、メールサービス利用者と電子メールサービスのプロバイダの敵です。 ユーザがメールシステムを使い続けてくれるように、プロバイダはユーザを保護したいと考える一方で、攻撃者はユーザーからお金を奪いたいと考えます。 敵を知ることは、その人たちの目標を知ることを意味します。 これらのスパマー (spammers) がいることが知らず、電子メールサービスの悪用法が音楽の海賊版の送信だけであると仮定すると、防御は異なったものになります。（例えば、スパムを判定するために文章を分析する代わりに、著作権で保護された添付ファイルをスキャンします。) プロアクティブであるということは、システムの弱点を進んでテストして特定することを意味します。 敵対的な例を用いて、積極的にモデルを騙し、さらにそれらを防ごうと試みる時、プロアクティブだと言えるでしょう。 解釈可能なモデルを用いてどの特徴量が重要であるかや、どのように予測に影響しているかを理解しようとすることもまた、機械学習モデルの弱点を知るためのプロアクティブなステップです。 データサイエンティストとして、この危険な世界においてテストデータセットにおけるモデルの予測能力を超えて試すこと無しにモデルを信用できますか。 異なったシナリオ下でモデルがどのように振る舞うか分析し、最も重要な入力を特定し、複数の例で予測の説明性を試しましたか。 敵対的な入力を見つけようとしたことがありますか。 機械学習モデルの解釈可能性はサイバーセキュリティーにおいて大きな役割を果たします。 プロアクティブと反対に、リアクティブであるということは、システムが攻撃されるまで何も策を講じず、攻撃されて初めて問題点を認識し守るための指標を取り入れることを意味します。 どうすれば敵対的サンプルから自分の機械学習モデルのシステムを守ることができるでしょうか。 プロアクティブなアプローチとしては敵対的サンプルを用いたクラス分類の反復学習が挙げられます。これは敵対的学習とも呼ばれます。 他のアプローチとしては、ゲーム理論に基づいた方法で、特徴量の不変変換学習やロバスト最適化（正則化）なども存在します。 他には、単一ではなく複数の分類器を用いて予測を投票（アンサンブル）で決定する手法も挙げられますが、全ての分類器が同様な敵対的サンプルに苦しむ可能性があるため、正しく機能する保証はありません。 さらに別のアプローチとしては、元のモデルの代わりに最も近い分類器を使うことで有用な勾配を用いないモデルを構成する、勾配マスキングもありますが、これもまた上手く機能しません。 攻撃者のシステムに対する理解度に応じて、攻撃の種類を区別できます。 まず、攻撃者が完璧な知識、つまりモデルの種類、パラメータ、学習データの情報を持っているパターン(white box attack)があります。 次に、攻撃者が部分的な情報しか持っていないパターン(gray box attack)があります。攻撃者が使用された特徴量の代表的なものと、モデルの種類しか知らず、学習データやパラメータにはアクセスできない場合がそれにあたります。 最後に、攻撃者が何の知識も持っていないパターン(black box attack)があります。この場合、攻撃者はブラックボックスなモデルから結果を取り出すことだけが出来て、学習データやモデルパラメータについての情報にはアクセス出来ません。 モデルを攻撃するため、敵は情報の程度に応じて異なるテクニックを用います。 例で見たように、ブラックボックスの場合でさえ敵対的サンプルを生成できるため、データやモデルの情報を隠すことは攻撃から身を守るために効果的ではありません。 攻撃者と防御者の関係はいたちごっこになるため、この分野では多くの開発とイノベーションが見られます。 スパムメール1つ取っても、そこには多くの種類が生じており、絶え間なく進化しています。 機械学習モデルに対する新しい攻撃手法が発明されては、それらに対する新しい防御策が提案されています。 最新の防御手法を回避するために、より強力な攻撃が開発され、それが永遠に続きます。 この章を通して、読者が敵対的サンプルの問題に対し敏感になることを願っています。 そしてその問題は機械学習モデルをプロアクティブに研究することによってのみ、弱点を発見し修正できます。 Szegedy, Christian, et al. &quot;Intriguing properties of neural networks.&quot; arXiv preprint arXiv:1312.6199 (2013).↩ Goodfellow, Ian J., Jonathon Shlens, and Christian Szegedy. &quot;Explaining and harnessing adversarial examples.&quot; arXiv preprint arXiv:1412.6572 (2014).↩ Su, Jiawei, Danilo Vasconcellos Vargas, and Kouichi Sakurai. &quot;One pixel attack for fooling deep neural networks.&quot; IEEE Transactions on Evolutionary Computation (2019).↩ Athalye, Anish, and Ilya Sutskever. &quot;Synthesizing robust adversarial examples.&quot; arXiv preprint arXiv:1707.07397 (2017).↩ Papernot, Nicolas, et al. &quot;Practical black-box attacks against machine learning.&quot; Proceedings of the 2017 ACM on Asia Conference on Computer and Communications Security. ACM (2017).↩ Biggio, Battista, and Fabio Roli. &quot;Wild Patterns: Ten years after the rise of adversarial machine learning.&quot; Pattern Recognition 84 (2018): 317-331.↩ "],["proto.html", "6.3 prototype と criticism", " 6.3 prototype と criticism prototype は、すべてのデータの代表であるデータインスタンスです。 criticism は prototype の集まりではうまく表現できないデータインスタンスです。 criticism の目的は、特に、prototype が良く表現できないデータ点について、prototype とともに見識を提供することです。 prototype と criticism は、データを記述するのに機械学習モデルとは独立に使用可能ですが、解釈可能なモデルを作成したり、ブラックボックスモデルを解釈可能にするために使用できます。 この章では、1つのインスタンスを指すときや、あるインスタンスはそれぞれの特徴量の次元である座標系における1つの点でもあることの説明を強調するために &quot;データ点&quot; という表現を使います。 次の図は、シミュレーションによるデータの分布を示していて、prototype として、または criticism として選択されたインスタンスもいくつかあります。 小さな点はデータ、大きな点は criticism で、大きな四角形は prototype です。 prototype はデータ分布の中央を覆うように(手動で)選択されていて、criticism は prototype を除くクラスタ内の点です。 prototype と criticism は常に実際のデータに含まれるインスタンスです。 FIGURE 6.6: 2つの特徴量 x1 と x2 を持つデータ分布の prototype と criticism。 手作業で prototype を選びましたが、これは上手くスケールせず悪い結果になりそうです。 データの中から prototype を見つけるためのアプローチはたくさんあります。 そのうちの1つが k-medoids であり、k-means アルゴリズムに関連するクラスタリングアルゴリズムです。 クラスタの中心として実際のデータ点を返すようなクラスタリングアルゴリズムであれば prototype の選択に適しているでしょう。 しかし、それらの手法の殆どは prototype しか見つけることができず、criticism は見つけられません。 この章では、Kim et.al (2016)61 による MMD-critic という、prototype と criticism を1つのフレームワークに組み合わせたアプローチを紹介します。 MMD-critic は、データの分布と選択された prototype の分布を比較します。 これが MMD-critic を理解するための中心的な概念です。 MMD-critic は、2つの分布間の差異を最小化するような prototype を選択します。 密度の高い領域のデータ点、特に、異なる&quot;データクラスタ&quot;から選ばれたとき、良い prototype となります。 prototype ではうまく説明できない範囲から得られるデータ点は criticism として選択されます。 理論を深掘りしましょう。 6.3.1 理論 高レベルでの MMD-critic の手順は以下のように簡潔にまとめられます。 見つけたい prototype と criticism の数を選びます。 prototype を貪欲法で探索します。 prototype の分布がデータの分布と近くなるように prototype は選択されます。 criticism を貪欲法で探索します。 prototype の分布とデータの分布が異なる点は criticism として選択されます。 データセットにおける prototype と criticism を MMD-critic によって見つけるために必要な構成要素がいくつかあります。 最も基本的な要素として、データの密度を推定するためにカーネル関数が必要です。 カーネルは2つのデータ点の近さに応じて重み付けする関数です。 密度推定に基づいて、選択された prototype の分布がデータの分布と近いかどうか知るために、2つの分布がどれだけ異なるのかを知る基準が必要です。 これは、 maximum mean discrepancy(MMD) を測ることで解決します。 また、カーネル関数に基づいて、特定のデータ点において2つの分布がどれだけ異なるのか知るための witness 関数が必要です。 prototype とデータの分布が乖離していて、witness 関数が大きな絶対値をとるデータ点を criticism として選択できます。 最後の要素は、良い prototype と criticism のための探索戦略であり、単に貪欲探索によって解決します。 2つの分布の違いを測る maximum mean discrepancy(MMD) から初めていきましょう。 prototype の選択によって prototype の密度分布を作ります。 我々は prototype の分布がデータの分布と異なるのかどうか評価したいです。 カーネル密度関数によってその両方を推定します。 maximum mean discrepancy は2つの分布間の差異を測る指標です。これは2つの分布に対応する期待値間の差異を表す関数空間上での上界です。 わかるでしょうか。 個人的に、これらの概念は、データからどんなものが計算されるのかを見た方が遥かに良く理解できます。 次の式は、MMD の二乗(MMD2)の計算方法を示しています。 \\[MMD^2=\\frac{1}{m^2}\\sum_{i,j=1}^m{}k(z_i,z_j)-\\frac{2}{mn}\\sum_{i,j=1}^{m,n}k(z_i,x_j)+\\frac{1}{n^2}\\sum_{i,j=1}^n{}k(x_i,x_j)\\] k は2点の類似度を測るカーネル関数ですが、これについては後述します。 m は prototype である z の数で、n は元々のデータセット上のデータ点 x の数です。 prototype の z はデータ点 x から選ばれたものです。 それぞれの点は多次元なので、複数の特徴量を持つことができます。 MMD-critic の目的は MMD2 を最小化することです。 MMD2 がゼロに近いほど、prototype の分布はデータによくフィットします。 MMD2 をゼロに近づける鍵となるのは中央の項で、これは prototype と全てのデータ点との平均的な（2乗された）近さを計算します。 この項と、最初の項（prototype どうしの平均的な近さ）に加え、最後の項（データどうしの平均的な近さ）を足し上げれば、prototype はデータを完璧に説明できます。 n 個全てのデータ点を prototype として使った時、この式に何がどうなるのか試してみてください。 次の図は、MMD2 measure を表現しています。 最初の図は、データ点を2つの特徴量で示していて、推定データ密度を背景にグラデーションで表現しています。 その他はそれぞれ異なる prototype の選択をしていて、その時の MMD2 measure を図の上部に記しています。 prototype は大きなドットで、その分布は等高線で表しています。 これらの中で最もデータを覆っている prototype の選択は（左下）最も低い乖離度になっています。 FIGURE 6.7: 二つの特徴量を持つデータセットに対する異なる prototype の MMD の二乗(MMD2) カーネルの選択は動径基底関数カーネルです。 \\[k(x,x^\\prime)=exp\\left(-\\gamma||x-x^\\prime||^2\\right)\\] ここで、||x-x'||2 は2点間のユークリッド距離で \\(\\gamma\\) はスケールパラメータです。 カーネルの値は2点間の距離に応じて減少し、0と1の範囲に収まります。 2点間が無限遠にある時に 0 になり、同じ点にある時 1 になります。 prototype を見つけるためのアルゴリズムとして MMD2、カーネル、貪欲法を組み合わせます。 prototype の空のリストを用意します。 prototype の数は選ばれた m 個以下として、 データセット上のそれぞれの点が prototype のリストに追加された時に MMD2 がどのくらい減少するのか確認します。MMD2 を最小化するようなデータ点をリストに追加します。 prototype のリストを返します。 criticism を探すための残りの成分は、ある点での2つの推定密度がどのくらい異なるのか知るための witness 関数になります。 これは次のように推定できます。 \\[witness(x)=\\frac{1}{n}\\sum_{i=1}^nk(x,x_i)-\\frac{1}{m}\\sum_{j=1}^mk(x,z_j)\\] 2つの同じ特徴量を持つデータセットがある時、witness関数は、点 x がどちらの経験的分布に良く適合するかを評価する手段となります。 criticism を見つけるために、witness 関数の外れ値を正負どちらの方向についても探します。 witness 関数の最初の項は点 x とデータの類似度の平均で、同様に、2番目の項はデータ点と prototype 間の類似度の平均です。 ある点 x での witness 関数が 0 に近づくと、データとプロトタイプの密度関数は互いに近づきます。これはプロトタイプの分布が点 x でのデータの分布と類似することです。 witness 関数が点 x で負であるとき、prototype の分布はデータの分布を過大評価しています(例えば、prototype を1つ選んだのに近くにデータ点が少ししかない時など)。 一方、正のとき、プロトタイプの分布はデータの分布を過小評価しています(例えば、x の周りのデータ点が沢山あるのに、近くの prototype を1つも選ばないなど)。 より直感的に理解するために、先ほどプロットした prototype のうち最も小さな MMD2 をもつものを再利用して、いくつか手動で選んだ点に対する witness 関数を表示してみましょう。 次の図のラベルは、三角形で表された様々な点に関する witness 関数の値を示しています。 中央の点のみが大きな絶対値をもつことから、criticism として良い候補になります。 FIGURE 6.8: 異なる点における witness 関数の評価 witness 関数を使うことで、prototype によって上手く表されていないデータインスタンスを明示的に探すことができます。 criticism は witness 関数の出力の中で、高い絶対値を持つ点のことを言います。 prototype と同様に、criticism も貪欲法により探索されます。 しかし、全体の MMD2 を減らすのではなく、witness 関数と正則化項を含むコスト関数を最大化する点が探されます。 正則化項は、異なるクラスターから点が選択されるよう、点の多様性を強制します。 この2つ目のステップは、prototype の決定方法とは独立しています。 criticism を学ぶため、ここで述べた手順といくつかの prototype を選んでみます。 もしくは、prototype は、例えば k-medoids のようなクラスタリング手法により選択できます。 これで、MMD-critic 理論の重要な部分は以上です。 ただし、1つ疑問が残されています。 機械学習の解釈性に、MMD-criticはどのように使われるのかということです。 MMD-critic は、1) データの分布の理解に役立つ、2) 解釈可能なモデルを構築する、3) ブラックボックスを解釈可能にする という3つの観点で、解釈性に寄与します。 prototype と criticism を探すために、データセットに MMD-critic を適用すると、データへの理解が深まります。 しかし、MMD-critic を用いると、さらに期待できることがあります。 例えば、解釈可能な予測モデルを作ることもできます。 これを&quot;nearest prototype model&quot;と呼びましょう。 この予測モデルは以下のように定義されます。 \\[\\hat{f}(x)=argmax_{i\\in{}S}k(x,x_i)\\] このモデルは、カーネル関数の出力が大きいという意味で、新しいデータに最も近い prototype が S から選ばれるという意味になります。 prototype 自体は、予測結果に対する説明として考えられます。 この方法には、カーネルの種類、カーネルのスケールパラメータ、prototypeの数という3つのハイパーパラメータがあります。 全てのパラメータは、クロスバリデーションで最適化できます。 criticism はこのアプローチでは使われません。 3つ目の MMD-critic の使い方として、prototype と criticism により任意の機械学習モデルを大域的に説明可能にできます。 方法としては、次の通りです。 prototype と criticism を MMD-critic を用いて探索 機械学習モデルを学習 prototype と criticism に対して、学習された機械学習モデルを用いて、予測結果を取得 予測結果を解析: どのケースでモデルは予測を間違えたのか。 これにより、機械学習モデルの弱点を見つけることができ、またデータをよく表現するいくつからのサンプルを見つけることができます。 これはどのような意味があるでしょうか。 Google の画像分類モデルが、黒人をゴリラとして分類した問題を思い出してください。 おそらく、彼らは、画像認識モデルをデプロイする前に、これらの手法を適用するべきだったのでは無いでしょうか。 99% 正しくても、この問題は残りの 1% にある問題なので、モデルの性能を確認するだけでは、不十分です。 そして、ラベルも間違っていることがあります。 全ての学習データを通して、予測結果に問題が無いか確認していれば、この問題は事前に明らかになったかもしれませんが、それは不可能です。 しかし、数千程度の prototype と ciriticisms を選択と確認し、データに関する問題を明らかにしたでしょう。 肌の黒い人の画像が少なく、つまりデータセットの多様性に問題があることを事前に明らかにできたかもしれません。 あるいは、肌の色が濃い人を prototype として、あるいは悪名高い「ゴリラ」という分類で criticism として、1つ以上の画像を表示していたかもしれません。 MMD-critic が確実にこの類のミスを取り除くとは約束できませんが、よいチェックにはなるでしょう。 6.3.2 例 MMD-critic の次の例では、手書き数字データセットを使っています。 実際の prototype の見ると、数字ごとの画像の枚数が異なることに気づくかもしれません。 これはクラスごとに固定の数を決めて prototype を選んだ訳ではなく、データセット全体で固定の数を決めて prototype を選んだためです。 予想通り、prototype は各数字の異なる書き方のものが選ばれています。 FIGURE 6.9: 手書き数字データセットにおける prototype 6.3.3 長所 ユーザ研究において、MMD-critic の著者は参加者に画像を与えました。参加者は（犬種など）2つのクラスの1つをそれぞれ表す二枚の画像の内、視覚的にマッチする一枚を選ぶ必要があります。 参加者は、ランダムな画像ではなく、prototype と criticism である画像を与えらた場合に、最高のパフォーマンスを示しました。 prototype と criticism の数は自由に選ぶことができます。 MMD-critic はデータの密度推定にも使えます。 また、MMD-critic はどんなタイプのデータでも、どんなタイプの機械学習モデルにも使うことができます。 アルゴリズムの実装も簡単です。 MMD-critic は解釈性を高めるために使う方法において非常に柔軟です。 複雑なデータ分布を理解するために使用できます。 解釈可能な機械学習モデルを作るために使用できます。 あるいは、black box モデルを理解するために役立ちます。 criticisms を見つけることは、prototype を選ぶプロセスとは独立しています。 しかし、MMD-critic に従って prototype を選ぶことは理にかなっています。なぜなら、prototype も criticisms も、prototype とデータ密度を比較する同じ方法で作れられるためです。 6.3.4 短所 数学的には、prototype と criticism は異なる定義ですが、これらの違いは、カットオフ値（prototype の数）に基づいています。 データの分布をカバーするために、少ない数の prototype を選択したとします。 この場合、criticisims はうまく説明されていない部分に集中してしまいます。 しかし、より多くの prototype を追加したとしても、同じ範囲に集まってしまうでしょう。 どのような解釈をするにしても、criticisims は prototype の数を決定する（任意の）カットオフ値と既存の prototype に強く依存することを考慮に入れる必要があります。 prototype と criticisims の数を選ぶ必要があります。 これはメリットにもなりますが、デメリットにもなります。 どの程度の prototype と criticism の数が実際必要でしょうか。 さらに多い方が良いのでしょうか、それとも少ない方が良いのでしょうか。 1つの答えは、人がどれくらいの時間で画像を見ているかを計測することで、prototype と criticism を決める方法です。 MMD-critic を用いて分類器を作成する場合、直接最適化する方法があります。 1つの解決手段は、横軸に prototype の数をとり、縦軸に MMD2 の値をプロットして見てみることです。 こうすることで、MMD2 カーブがフラットになったところで、prototype の数を選ぶことができます。 他のパラメータは、カーネルとカーネルのスケールパラメータになります。 ここで、prototype と criticism の数の場合と同様の問題があります。 どのようにカーネルとカーネルのスケールパラメータを選べば良いでしょうか。 繰り返しになりますが、MMD-critic を最も近い prototype を返す分類器として使う場合、カーネルパラメータも調整できます。 しかし、MMD-critic の教師なしにおけるユースケースの場合、明らかではありません。 全ての教師なし手法は同様の問題を抱えているため、少し難しいかもしれません。 これは、すべての特徴量を入力として受け取り、いくつかの特徴量が関心のある結果を予測するために関連性がないかもしれないという事実を無視しています。 1つの解決策は、生の画像の代わりに、例えば画像の埋め込みなどの関連する特徴量のみを使うことです。 これは、元のインスタンスを、関連する情報のみを含む表現に射影する方法がある場合のみ有効です。 利用できるコードはありますが、綺麗にパッケージ化され、かつドキュメントがあるものはまだありません。 6.3.5 コードと代替手法 MMD-critic の実装は以下にあります。 https://github.com/BeenKim/MMD-critic. prototype を見つけるための、最もシンプルな代替手法は以下です。 k-medoids by Kaufman et. al (1987).62 Kim, Been, Rajiv Khanna, and Oluwasanmi O. Koyejo. &quot;Examples are not enough, learn to criticize! Criticism for interpretability.&quot; Advances in Neural Information Processing Systems (2016).↩ Kaufman, Leonard, and Peter Rousseeuw. &quot;Clustering by means of medoids&quot;. North-Holland (1987).↩ "],["influential.html", "6.4 Influential Instances", " 6.4 Influential Instances 機械学習モデルは、学習データから生み出されたものであり、学習のインスタンス を1つ削除するとモデルの結果に影響が生じます。 学習データから削除するとモデルのパラメータや予測値を大幅に変化させるようなインスタンスのことを&quot;影響力がある (influential)&quot;と言います。 影響力のある学習インスタンスを特定することによって、機械学習モデルを&quot;デバッグ&quot;でき、モデルの振る舞いや予測をうまく説明できます。 この章では、影響力のあるインスタンスを特定するための2つのアプローチ、deletion diagnostics と影響関数 (influence functions)について説明します。 どちらのアプローチもロバスト統計に基づいており、外れ値やモデルの仮定のずれに頑健な推定方法です。 ロバスト統計は、データから予測モデルの重みや平均推定値のような、ロバストな推定量を得るための方法でもあります。 あなたの街にいる人々の平均収入を推定するため、通りすがりの10人にいくら稼いでいるか尋ねたい場合を考えてみましょう。 あなたが持つ標本が推定に全く適さない場合を除いて、あなたが平均収入を推定する際に1人から受ける影響はどの程度なのでしょうか。 この疑問に答えるには、個々の回答を省略することによって平均値を再計算する方法や、 どの程度平均値が影響を受けるかを、&quot;影響関数&quot;を用いて数学的に導出する方法が考えられます。 1つ目の手法である削除の方法(deletion approach)では、平均値を10回計算し直します。各回において 収入のデータの1つを省き、平均推定値がどの程度変化するかを測定します。 大きな変化は影響力の大きなインスタンスであることを意味します。 2つ目の手法は、統計量もしくはモデルの1次導関数の計算に対応した非常に小さな重みによって、ある人の収入を重み付けします。 この手法は&quot;無限小解析 (infinitesimal approach)&quot;や&quot;影響関数 (influence function)&quot;としても知られています。 ちなみに、平均値は単一の値に直線的にスケールするため、平均推定値は1つの回答に強く影響を受ける可能性があります。 よりロバストな選択肢は中央値です。なぜなら標本の中で1人だけ他の人の10倍稼いでいるような場合であっても、中央値は変化しないからです。 deletion diagnostics と影響関数は、機械学習モデルのパラメータや予測値に対して適用することで、それらの振る舞いをより良く理解したり、個々の予測値を説明することも出来ます。 影響力のあるインスタンスを見つけるための2つのアプローチを見る前に、外れ値と影響力のあるインスタンスとの違いについて説明します。 外れ値 外れ値とは、データセットの内の他のインスタンスから離れたインスタンスです。 &quot;離れた&quot;とは、他のすべてのインスタンスとの距離、ユークリッド距離などが非常に大きいことを意味します。 例えば、新生児のデータセットでは、体重が 6kg の新生児は外れ値とみなされます。 預金が多い銀行口座のデータセットでは、ローン専用口座（マイナス残高が大きく、取引が少ない）は外れ値と考えられます。 次の図は、1次元分布の外れ値を示しています。 FIGURE 6.10: 特徴量 x は x=8 において外れ値を持つガウス分布に従う 外れ値は、興味深いデータ点になり得ます(例えば、criticism)。 外れ値がモデルに影響するとき、それは影響力のあるインスタンスとなります。 影響力のあるインスタンス (Influential instance) 影響力のあるインスタンスとは、削除すると学習されたモデルに強い影響を与えるインスタンスのことです。 学習データから特定のインスタンスを削除してモデルを再学習したときに、モデルのパラメータや予測値が変化するほど、そのインスタンスの影響力が大きくなります。 学習されたモデルにとってインスタンスが影響力を持つかどうかは、ターゲット y の値にも依存します。 次の図は、線形回帰モデルの影響力のあるインスタンスを示しています。 FIGURE 6.11: 特徴量が1つの線形モデル。一度は全てのデータで、もう一度は影響力のあるインスタンスを除いて学習されている。影響力のあるインスタンスを除くと、学習された傾き (重み、係数) が劇的に変化します。 影響力のあるインスタンスがモデルの理解に役立つ理由 解釈可能性において影響力のあるインスタンスの背後にある重要なアイデアは、モデルのパラメーターと予測を、学習データにまでさかのぼって追跡することです。学習器、つまり、機械学習モデルを生成するアルゴリズムは、特徴量 X とターゲット y からなる学習データを受け取って機械学習モデルを生成して返す関数です。例えば、決定木の学習器は、分割する特徴量と値を選択するアルゴリズムです。 ニューラルネットワークの学習器はバックプロパゲーションを用いて最適な重みを見つけます。 FIGURE 6.12: 学習器は学習データ（特徴量とターゲット）からモデルを学習する。学習されたモデルは新たなデータに対して予測を行う。 学習時にインスタンスを学習データから除いたら、モデルパラメータや予測はどの様に変化するのかを考えます。 これは、 部分依存プロット や 特徴量重要度 といった予測に用いるインスタンスの特徴量を操作したときに予測がどう変わるかを分析する他の解釈性へのアプローチとは対照的です。 影響力のあるインスタンスについては、モデルを固定されたものとして扱うのではなく、学習データの関数として扱います。 影響力のあるインスタンスは、大域的なモデルの振る舞いや、予測全般についての疑問に答える手助けをしてくれます。 どのように影響力のあるインスタンスを見つけるのでしょうか。 影響力を測るには2つの方法があります。 1つ目は、学習データからインスタンスを削除し、削減されたデータセットでモデルを再学習してモデルパラメータや (個別あるいはデータセット全体に対する) 予測の違いを観察することです。 2つ目は、モデルパラメータの勾配に基づいたパラメータの変化を近似することで、インスタンスの重みづけする方法です。 削除アプローチは理解が容易で、重みづけアプローチへの意欲を起こさせるため、前者から始めていきましょう。 6.4.1 Deletion Diagnostics 統計学者は影響力のあるインスタンスの領域、特に (一般化された) 線形回帰モデルでは多くの研究を既に行ってきました。 &quot;影響力測定 (influential observations)&quot;と調べると、最初の検索結果は DFBETA やクック距離 (Cook's distance) といった測定方法についてになるでしょう。 DFBETA は、インスタンスを削除した際のモデルパラメータへの影響を測ります。 クック距離 (Cook, 197763) はインスタンスを削除した際の予測への影響を測ります。 それぞれの測定のためには、毎回個別のインスタンスを除外しながらモデルを繰り返し再学習する必要があります。 全インスタンスでのモデルのパラメータや予測は、学習データからインスタンスを削除して得られたパラメータや予測と比較されます。 DFBETA は以下のように定義されます。 \\[DFBETA_{i}=\\beta-\\beta^{(-i)}\\] ここで \\(\\beta\\) はモデルが全データで学習された場合の重みベクトル、\\(\\beta^{(-i)}\\) はインスタンス i を除いて学習された場合の重みベクトルです。 全く直感的と言っていいでしょう。 DFBETA は、ロジスティック回帰やニューラルネットワークのように重みパラメータを持つモデルにのみ有効で、決定木、アンサンブル、サポートベクタマシンなどには適用できません。 クック距離は線形回帰モデルのために考案され、一般化線形回帰モデルのための近似法が存在します。 学習インスタンスにおけるクック距離は、i 番目のインスタンスが学習時に除外されたときの、予測結果の差を2乗して(正規化された)総和をとったものとして定義されます。 \\[D_i=\\frac{\\sum_{j=1}^n(\\hat{y}_j-\\hat{y}_{j}^{(-i)})^2}{p\\cdot{}MSE}\\] ここで、分子は i 番目のインスタンスの有無によるモデルの予測結果の差の2乗をデータセット全体について総和したものです。 分母は特徴量の数 p に平均二乗誤差をかけたものです。 分母は全インスタンスにおいて同じであり、どのインスタンス i が除外されたかは問題ではありません。 クック距離は、学習時に i 番目のインスタンスを除外すると線形モデルの予測結果がどの程度変化するかを教えてくれます。 クック距離と DFBETA は、あらゆる機械学習モデルに適用できるのでしょうか。 DFBETA はモデルパラメータを要求するため、この測定方法はパラメータをもつモデルにしか使えません。 クック距離はモデルパラメータを要求しません。 面白いことにクック距離は線形モデルと一般化線形モデルの文脈外では普段見かけませんが、特定のインスタンスを除く前後でモデルの予測の差をとるというアイデアは非常に普遍的なものです。 クック距離の定義における問題点は MSE (平均二乗誤差)であることで、これは全てのタイプの予測モデルで意味があるとは限りません(例: 分類器)。 モデルの予測への影響における最も単純な影響力測定は、次のように書けます。 \\[\\text{Influence}^{(-i)}=\\frac{1}{n}\\sum_{j=1}^{n}\\left|\\hat{y}_j-\\hat{y}_{j}^{(-i)}\\right|\\] この表現は基本的にクック距離の分子ですが、違いは差の2乗の代わりに差の絶対値を使っていることです。 後の例で役立つため、私がそのように選択しました。 deletion diagnostic の一般形は、(予測結果のような)基準を選ぶことと、学習時にインスタンスを除いた場合と含む場合での基準の差を計算することで構成されます。 インスタンス j の予測について i 番目の学習インスタンスが与えた影響が何であったのかを説明するため、我々は簡単に影響を分解できます。 \\[\\text{Influence}_{j}^{(-i)}=\\left|\\hat{y}_j-\\hat{y}_{j}^{(-i)}\\right|\\] これはモデルのパラメータの差や損失の差に対しても有効でしょう。 次の例では、これらの単純な影響力測定を利用します。 Deletion diagnostics の例 以下の例では、子宮頸がんを与えられた危険因子から予測するためにサポートベクトルマシンを学習させ、どの学習インスタンスが最も全体に対して影響力を与えているのか、また、ある予測に対してどのインスタンスが最も影響を与えるのかについて測定しています。 がんの予測は分類問題であるため、がんの予測確率の違いを影響力として測定しています。 あるインスタンスをモデルの学習から取り除いた時、もし予測確率がそのデータセットの平均値から大きく増加（減少）する場合、そのインスタンスは影響力が強いインスタンスと言えます。 858 件の学習インスタンス全ての影響力を測定するためには、一度すべてのデータでモデルの学習し、インスタンスのうちの1つを取り除き再学習することを 858 回（＝学習データの数）繰り返す必要があります。 最も影響力のあるインスタンスは約 0.01 の影響度を持っています。 0.01 の影響度は、もし 540 番目のインスタンスを取り除いた場合には予測確率が 1 %だけ平均値から変化することを意味しています。 この値は、癌の平均予測確率が 6.4 %であることを考慮するとかなり重要なものです。 可能な限りの全ての削除によって測定された影響度の平均値は 0.2 %です。 これでどの学習インスタンスがモデルに対して最も影響を与えたのかどうかが分かりました。 問題のあるインスタンスはないでしょうか。 測定に誤りはありませんか。 影響力のあるインスタンスは、それらに内包されるエラーがモデルの予測に対して強く影響を与えるため、エラーがないかどうか最初にチェックされるべきです。 モデルのデバッグとは別に、モデルをより良く理解するために何か知見を得ることは出来ないでしょうか。 ただ単に、最も影響力のある上位10個のインスタンスを表示するだけではあまり意味がありません。なぜなら、それらは多くの特徴量から構成されるインスタンスのただの表に過ぎないからです。 インスタンスを戻り値として返すメソッドは、それらをうまく表現する方法があるときにのみ有効です。 しかし、「影響力の無いインスタンスと影響力のあるインスタンスを差別化しているものはなにか」という問いを持つことで、どのような種類のインスタンスが影響力があるのかということをより良く理解できます。 この問いを回帰問題に落とし込み、インスタンスの影響度を、特徴量を入力とする関数としてモデリング出来ます。 &quot;Interpretable Machine Learning Models&quot; の章からどのようなモデルを自由に選択できます。 この例では、私は決定木（以下の図）を選びました。決定木によると、35歳以上の女性のデータがサポートベクトルマシンにとって最も影響力が有ることが示されています。 データセット中の、858 のうちの 153 の女性が 35 歳以上でした。 Partial Dependence Plots の章で、40歳を超えると急激にがんの予測確率が上昇する傾向が見られ、Feature Importance の章で、特徴量重要度として年齢が検出されていました。 影響力分析の結果、このモデルは年齢が高い部分でがんの予測が不安定になることを明らかにしています。 これは、それ自体が有益な情報です。 そして、これらのインスタンスに対する誤差がモデルに強い影響を持っていることを意味します。 FIGURE 6.13: インスタンスの特徴量と影響力の関係の決定木モデル。木の最大深さは 2 に設定されている。 この最初の影響分析によって、全体で一番大きな影響力のあるインスタンスが明らかになりました。 ここでインスタンスの1つ、つまり 7 番目のインスタンスを選びます。 選んだインスタンスに対して、最も影響力のある学習データのインスタンスを見つけることで予測値を説明します。 これは、反事実的な質問です。 インスタンス i を学習のプロセスから除いた場合、7 番目のインスタンスに対する予測値はどのように変化するでしょうか。 全てのインスタンスに対してこの削除の処理を繰り返したとします。 次に 7 番目のインスタンスの予測値の中で最も大きな変化が生じた学習インスタンスを選びます。 この時選んだインスタンスは学習から削除し、インスタンスに対するモデルの予測値を説明するために用います。 7 番目のインスタンスは、がんの予測確率が最も高いインスタンス(7.35%) だったため、より深く分析するための面白い事例であると考え説明の対象として選びました。 例えば、 7 番目のインスタンスを予測するために影響力の最も強い上位 10 件を表形式で出力できます。 しかし、把握できない事もあるため、非常に便利というわけではありません。 繰り返しになりますが、影響力のあるインスタンスとそうでないインスタンスの特徴量を分析し、両者を区別するものを見つける方が、理にかなっています。 与えられた特徴量から影響度を予測するために決定木を学習しましたが、実際には、予測のためではなく、構造を発見するために使用されています。 次の決定木は、どの種類の学習インスタンスが 7 番目のインスタンスの予測に最も影響を与えたかを示しています。 FIGURE 6.14: 7 番目のインスタンスの予測に最も影響を与えたインスタンスを説明する決定木。18.5年以上喫煙をする女性のデータは、7 番目のインスタンスの予測に対して大きな影響力を持ち、がんの確率を絶対値として平均 11.7 パーセント変化させる。 18.5年以上に渡って喫煙をしたことのある女性のインスタンスは，7 番目のインスタンスの予測に大きな影響を与えています。 7 番目のインスタンスに相当する女性は、34 年間の喫煙期間がありました。 データによると、12 人の女性 (1.40%) は、18.5年以上の喫煙期間がありました。 これらの女性の内1人でも喫煙期間に誤りがあった場合、7 番目のインスタンスに対する予測結果に大きな影響があります。 予測する上で最も極端な変化は、インスタンス 663 を削除した時に発生します。 患者が既に 22 年間喫煙していて、決定木による結果と一致していたとします。 インスタンス 663 を削除すると、7 番目のインスタンスに対する予測確率は、7.35% から 66.60% に変化します．。 最も影響力のあるインスタンスの特徴量を詳しく見ると、別の問題が見えてきます。 データによると、28才で22年間の喫煙経験を持つ女性がいるようです。 これは極端なケースであり、本当に6才で喫煙を始めたか、このデータが間違っているかのどちらかです。 私なら後者の可能性が高いと考えるでしょう。 まさにこれはデータの正確性について、疑問を持つべき状況です。 これらの例はモデルをデバッグするために、影響力のあるインスタンスを特定することがいかに有用であるかを示しています。 提案されたアプローチの問題点の1つは、学習インスタンスごとにモデルを再学習する必要があることです。 何千もの学習インスタンスがある場合、何千回と再学習しなければならないため、再学習全体が非常に遅くなる可能性があります。 モデルの学習に1日かかるとして、インスタンスが1000個あれば、影響力のあるインスタンスの計算には、並列計算なしなら約3年かかるでしょう。 誰もこんなことをする時間はありません。 この章の残りの部分では，再学習を必要としない方法を紹介します。 6.4.2 影響関数 (Influence Functions) あなた: 学習インスタンスが特定の推論結果に与える影響を知りたいです。 研究者: その学習インスタンスを削除した上で、モデルを再学習し、推論結果の差を確認すればいいですよ。 あなた: いいですね！でも、再学習せずに測る方法は無いのですか？時間がかかりすぎちゃいますよ。 研究者: パラメータに対して二階微分可能な損失関数付きのモデルはありますか？ あなた: 私は logistic loss のニューラルネットワークを学習しました。つまり、答えは「はい」です。 研究者: とすると、インスタンスがモデルパラメータと予測値に与える影響を 影響関数 で測ることができます。 影響関数は、モデルパラメータまたは推論結果が学習インスタンスにどれだけ強く依存しているかを示す尺度です。 インスタンスを削除する代わりに、損失の中のインスタンスに非常に小さいステップで重み付けする方法です。 この方法では、勾配行列と Hessian 行列を用いて、現在のモデルパラメータ周辺の損失を近似します。 損失の重みを変える事は、インスタンスを削除することに似ています。 あなた: いいですね、これこそがまさに私が探していたものですよ！ Koh と Liang (2017)64 はインスタンスがモデルのパラメータや推論結果にどの程度影響するか測定するため、ロバスト統計学の手法の1つである影響関数を使うことを提案しました。 Delition diagnostics と同様に、影響関数は、モデル・パラメータと予測値から、影響のある学習インスタンスに遡ってトレースします。 ただし、学習インスタンスを削除するのに代わって、この方法では、インスタンスを経験的リスク（学習データに対する損失の合計）で重み付けしたときに、モデルがどの程度変化するかを近似しています。 影響関数の方法は、モデルパラメータに関する損失勾配を取得することを必要としますが、これは機械学習モデルの一部に対してのみ使用可能です。 ロジスティック回帰、ニューラルネットワーク、SVMでは使用可能ですが、ランダムフォレストのような決定木ベースの手法では使用できません。 また、影響関数は、モデルの動作の理解、モデルのデバッグ、データセットのエラー検出にも役立ちます。 ここでは、影響関数の直感的理解と数学的背景について説明します。 影響関数の数学的視点 影響関数の背後にある重要なアイデアは、学習インスタンスの損失を無限に小さいステップ \\(\\epsilon\\) で重み付けすることであり、結果として新しいモデルパラメータが得られます。 \\[\\hat{\\theta}_{\\epsilon,z}=\\arg\\min_{\\theta{}\\in\\Theta}(1-\\epsilon)\\frac{1}{n}\\sum_{i=1}^n{}L(z_i,\\theta)+\\epsilon{}L(z,\\theta)\\] ここで、\\(\\theta\\) はモデルパラメータに関するベクトルであり、\\(\\hat{\\theta}_{\\epsilon,z}\\) は非常に小さい数字である \\(\\epsilon\\) で z を重み付けした後のパラメータベクトルです。 L はモデルで使用する損失関数、\\(z_i\\) は学習データ、 z は削除をシミュレートするために重み付けしたい学習インスタンスです。 学習データの、あるインスタンス \\(z_i\\) を少し(\\(\\epsilon\\))アップウェイトして、他のインスタンスをダウンウェイトした場合、損失はどのくらい変わるのでしょうか。 この新しい2つが組み合わさった損失を最適化するために、パラメータベクトルはどのようになるでしょうか。 パラメータの影響関数、すなわち、学習インスタンス z をアップウェイトしたことによるパラメータへの影響は、以下のように計算できます。 \\[I_{\\text{up,params}}(z)=\\left.\\frac{d{}\\hat{\\theta}_{\\epsilon,z}}{d\\epsilon}\\right|_{\\epsilon=0}=-H_{\\hat{\\theta}}^{-1}\\nabla_{\\theta}L(z,\\hat{\\theta})\\] 最後の式 \\(\\nabla_{\\theta}L(z,\\hat{\\theta})\\) は、アップウェイトされた学習インスタンスのパラメータに対する損失勾配であり、勾配は学習インスタンスの損失の変化率です。 これは、モデルのパラメータ \\(\\hat{\\theta}\\) を少し変えると、損失がどれくらい変わるかを教えてくれます。 勾配ベクトルの正の項は、対応するモデルパラメータの小さな増加が損失を増加させることを意味し、負の項はパラメータの増加が損失を減少させることを意味します。 最初の部分 \\(H^{-1}_{\\hat{theta}}\\) はヘシアン行列（モデルパラメータに対する損失の2次微分）の逆行列です。 ヘシアン行列は、勾配の変化率、すなわち損失と表現され、これは次のように計算されます。 \\[H_{\\theta}=\\frac{1}{n}\\sum_{i=1}^n\\nabla^2_{\\hat{\\theta}}L(z_i,\\hat{\\theta})\\] くだけていうと、ヘシアン行列は、ある点で損失がどれだけ曲がっているかを記述しています。 ヘシアンは、&quot;損失の曲率&quot; を記述しており、曲率は見る方向に依存するため、単なるベクトルではなく行列です。 ヘシアン行列の実際の計算は、パラメータが多いと時間がかかります。 Koh と Liang は、効率的に計算するためのトリックを提案しています (この章ではサポートしていません)。 上式で説明したように、モデルパラメータを更新することは、推定されたモデルパラメータの周りで二次展開をした後に、ニュートン法を1回行うことと等価です。 この影響関数を直感的に理解するにはどうすればいいでしょうか。 この式は、パラメータ \\(\\hat{\\theta}\\) の周りに二次展開を形成することから来ています。 つまり、実際にはわからない、あるいは、インスタンス z の損失が削除、アップウェイトされたときにどのように変化するかを正確に計算することは複雑すぎるということです。 現在のモデルパラメータ設定における急峻度（＝勾配）と曲率（＝ヘシアン行列）の情報を用いて、関数を局所的に近似します。 この損失近似を使用して、インスタンス z をアップウェイトした場合の新しいパラメータがおおよそどのように見えるかを計算できます。 \\[\\hat{\\theta}_{-z}\\approx\\hat{\\theta}-\\frac{1}{n}I_{\\text{up,params}}(z)\\] 近似されたパラメータベクトルは、基本的に元のパラメータから z の損失の勾配を引いたもの（損失を減らしたいので）を曲率（＝逆ヘシアン行列を掛けたもの）でスケーリングされ、さらに 1/n でスケーリングされます。なぜなら、これは単一の学習インスタンスの重みであるためです。 次の図は、アップウエイトの仕組みを示しています。 x 軸は、\\(\\theta\\) パラメータの値、y 軸は、アップウエイトされたインスタンス z の損失に対応する値を示しています。 ここでのモデルパラメータは簡単のために1次元としていますが、実際には高次元であることが多いです。 1/ｎ だけ、インスタンス ｚ の損失が改善される方向に移動します。 z を削除したときに損失が実際にどのように変化するかはわかりませんが、損失の1次微分と2次微分を用いて、現在のモデルパラメータの周りに、この2次近似を作成し、実際の損失がどのように振る舞うかを近似します。 FIGURE 6.15: 現在のモデル・パラメータを中心とした損失の二次展開を形成してモデル・パラメータ（x軸）を更新し、1/n をアップウェイトされたインスタンス z での損失（y軸）が最も改善される方向に移動させます。この損失におけるインスタンスzに対してアップウェイトを行うことは、z を削除し、縮小されたデータでモデルを学習した場合のパラメータの変化を近似しています。 実際には新しいパラメータを計算する必要はありませんが、パラメータに対する z の影響度の指標として影響関数を使用できます。 学習インスタンス z をアップウェイトしたとき、 予測値はどのように変化するのでしょうか。 新しいパラメータを計算して、新たにパラメトライズモデルを用いて予測するか、あるいは、連鎖律を用いて影響度を計算できるので、予測に対するインスタンス z の影響度を直接計算できます。 \\[\\begin{align*}I_{up,loss}(z,z_{test})&amp;=\\left.\\frac{d{}L(z_{test},\\hat{\\theta}_{\\epsilon,z})}{d\\epsilon}\\right|_{\\epsilon=0}\\\\&amp;=\\left.\\nabla_{\\theta}L(z_{test},\\hat{\\theta})^T\\frac{d\\hat{\\theta}_{\\epsilon,z}}{d\\epsilon}\\right|_{\\epsilon=0}\\\\&amp;=-\\nabla_{\\theta}L(z_{test},\\hat{\\theta})^T{}H^{-1}_{\\theta}\\nabla_{\\theta}L(z,\\hat{\\theta})\\end{align*}\\] この式の1行目は、ある予測 \\(z_{test}\\) に対する学習インスタンスの影響を、インスタンス z をアップウェイトして新しいパラメータを得たときのテストインスタンスの損失の変化として測定することを意味しています。 式の2行目については、導関数の連鎖律を適用して、パラメータに対するテストインスタンスの損失の導関数とパラメータに対する z の影響力の積を求めています。 3行目では、パラメータに対する影響関数に置き換えます。 3行目の第１項 \\(\\nabla_{\\theta}L(z_{test},\\hat{\\theta})^T{}\\) は、モデルパラメータに関するテストインスタンスの勾配です。 式を展開することは素晴らしいことであり、科学的かつ正確な方法で物事を示すことができます。 しかし、式の意味をある程度直感的に理解することはとても大切なことだと思います。 \\(I_{\\text{up,loss}}\\) に対する式は、インスタンス \\(z_{test}\\) の予測に対する学習インスタンス z の影響関数は、&quot;インスタンスがモデルパラメータの変化にどれだけ強く反応するか&quot; に &quot;インスタンス z をアップウェイトしたときにパラメータがどれだけ変化するか&quot; を掛け合わせたものです。 この式のもう1つの見方として、影響力は、学習とテストの損失の勾配の大きさに比例します。 学習の損失の勾配が大きいほどパラメータへの影響が大きく、テストの予測への影響が大きいということになります。 テストの予測の勾配が大きいほど、テストインスタンスの影響力が大きくなります。 全体としては、学習インスタンスとテストインスタンスの間の類似性（モデルによって学習されたもの）の尺度としても見ることができます。 これが理論と直感的な解釈です。 次のセクションで、影響関数の適用方法について説明します。 影響関数の応用 影響関数には多くの応用例があり、いくつかは、この章で既に説明されています。 モデルの挙動の理解 異なる機械学習のモデルは、異なる手法で予測します。 もし、2つのモデルが同じ性能を持っていたとしても、それぞれのモデルが特徴量から予測する方法が異なっていると、別の場面では失敗する可能性があります。 影響力のあるインスタンスを明らかにし、モデルの特定の弱点を理解することは、機械学習モデルの動作の「メンタルモデル」を形成するのに役立ちます。 ドメイン不一致の対処 / モデルのデバック ドメインの不一致を対処することは、モデルの挙動をより理解することと密接に関わっています。ドメインの不一致は学習データとテストデータの分布が異なっていることを意味し、これは、テストデータにおいて、モデルの精度低下を引き起こします。 影響関数は、エラーを引き起こした学習インスタンスを明らかにできます。 手術を受けた患者の結果を予測するモデルを学習したとします。 このとき、全ての患者は同じ病院の患者でした。 この学習されたモデルを、違う病院で試すと多くの患者に対してうまく動かないことが確認されました。 当然、2つの病院が異なる患者を持っていると考え、それぞれの病院のデータを見比べたとき、多くの特徴量が異なっていることがわかります。 しかし、モデルを壊した特徴量またはインスタンスは何なのでしょうか。 ここでもまた、影響関数が、問題に対する良い回答になります。 モデルが間違った予測を出した新しい患者の内の一人を取り出し、最も影響力の高いインスタンスを探し、分析します。 例えば、2番目の病院には平均的に高齢の患者が多く、学習データの中で最も影響力のあるインスタンスが最初の病院の少数の高齢患者であったとすると、単純に、モデルがこのサブグループを適切に予測するための学習データが不足していることがわかります。 モデルを2番目の病院でもうまく動かすためには、より多くの高齢の患者のデータで学習する必要があるという結論が得られるでしょう。 学習データを修正する 正しさを確認できる学習データの数に限りがある場合、どのように効果的な選択すると良いでしょうか。 最善の方法は、最も影響力のあるインスタンスを選択することです。なぜなら、定義より、それらが最もモデルに影響を与えるからです。 もし、明らかに間違った値を持つインスタンスがあったとしても、インスタンスに影響力がなく、予測モデルのためのデータだけを欲している場合、影響力のあるインスタンスを確認することがより良い方法です。 例えば、患者が病院に残るべきか、それとも早期退院してもよいかを予測するモデルを学習します。 患者を間違った時期に退院させることは、悪い結果を招く恐れがあるため、モデルが頑健で正しい予測をすることを確認する必要があります。 患者の記録はまとまりのない物になることもあり、データの質に完全な自信はありません。 しかし、患者のデータを確認し直すことは、とても時間がかかります。なぜなら、ある患者を再確認するには、病院は保管庫に眠る手書きの記録を調べるために、実際に誰かを派遣する必要があるからです。 患者のデータを確認するには1時間か、それ以上の時間がかかることがあります。 これらの費用を抑えるために、少数の重要なインスタンスを確認する方が合理的です。 最良の方法は、予測モデルに高い影響を与えた患者を選ぶことです。 Koh と Liang (2017)らは、この種類の選択が、ランダムな選択、損失が最も大きい患者、または間違った分類をされた患者を選ぶより遥かにうまく機能することを示しました。 6.4.3 長所 Deletion diagnostics と影響関数のアプローチは、 モデル非依存の章 で紹介されているほとんどの特徴量を摂動させるアプローチとは大きく異なります。影響力のあるインスタンスに注目すると、学習フェーズにおける学習データの役割が強調されます。 これによって、影響関数と deletion diagnostics は機械学習モデルに最適なデバッグツールの1つになります。 この本に紹介されている手法の中で、エラーに対してチェックすべきインスタンスはどれかを特定するのに直接役立つ唯一の方法です。 Deletion diagnostics はモデル非依存です。つまり、あらゆるモデルに対して適用可能です。 また、導関数に基づく影響関数は幅広いクラスのモデルに適用できます。 これらを使って様々な機械学習モデルを比較し、予測パフォーマンスを比較するだけでなく、モデルの様々な挙動をより深く理解できます。 この章ではこのトピックについては説明していませんが、導関数による影響関数は敵対的学習データを作るためにも使うことができます。 学習されたモデルが特定のテストインスタンスに対して正しく予測できないように操作されたインスタンスのことです。 敵対的な例の章 の方法との違いは、これらは学習時に行われるということです。これは poisoning attack として知られています。 興味のある方は、Koh and Liang (2017) の論文を読んでください。 Deletion diagnostics と影響関数については予測の差を、影響関数については損失の増加を考慮しました。 実際には、このアプローチは一般化可能であり、&quot;インスタンス z を削除または重み付けを変えたときに、...はどうなりますか&quot; というあらゆる質問に対応できます。 ここで、 &quot;...&quot; はモデルの任意の関数に入れ替えることができます。 例えば、ある学習インスタンスがモデルの全体的な損失にどれだけ影響するか、 ある学習インスタンスが特徴量重要度にどれだけ影響するか、または、ある学習インスタンスが決定木の学習時に最初の分割で選ばれる特徴量にどれだけ影響するかを分析できます。 6.4.4 短所 Deletion diagnostics は再学習が必要なため、非常に 計算コストがかかる方法 です。 ただ、歴史的にはコンピュータリソースは絶えず増加しています。 20年前には考えられたなかったような計算が、今ではスマートフォンで簡単に計算できます。 また、ノートパソコン上では数千の学習データを使って数百のパラメータを持ったモデルを数秒、もしくは数分で学習可能です。 従って deletion diagnostics が10年以内に大規模なニューラルネットワークでも機能すると想定することは、飛躍があるとは言い切れません。 影響関数は deletion diagnostics に代わる良い方法ですが、ニューラルネットワークのような微分可能なパラメータを持ったモデルにのみ適用可能です。 これらは、ランダムフォレスト、ブースティングツリー (boosted trees)、決定木のような決定木ベースの方法では使えません。 パラメータと損失関数を持つモデルであっても、損失を微分できない場合があります。 しかし、最後の問題にはトリックがあります。 たとえば、モデルが微分可能な損失の代わりに、例えば Hinge ロス を使用する場合、影響を計算するときに微分可能損失を代わりに使用します。 損失は、影響関数に対して問題のある損失を平滑化したものに置き換えられていますが、モデルは平滑化されていない損失で学習できます。 このアプローチはパラメータに対する二次展開を使用しているため、影響関数は近似にすぎません。 この近似は間違うかも知れず、実際に取り除いても、インスタンスへの影響が大きくなったり小さくなったりします。 Koh と Liang (2017) は影響関数によって計算された影響度が、実際にインスタンスを消去した後、再学習されたモデルから得た影響度と近い値だった例がいくつか示されています。 しかし、この近似が常に近いという保証はありません。 あるインスタンスが影響力があるかないか、判断するためのはっきりとした影響度の閾値は存在しません。 インスタンスを影響度でソートするのは便利ですが、単にソートするだけでなく、実際に影響力のあるインスタンスと影響力のないインスタンスを区別する手段があればもっと良いでしょう。 例えば、最も影響力のある10個のインスタンスを特定しても、例えばそのうちの上位3つだけが本当に影響力を持っているとすると、影響力がないものも含まれてしまいます。 影響度の測定は個々のインスタンスの削除のみを考慮し、複数のインスタンスの削除は考慮しません。 インスタンスのグループの効果によって、モデルの学習と予測に強く影響することがありえます。 しかし、問題は組み合わせ論にあります。 データから1つのインスタンスを削除するには \\(n\\) 通りの可能性があります。 データから2つのインスタンスを削除するには \\(n(n-1)\\) 通りの可能性があります。 データから3つのインスタンスを削除するには \\(n(n-1)(n-2)\\) 通りの可能性があります。 ここまで来たらもう分かると思いますが、組み合わせが多すぎます。 6.4.5 ソフトウェアと代替手法 Deletion diagnostics は非常に簡単に実装できます。 この章の例題のために書いたコードを見てください。(https://github.com/christophM/interpretable-ml-book/blob/master/manuscript/06.5-example-based-influence-fct.Rmd) 線形モデルや一般線形モデルでは、クック距離のような多くの influence 尺度が R パッケージの stats に実装されています。 Koh と Liang が論文から 影響関数の python コードを公開しています。 しかし残念ながら、それは論文のコードだけであり、保守性やドキュメントがない python モジュールです。 これは Tensorflow ライブラリ向けのコードであり、scikit learn などの他のフレームワークのブラックボックスモデルに直接使用できません。 Keita Kurita による 影響関数のブログ記事のおかげで、Koh と Liang の論文の理解が深まりました。 このブログ中では、ブラックボックスモデルの影響関数の背後にある数学的背景についてさらに深く掘り下げており、この手法を効率的に実装するための数学的な 'トリック' についても紹介されています。 Cook, R. Dennis. &quot;Detection of influential observation in linear regression.&quot; Technometrics 19.1 (1977): 15-18.↩ Koh, Pang Wei, and Percy Liang. &quot;Understanding black-box predictions via influence functions.&quot; arXiv preprint arXiv:1703.04730 (2017).↩ "],["neural-networks.html", "Chapter 7 ニューラルネットワークの解釈", " Chapter 7 ニューラルネットワークの解釈 This chapter is currently only available in this web version. ebook and print will follow. この章は、ニューラルネットワークの解釈方法に重点を当てていきます。 この方法によって、ニューラルネットワークによって学習された特徴や概念が可視化され、個々の予測が説明され、ニューラルネットワークが単純化されます。 深層学習は様々な領域、特に、画像分類や言語翻訳といった画像やテキストに関連する領域で成功してきました。 この深層学習のサクセスストーリーは2012年、the ImageNet image classification challenge 65において深層学習によるアプローチが勝利を収めた時から始まっています。 それから、深層学習の構造のカンブリアン爆発が起こり、トレンドはより深い構造、より多くのハイパーパラメーターを持つものへと向かって行っています。 ニューラルネットワークで予測をするために、入力データは学習された重みによる何層もの乗算と非線形変換を通ります。 ニューラルネットワークの構造によっては、1つの予測に何百万もの数学的演算を必要とすることがあります。 われわれ人間が入力データから予測への正確なマッピングをたどることは不可能です。 ニューラルネットワークによる予測を理解するためには、複雑な方法で相互作用する何百万もの重みについて考慮する必要があるでしょう。 ニューラルネットワークの挙動や予測を理解するためには、それ相応の解釈方法が必要です。 この章は、読者が convolutional neural network 含む深層学習についての知識を有していることを前提としています。 確かに local models や partial dependence plots といったモデル非依存の手法を用いることは可能ですが、ニューラルネットワークに対して特別な解釈方法を考えるのが合理的である理由が2つあります。 1つ目は、ニューラルネットワークは隠れ層で特徴や概念を学習するので、それらを明らかにするツールが必要になるためです。 2つ目は、モデルを&quot;外側から&quot;見るモデル非依存の手法 よりも効率的な解釈方法を実装するために、勾配が利用できるためです。 また、この本のほとんどの方法が表形式のデータに対するモデルの解釈を意図しています。 画像やテキストデータには違った方法が必要となるでしょう。 次の章では、以下のトピックについて取り上げます。 特徴量の可視化 (Feature Visualization): ニューラルネットワークはどのような特徴を学習したのか。 例示に基づいた解釈手法の敵対的サンプル (Adversarial Examples)は特徴量の可視化と密接な関連があります。入力を操作して間違った分類をさせるにはどうしたらよいでしょうか。 概念 (Concepts) (執筆中): ニューラルネットワークが学習したより抽象的な概念は何か。 特徴量の帰属 (Feature Attribution) (執筆中): 各入力はどのようにして特定の予測に貢献したのか。 モデル蒸留 (Modell Distillation) (執筆中): ニューラルネットワークをよりシンプルなモデルで説明するには。 Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg and Li Fei-Fei. (* = equal contribution) ImageNet Large Scale Visual Recognition Challenge. IJCV, 2015↩ "],["cnn-features.html", "7.1 学習された特徴量", " 7.1 学習された特徴量 This chapter is currently only available in this web version. ebook and print will follow. 畳み込みニューラルネットワークは抽象的な特徴量や、元画像ピクセルから概念を学習します。 特徴量の可視化では学習された特徴量を activation maximization により可視化します。 ネットワーク分析ではニューラルネットワークのユニット（チャネルなど）を人間の理解によってラベル付けします。 深層ニューラルネットワークは隠れ層で高レベルの特徴量を学習します。 これは深層ニューラルネットワークの最も重要な強みの1つで、特徴量に関する作業を減らします。 サポートベクターマシンによる画像分類器を構成したいと仮定して下さい。 生のピクセル行列は SVM の訓練に最も良い入力形式ではないため、色、周波数領域、輪郭検出器などの新たな特徴量を作りだす必要があります。 畳み込みニューラルネットワークでは、画像は生の形式(ピクセル)のままネットワークに与えられます。 ネットワークは画像を何度も変換します。 初めに、画像は多数の畳み込み層を通過します。 それらの畳み込み層では、ネットワークは新たな、そして次第に複雑な特徴量をその層において学習します。 その後、変換された画像の情報は全結合層を通過し、分類や予測へ変化します。 FIGURE 7.1: ImageNet データにより訓練された畳み込みニューラルネットワーク (Inception V1) による、学習された特徴量。特徴量は、低い畳み込み層(左)の単純な特徴量から、高い畳み込み層(右)のより抽象的な特徴量までの範囲を持つ。Olah, et al. 2017 (CC-BY 4.0) https://distill.pub/2017/feature-visualization/appendix/. 最初の畳み込み層は輪郭や単純なテクスチャといった特徴量を学習します。 その後の畳み込み層はより複雑なテクスチャや模様といった特徴量を学習します。 最後の特徴量は物体や物体の一部といた特徴量を学習します。 全結合層は、高レベル特徴量からの活性化を予測されるべき個別のクラスへと接続するよう学習します。 クールですね。 しかし、実際にはどのようにそんな幻覚めいた画像を得ているのでしょうか？ 7.1.1 特徴量の可視化 学習された特徴量を明示的なものにしようとする試みを特徴量の可視化と呼びます。 ニューラルネットワークのあるユニットの特徴量の可視化はその部分の活性化関数を最大化する入力を見つけることによって行います。 「ユニット」はそれぞれのニューロン、チャンネル(特徴量マップとも呼ばれる)、全体の層、およびクラス分類における最終的なクラスの確率(や対応するソフトマックスの前のニューロン、これが推奨される)のことを指します。 しかし、問題があります。 ニューラルネットはしばしば何百万ものニューロンを含むことがあります。 それぞれのニューロンの特徴量の可視化を見ることはあまりにも時間がかかりすぎるでしょう。 ユニットとしてのチャンネル(activation map とも呼ばれることもある) は特徴量を可視化するのに良い選択となります。 もう一段上がって全体の畳み込み層を可視化できます。 ユニットとしての層が Google の DeepDream で使われています、これは元の画像に繰り返し可視化された特徴量を加えることで、入力の夢バージョンを出力するものです。 FIGURE 7.2: 特徴量の可視化は異なるユニットで行うことができる。A)畳み込みニューロン、B)畳み込みチャンネル、C)畳み込み層、D)ニューロン、E)隠れ層、F)クラス確率を示すニューロン(または、それに対応するソフトマックスの前のニューロン) 7.1.1.1 最適化を通した特徴量の可視化 数学的な観点では、特徴量の可視化は最適化問題になります。 ニューラルネットワークの重みが固定されていると仮定します。これはネットワークが学習済みであることを意味します。 ユニット、ここでは1つのニューロンを指しますが、の活性化関数の計算結果(の平均)を最大化する新しい画像を探します。 \\[img^*=\\arg\\max_{img}h_{n,x,y,z}(img)\\] 関数 \\(h\\) はニューロンの活性化関数、img はネットワークの入力(画像)、x と y はニューロンの空間的な位置を表していて、n は層を特定し、z はチャンネルのインデックスです。 層 n のチャンネル z の全体の活性化関数の計算結果の平均を最大化するには次式のようにします。 \\[img^*=\\arg\\max_{img}\\sum_{x,y}h_{n,x,y,z}(img)\\] この方程式では、チャンネル z のすべてのニューロンが等しく重み付けされています。 あるいは、ランダムな方向に最大化できて、これはニューロンが負の方向を含む様々なパラメータをかけられることを意味します。 このようにして、ニューロンがチャンネルにどのように作用するかを学びます。 活性化関数を最大化するかわりに、最小化もできます(これは負の方向に最大化することに相当します)。 面白いことに、負の方向に最大化すると同じユニットでも全く異なる特徴量が得られます。 FIGURE 7.3: Inception V1 の、ReLU の前の mixed4d 層のニューロン484 の正(左)と負(右)の活性化関数の値。ニューロンは車輪で最大に活性化されている一方、目を持つようなものが負の活性化を産んでいます。コード: https://colab.research.google.com/github/tensorflow/lucid/blob/master/notebooks/feature-visualization/negative_neurons.ipynb この最適化問題に異なる方法で取り組むことができます。 はじめに、なぜ新しい画像を作るべきなのでしょうか。 単純に学習する画像を検索して、活性化関数を最大化するもの選ぶこともできます。 これは有効なアプローチですが、学習データを使うことは画像の要素が一致してしまい、ニューラルネットが本当に探しているものがわからなくなるという問題があります。 もしある特定のチャンネルをよく活性化させる画像が犬やテニスボールを示していたら、ニューラルネットワークが犬を見ているのか、テニスボールを見ているのか、はたまた両方を見ているのかわかりません。 別のアプローチはランダムなノイズから始めて新しい画像を生成することです。 意味のある可視化を得るために、例えば少しの変化しか許容しないなど、たいてい画像に対する制約があります。 特徴量の可視化のノイズを減らすために、最適化のステップの前に摂動、回転やスケーリングを画像に適用することもあります。 他の正則化の選択肢には、周波数が高いところのペナルティ化 (近い画素の分散を減らす など)、 または、 学習されたプライアーから画像を生成する手法 (generative adversarial network(GAN) 66, denoising autoencoder 67 など) があります。 FIGURE 7.4: ランダムな画像による活性化関数を最大化する反復的な最適化。Olah, et al. 2017 (CC-BY 4.0) https://distill.pub/2017/feature-visualization/. もし特徴量の可視化にもっと深く潜りたいなら、オンラインの論文誌 distill.pub、特に、私が画像を多く引用した Olah らによる特徴量可視化の投稿 68 や説明可能なブロックの構成に関するもの 69 を見てみてください。 7.1.1.2 敵対的サンプルとの繋がり 特徴量の可視化と敵対的サンプルには繋がりがあります。 両者の技術はニューラルネットワークのユニットの活性化を最大化します。 敵対的サンプルでは、敵対的な(間違った)クラスへのニューロンの最大活性化を探しました。 1つの違いは、スタート時の画像です: 敵対的サンプルでは、敵対的な画像を生成しようとした画像です。 特徴量の可視化では、アプローチによっては、ランダムノイズです。 7.1.1.3 テキストおよび表形式データ ここでは、画像認識のための畳み込みニューラルネットワークの特徴量の可視化に焦点を当てます。技術的には、表形式データの為の全結合型ニューラルネットワークやテキストデータの為の再帰型ニューラルネットワークのニューロンを最大限活性化する入力を探すのにも何の支障もありません。 債務不履行予測では、入力は事前の信用の数値、携帯電話の契約数、住所やそのほか数十もの特徴量になるかもしれません。 ニューロンの学習された特徴量はその場合、数十の特徴量のいくつかの組み合わせになるでしょう。 再帰型ニューラルネットワークでは、ネットワークが学習したものは少しだけマシなものになるでしょう。 Karpathy et. al (2015)70 は再帰型ニューラルネットワークが実際にニューロンが説明可能な特徴量を学習することを示しました。 彼らは文字列において前の文字から次の文字を予測するという、文字レベルのモデルを訓練しました。 括弧開き &quot;(&quot;&quot; が現れると、ニューロンの1つが強く活性化され、対応する括弧閉じ &quot;)&quot;が現れたときに不活性化しました。 他のニューロンは行の終端で活性化しました。 URLで活性化するニューロンもありました。 CNNの特徴量の可視化との違いは、そのような例が最適化を通じてではなく、学習データへのニューロン活性化を調査することで見つかったことです。 いくつかの画像は、犬の鼻や建物といった周知の概念を表すように見えます。 しかし、どのように確信できるのでしょうか。 ネットワーク解剖の手法は人間の理解とニューラルネットワークの個別のユニットを結びつけます。 ネタバレ注意: ネットワーク解剖では、誰かが人間の理解によってラベル付けした追加のデータセットが必要となります。 7.1.2 ネットワークの解剖 Bau と Zhou ら(2017)71 による「ネットワークの解剖」の手法によって畳み込みニューラルネットワークのユニットの解釈性が定量化されました。 CNN チャンネルの大きく活性化している領域を人間の概念(物体、経路、模様、色...)で結びつけました。 特徴量の可視化 の章で見たように、畳み込みニューラルネットワークのチャンネルは新たな特徴量を学習します。 しかしこの可視化はあるユニットがある特定の概念を学習したということではありません。 あるユニットが例えば摩天楼をどれくらいよく検出しているかを測る方法もありません。 ネットワークの解剖の詳細に向かう前に、その研究の主題の前にある大きな仮説について話さなければなりません。 その仮説とは: (畳み込みチャンネルのような)ニューラルネットワークのあるユニットは解きほぐされた概念を学習する事です。 解きほぐされた特徴量の問題 (畳み込み)ニューラルネットワークは解きほぐされた特徴量を学習するのでしょうか？ 解きほぐされた特徴量とはネットワークの個々のユニットが現実世界の特定の概念を 検出することを意味します。 畳み込みチャンネル394は摩天楼を検出し、チャンネル121はイヌの鼻を検出し、チャンネル12は30度の角度で縞をつける...などです。 解きほぐされたネットワークの反対は完全にもつれたネットワークです。 完全にもつれたネットワークでは、例えば、イヌの鼻を検出する個々のユニットはないでしょう。 すべてのチャンネルがイヌの鼻の認識に役立っています。 特徴量が解きほぐされていることはネットワークがよく説明できることを示しています。 知っている概念でラベル付けされた完全に解きほぐされたユニットをもつネットワークを感得てみましょう。 これによってネットワークの決定がなされる過程を追跡できる可能性を広がります。 例えば、ネットワークがどのようにオオカミとハスキーを分離しているかを解析できます。 はじめに、「ハスキーの」ユニットを特定します。 そしてそのユニットが前の層から「イヌの鼻」「ふわふわな毛皮」「雪」のユニットのうちどれに依存しているかを確認します。 そうすれば、そのネットワークが雪の背景のハスキーの画像をオオカミと間違って分類してしまうことがわかるでしょう。 解きほぐされたネットワークでは、問題がある何気なくない相関を特定できます。 それぞれの予測を説明するためによく活性化しているユニットとそれが何の概念についてかをすべて自動的にリストアップできます。 そのニューラルネットワークの傾向を簡単に探すことができます。 例えば、給料を予測するためにネットワークが「白い肌」の特徴量を学習するでしょうか。 ネタバレ注意: 畳み込みニューラルネットワークは完全に解きほぐされているわけではありません。説明可能なニューラルネットワークがどんなものであるか見るためにネットワークの解剖についてより詳しく見ていきましょう。 7.1.2.1 ネットワークの解剖のアルゴリズム ネットワークの解剖は3つのステップからなっています。 縞から摩天楼まで人間がラベル付けした視覚的な概念を持つ画像を入手します それらの画像に対する CNN チャンネルの活性化度合いを計測します 活性化度合いとラベル付けされた概念の組み合わせを定量化します 次の画像はある画像がどうチャンネルに進んでいってラベル付けされた概念と一致するかを可視化したものです。 FIGURE 7.5: 与えられた画像と学習済みのネットワーク(重みが固定化されている)に対して、画像をターゲット層まで順伝搬して、活性化層の出力を元の画像サイズに合うようにスケールアップさせ、ground truth の pixel-wise セグメンテーションと活性化関数の最大値を比較しました。図は Bau と Zhou ら(2017)のものの引用です。 ステップ 1: Broden データセット 最初の困難だが重大なステップは、データ収集です。 ネットワーク解剖は、(色から街路の光景までのような)異なる抽象化レベルの概念によってピクセル単位でラベル付けされた画像を必要とします。 Bau &amp; Zhou らは、いくつかのデータセットをピクセル単位の概念と結びつけました。 彼らはこの新たなデータセットを、広範(broadly)かつ稠密(densely)にラベル付けされたデータであることを表して ‘Broden’ と呼びました。 Broden データセットは殆どピクセルレベルまで断片化されており、一部のデータセットでは画像全体でラベル付けされています。 Broden は 60,000 枚の画像と、異なる抽象化レベルにおける 1,000 以上の視覚概念を含んでいます: 468の場面、585の物体、234の部品、32の物質、47の質感、11の色です。 次の画像では Broden データセットの幾つかのサンプル画像を示しています。 FIGURE 7.6: Broden データセットからの画像例。オリジナルは Bau &amp; Zhou et. al (2017) によるもの。 ステップ 2: ネットワークの活性化を読み出す 次に、チャンネルごとおよび画像ごとに最も活性化している領域のマスクを作成します。 この時点では、概念ラベルはまだ関係ありません。 それぞれの畳み込みチャンネル k について: Broden データセット内のそれぞれの画像 x について チャンネル k を含む目的の層への、画像 x の順伝播 畳み込みチャンネル k のピクセル活性化の抽出: \\(A_k(x)\\) 画像全体にわたるピクセル活性化 \\(\\alpha_k\\) の分布計算 活性化 \\(\\alpha_k\\) の0.005-分位点 \\(T_k\\) の決定。これは画像 x に対するチャンネル k の全活性化のうち 0.5% は \\(T_k\\) よりも大きいことを意味します。 Broden データセット内のそれぞれの画像 x について: (場合によっては)低解像度の activation map \\(A_k(x)\\) を画像 x の解像度にまで拡大します。この結果を \\(S_k(x)\\) と呼びます。 その activation map を二値化: 活性化閾値 \\(T_k\\) を超えるか否かにより、ピクセルがオンかオフになります。この新しいマスクを \\(M_k(x)=S_k(x)\\geq{}T_k(x)\\) とします。 ステップ 3: 活性化概念の団結 ステップ 2により、チャンネルおよび画像ごとに1つの活性化マスクを得ました。 これらの活性化マスクは強く活性化された領域を印付けます。 各チャンネルについて、チャンネルを活性化する人間的な概念を見つけたいです。 活性化マスクと全てのラベル付けされたコンセプトを比較することで、概念を発見します。 活性化マスク k と概念マスク c の間の団結を Intersection over Union (IoU) スコアにより定量化します。 \\[IoU_{k,c}=\\frac{\\sum|M_k(x)\\bigcap{}L_c(x)|}{\\sum|M_k(x)\\bigcup{}L_c(x)|}\\] ここで \\(|\\cdot|\\) は集合の濃度です。 IoU は2つの領域の団結を比較します。 \\(IoU_{k,c}\\) は、ユニット k が概念 c を検出する精度として解釈できます。 \\(IoU_{k,c}&gt;0.04\\) のときにユニット k を概念 c の検出器と呼ぶことにします。 次の図は1枚の画像に対する活性化マスクと概念マスクの IoU を説明しています。 FIGURE 7.7: 人間がつけた正解アノテーションと最上位活性化ピクセルを比較して計算された Intersection over Union (IoU)。 次の図は犬を検出するユニットを見せています。 FIGURE 7.8: \\(IoU=0.203\\) で犬を検出する inception_4e のチャンネル 750 の活性化マスク。オリジナルは Bau &amp; Zhou et. al (2017) によるもの。 7.1.2.2 実験 ネットワーク解析者達は様々なネットワーク構造 (AlexNet, VGG, GoogleNet, ResNet)を異なるデータセット(ImageNet, Places205, Places365)を用いて訓練しました。 ImageNet は物体に焦点をおいた1000のクラスの160万枚の画像を含んでいます。 Places205とPlaces365は205/365の異なるシーンからの画像240/160万枚の画像を含んでいいます。 彼らは追加で AlexNet をビデオフレームの順序や画像の色付けなどの自己管理型のタスクについて訓練しました。 これらの多くの異なる設定において、彼らは、解釈性の尺度として、unique concept detectors の数を数えました。 下記の物がいくつかの発見です。 ネットワークは下の層で下位レベルの概念（色、テクスチャー）を検知し、上層で上位の概念を検知します（パーツ、物体）。 私たちは既にこれについて特徴量視覚化で学んでいます(#feature-visualization)。 バッチ正規化により、unique concept detectors の数を減らします。 多くのユニットは同じ概念を検知します。 例えば、 \\(IoU \\geq 0.04\\) を検知のカットオフとして使用した場合、ImageNet で訓練された VGG には 95（！）の犬のチャンネルがあります。 層の中のチャンネルの数を増やせば、解釈可能なユニットの数も増えます。 ランダムな初期化の使用によって（異なるランダムシードと共に訓練すること）少しだけ異なる数の解釈可能なユニットに行き着くことがあります。 Places356 で最も多くの unique concept detectors が学ばれ、次に Places205 と ImageNet が続きます。 FIGURE 7.9: Places365で訓練されたResNetga一番多くのunique detectorsを持ちます。ランダムな重みを持つAlexNetが最も少ないunique detectorsの数を持ちベースラインの役割を果たします。. 画像は Bau &amp; Zhou et. al (2017)からの出典です。 自己管理タスクで訓練されたネットワークは教師ありタスクで訓練されたネットワークと比べて少ない unique detectors を持ちます。 転移学習において、チャンネルの概念は変わります。例えば、犬の検知器は滝の検知器になりました。これは、最初、物体を分類するためのモデルを fine-tuning を使い、シーンを分類するためのモデルに変更しようとした時に起こりました。 実験のひとつで、著者は、チャンネルを新しいローテーション基底に投影しました。 これは ImageNet で訓練された VGG ネットワークで行われました。 &quot;ローテーション&quot;は画像が回転することを意味しているのではありません。 &quot;ローテーション&quot;は、私たちは画像の conv5層から 256 のチャンネルを取得し、元のチャンネルの線形結合として新たな 256 のチャンネルを計算することを意味します。 この過程で、チャンネルはかみ合います。 ローテーションは解釈性を減少させます。ex.概念に沿った、チャンネルの数が減少します。 ローテーションはモデルの性能を同じに保つために設計されました。 最初の結論は、「CNN の解釈性は軸に依存する」です。 これは、ランダムな組み合わせのチャンネルが unique concepts を検知する確率が低いことを意味します。 2つ目の結論は、「解釈性は識別力とは無関係」です。 チャンネルは直交変換で識別力を保ったまま変換できますが、解釈性は減少します。 FIGURE 7.10: AlexNet,conv5（ImageNetで学習済み）の256のチャンネルがランダムな直交変換を使用して徐々に変更されると、unique concept detectorsの数は減少します 画像は Bau &amp; Zhou et. al (2017).からの出典です。 著者は GAN (Generative Adversarial Networks) に対してもネットワーク解剖を行なっています。 GAN に対するネットワーク解剖は、こちらの[webサイト]((https://gandissect.csail.mit.edu/)をご覧ください。 7.1.3 利点 特徴量視覚化は、特に画像認識において、ニューラルネットワークの働きに対してユニークな視点を与えます。 ニューラルネットワークの複雑性と不透明性において、特徴量の視覚化はニューラルネットワークを分析、説明するための重要な一歩です。 特徴量視覚化を通して、ニューラルネットワークは最初にシンプルなエッジやテクスチャを検知し、次にもっと抽象的な部分、そして最後に物体検知の層になっていることがわかりました。 ネットワーク分析はこれらの洞察を拡大し、ネットワークユニットの解釈性を測定可能なものにします。 ネットワーク分析は、私たちに「概念を自動的にユニットとリンクさせること」を許します。これはとても便利です。 特徴量視覚化は、ニューラルネットワークがどのように動いているかを非技術的な方法で伝えるための素晴らしい手法です。 ネットワーク分析と共に、分類のタスクにおいてクラスを超えた概念**を検知できます。 しかし、私たちはピクセル単位でラベル付けされた概念を持つ画像のデータセットが必要です。 7.1.4 欠点 多くの特徴量可視化画像は全く解釈可能ではありませんが、それを表す言葉や精神的概念が存在しないような、いくつかの抽象的な特徴量を含んでいます。 学習データと同時に特徴量可視化を表示することは、助けになります。 画像はニューラルネットワークが何に反応し、&quot;画像に何か黄色い部分があるはずだ&quot;といったことだけを示すのかまでは、明らかにしないかもしれません。 ネットワーク解剖によっても、人間的な概念と結び付けられないチャンネルがあります。 例えば、ImageNet で訓練された VGG の conv5_3 層は(512のうち)193のチャンネルは人間的な概念と組み合わせられませんでした。 チャンネルのアクティブ化「のみ」を視覚化する場合でも、見るユニットが多くなりすぎます。 Inception V1 の場合、既に 5000 を超えるチャンネルが9つの畳み込み層からあります。 さらに負の活性化に加え、チャンネルを最大または最小に活性化する学習データからのいくつかの画像（例えば、4つの正、4つの負の画像）も表示する場合は、50000 を超える画像を表示する必要があります。 少なくとも、ネットワーク解剖のおかげで、ランダムな方向を調査する必要がないことは分かっています。 解釈可能の錯覚 特徴の視覚化は、ニューラルネットワークが何をしているのかを理解しているという錯覚を私たちに伝える可能性があります。 しかし、ニューラルネットワークで何が起こっているのかを本当に理解していますか。 仮に数百または数千の特徴の視覚化を見ても、ニューラルネットワークは理解できません。 チャンネルは複雑な方法で相互作用し、正と負の活性化は無関係であり、複数のニューロンが非常に類似した機能を学習する可能性もあり、特徴の多くについては人間に同等の概念は存在しません。 レイヤー7のニューロン349がヒナギクによって活性化されるのを確認できたからといって、ニューラルネットワークを完全に理解していると信じる罠に陥ってはなりません。 Network Dissection は、ResNet や Inception のような設計には、特定の概念に反応するユニットがあることを示しました。 しかし、IoU はそれほど優れたものではなく、多くの場合、多くのユニットが同じ概念に反応し、一部のユニットはまったく概念に反応しません。 チャンネルは完全に解きほぐされておらず、単独で解釈できません。 ネットワーク解剖の場合、ピクセルレベルでラベル付けされたデータセットの概念が必要になります。 これらのデータセットは、各ピクセルにラベルを付ける必要があるため、収集に多大な労力を要します。これは通常、画像上の物体の周囲に境界線を引くことでなされます。 ネットワーク解剖は、人間の概念を正の活性化と一致させるだけであり、チャンネルの負の活性化とは一致させません。 特徴の視覚化が示したように、負の活性化は概念とリンクしているようです。 これは、活性化の下位分位数をさらに調べることで修正される可能性があります。 7.1.5 ソフトウェアとその他の資料 Lucidと呼ばれる特徴視覚化のオープンソース実装があります。 Lucid Github ページにあるノートブックのリンクから、ウェブブラウザ上で簡単に試すことができます。 追加のソフトウェアは必要ありません。 他には Tensorflow の tf_cnnvis 、Keras のKeras Filters、Caffe のDeepVis で実装できます。 Network Dissection には素晴らしいプロジェクトのウェブサイトがあります。 ウェブサイトでは出版物の隣にコード、データ、活性化マスクなどが視覚化された追加資料がホストされています。 Nguyen, Anh, et al. &quot;Synthesizing the preferred inputs for neurons in neural networks via deep generator networks.&quot; Advances in Neural Information Processing Systems. 2016.↩ Nguyen, Anh, et al. &quot;Plug &amp; play generative networks: Conditional iterative generation of images in latent space.&quot; Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2017.↩ Olah, et al., &quot;Feature Visualization&quot;, Distill, 2017.↩ Olah, et al., &quot;The Building Blocks of Interpretability&quot;, Distill, 2018.↩ Karpathy, Andrej, Justin Johnson, and Li Fei-Fei. &quot;Visualizing and understanding recurrent networks.&quot; arXiv preprint arXiv:1506.02078 (2015).↩ Bau, David, et al. &quot;Network dissection: Quantifying interpretability of deep visual representations.&quot; Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2017.↩ "],["future.html", "Chapter 8 解釈可能な機械学習の未来", " Chapter 8 解釈可能な機械学習の未来 解釈可能な機械学習の未来とは何でしょうか。 この章では、解釈可能な機械学習が今後どのように発展していくかについて主観的な熟考により推測してみようと思います。 私は、3つの前提に基づいて機械学習の未来を「予測」してみます。 デジタル化: （興味のある） 情報は全てデジタル化されるでしょう。 電子マネーやオンライン取引について考えてみてください。 電子書籍、音楽、ビデオについて考えてみてください。 私たちの環境、行動、産業生産プロセスなどに関する全ての知覚的なデータについて考えてみてください。 すべてのデジタル化は、安価なコンピュータ/センサー/ストレージ、スケーリング効果（勝者が全てを手に入れる）、新しいビジネスモデル、モジュラーバリューチェーン、コスト圧力などによって推進されます。 自動化: タスクが自動化可能で、自動化のコストがタスクの実行コストよりも低くなる場合、タスクは自動化されるでしょう。 コンピュータが導入される以前でさえ、ある程度の自動化は行われてきました。 例えば、織機や蒸気式機械です。 しかしながら、コンピュータとデジタル化は、自動化を次のレベルに引き上げます。 forループのプログラムやExcelのマクロ、電子メールの自動応答など、個人でも様々な自動化ができます。 券売機は電車の切符の購入を自動化し（販売係はもう不要です）、洗濯機は洗濯を自動化し、銀行の自動振替は支払い取引を自動化します。 タスクを自動化すれば時間とお金から開放されるため、自動化は経済的にも個人的にも大きなインセンティブとなります。 私たちは現在、翻訳、運転、そして僅かな程度であれ科学的な発見の自動化を目の当たりにしています。 誤った仕様: 私たちは全ての制約を利用して目標を完璧に定めることはできません。 あなたの願いを常に文字通り受けとってくれる魔人を考えてみてください。 「私は世界で一番の金持ちになりたい！」→ あなたは最もリッチな人になりますが、副作用としてあなたの保有する通貨はインフレのために台無しになります。 「私は残りの人生ずっと幸せでいた！」 → 次の5分はとても幸せだと感じますが、そこで魔人はあなたを殺します。 「私は世界平和を願っています！」→ 魔人は全人類を抹殺します。 私たちは、全ての制約を知らないか、あるいは、それらを測定することが出来ないため目標を誤って設定してしまいます。 不完全な目標の仕様の例として、企業をみてみましょう。 ある企業は株主のためにお金を稼ぐという単純な目標をもっています。 しかし、この仕様は、私たちが本当に努力すべき全ての制約をみたす真の目標とは言えません。 例えば、お金を稼ぐために人を殺したり、川を汚染させたり、単に自社のためにお金を印刷するような企業ではありがたみがありません。 私たちは、不完全な目標の仕様を修正するために、法律、規制、制裁、コンプライアンス手続き、労働組合などを発明してきました。 自身で体験できるもう1つの例は、Paperclipsです。 これは、出来るだけ多くのペーパークリップを生産することを目標とした機械で遊ぶゲームです。 ただし、中毒性があります。 台無しにしたくはありませんが、物事があっという間に制御不能になったとしましょう。 機械学習では、目標仕様の不完全さは、不完全なデータの抽象化（偏った母集団、測定誤差、など）、制約のない損失関数、制約に関する知識の欠如、学習データとアプリケーションデータの間の分布のシフトなどに起因します。 デジタル化により自動化が進んでいます。 不完全な目標仕様は自動化と対立します。 私はこの対立は部分的には解釈法によって媒介されると主張します。 これで「予測」の準備が整い水晶玉の準備ができました。 それでは何処へ向かおうとしているのか占ってみましょう。 "],["機械学習の未来.html", "8.1 機械学習の未来", " 8.1 機械学習の未来 機械学習がなければ、解釈可能な機械学習はあり得ません。 それゆえ、解釈性について語る前に、機械学習がどの方向へ向かっていくか推測する必要があります。 機械学習（もしくはAI）は非常に有望であり、多くの期待が持たれています。 ですが、あまり楽観的ではない事柄から始めましょう。 科学は多くの魅力的な機械学習ツールを開発していますが、私の経験上、それらを既存のプロセスやプロダクトに組み込むことは非常に困難です。 その理由としては、不可能ではないにしても、企業や機関がキャッチアップするのに多くの時間がかかるからです。 現在AI全盛期のさなかで、企業は「AI研究所」「機械学習部門」を開設し、「データサイエンティスト」「機械学習エキスパート」「AIエンジニア」などを雇っていますが、私の経験上現実は甘くはありません。 しばしば、企業は要求された形でのデータを持ってすらおらず、データサイエンティストは数か月もの間待ちぼうけの状態であったりします。 時には、データサイエンティストが決して応えられないようなAIやデータサイエンスに対する期待がメディアによって創り出されてきました。 そして、多くの場合、データサイエンティストを既存の構造やその他多くの問題にどのように組み込めばいいのか誰も知らないのです。 これにより、私の第一の予測が導かれます。 機械学習はゆっくりだが着実に成長していくでしょう。 デジタル化は先進的で、自動化への誘惑は常に我々を引っ張っていきます。 たとえ機械学習の適用への道のりが遅く、石のように不動であったとしても、機械学習は科学からビジネスプロセスや製品、実世界への応用に常にシフトしていきます。 どのタイプの問題が機械学習の問題として形式化できるのかを非専門家にわかりやすく説明する必要があるでしょう。 私は機械学習を適用するのではなく、レポートやSQLクエリを用いたExcel計算や古典的なビジネスインテリジェンスを行う高給取りのデータサイエンティストを多く知っています。 しかし、すでにいくつかの企業が機械学習の活用に成功しており、インターネットの大手企業が最前線で活躍しています。 機械学習をプロセスやプロダクトに取り込み、人々を訓練し、使いやすい機械学習ツールを開発するより良い方法を見つける必要があります。 機械学習はもっと使いやすくなると私は信じています。 私たちはすでに、機械学習が、例えばクラウドサービス（「サービスとしての機械学習」 - いくつかのバズワードを投げかけるだけ）を介して、より身近なものになってきていることを見ることができます。 機械学習が成熟したら - そしてすでにそのはじめの一歩を踏みだしていますが - 私の次の予測はこうです。 機械学習によって様々な物事がたきつけられるでしょう。 「自動化できる物は何であれ自動化されるであろう」という原則に基づき、可能な時はいつでもタスクは予測問題として定式化され、機械学習によって解決されるだろうと確信しています。 機械学習は自動化の1つの形であるか、少なくともその一部となり得ます。 現在人間の手によって行われてきた多くのタスクは機械学習に置き換えられています。 ここでは、機械学習を使って部分的に自動化を行うタスクの例を紹介します。 分類 / 意思決定 / 文書の完成（例えば、保険会社、法律分野、コンサル企業） 与信取引申請書などのデータ主導の意思決定 新薬開発 流れ作業の品質管理 自動運転 病気診断 翻訳 この本においては、私はディープニューラルネットワークを搭載した (DeepL) という翻訳サービスを使って、英語からドイツ語に翻訳し、英語に戻すことで文章を改善しました。 ... 機械学習のブレイクスルーはより良いコンピューター / より多くのデータ / より良いソフトウェアだけでなく次のことによっても実現します。 解釈可能なツールによって機械学習の採用が触媒されること。 機械学習モデルの目標を完全には指定できないという前提に基づいて、誤って指定された目標と実際の目標の間のギャップを埋めるために、解釈可能な機械学習が必要であるということになります。 多くの分野やセクターで、解釈可能性が機械学習の採用のきっかけになるでしょう。 いくつかの逸話的な証拠があります。 私が話したことのある人の中には、他者にモデルを説明できないからと言って、機械学習を用いない人が多くいます。 解釈可能性によってこの問題に対処でき、機械学習は透明性を必要とする機関や人々にとって魅力的なものになっていくでしょう。 問題の誤った仕様に加えて、多くの業界では、法的な理由であれ、リスク回避のためであれ、根本的なタスクの洞察力を得るためであれ、解釈可能性が求められています。 機械学習によってモデル化プロセスは自動化され、人類はデータと根底にある問題から少しだけ離れた場所に行くことができます。 これによって、実験デザイン、学習データ分布の選択、サンプリング、データエンコーディング、特徴量エンジニアリングなどの問題が発生するリスクは大きくなります。 解釈ツールを使用することで、これらの問題点を簡単に特定できます。 "],["解釈性の未来.html", "8.2 解釈性の未来", " 8.2 解釈性の未来 機械学習の解釈可能性の未来について見ていきましょう。 モデル非依存の解釈可能なツールにフォーカスされていくでしょう。 解釈可能性の自動化は元の機械学習モデルから分けて考える方が簡単です。 モデルに依存しない解釈性の利点にはモジュール性があります。 基盤となる機械学習モデルを簡単に置き換えることができます。 解釈方法も同様に簡単に置き換えることができます。 これらの理由から、モデルに依存しない手法にははるかに優れた拡張性があります。 長期的にはモデルに依存しない手法が主流となるでしょう。 しかし、本質的に解釈可能な方法にも居場所はあるでしょう。 機械学習は自動化され、それに伴い解釈性を持つようになるでしょう。 すでに目に見える傾向として、モデルの学習の自動化があります。 これらには自動化されたエンジニアリングや特徴選択、自動化されたパラメータ最適化、異なるモデルの比較、モデルのアンサンブルまたはスタッキングが含まれます。 その結果、可能な限りの最良な予測モデルが得られます。 モデルに依存しない解釈モデルを使用した時、自動化された機械学習のプロセスから作成された任意のモデルに対して自動的に適用できます。 ある意味では、この2番目のステップも自動化できます。 これらのすべての解釈を自動的に行うことを止める人はいません。 実際の解釈では、依然として人が必要です。 想像してみてください。あなたがデータセットをアップロードし、予測目標を指定すると、ボタンを押すだけで最高の予測モデルが学習され、プログラムはモデルの全ての解釈を吐き出します。 すでにこれが実現可能な最初の製品は存在し、多くのアプリケーションではこれらの自動化された機械学習サービスで十分だと考えます。 今日ではHTMLやCSS、Javascriptを知らなくてもWebサイトを作ることができますが、周りにはまだ多くのWeb開発者がいます。 同様に、プログラミングの知識がなくても機械学習モデルの学習が行えるようになり、機械学習の専門家も必要とされるでしょう。 データではなくモデルを分析します。 生データ自体は常に役に立ちません。 （これは意図的な誇張です。意味のある分析のためには、データを深く理解する必要があるというのが実際のところです。） 私はデータ自体は気にしません。 関心の対象はデータに含まれる知識です。 解釈可能な機械学習は、データから知識を発見するための優れた手法です。 あなたはモデルを広範囲に調査でき、モデルがその特徴について予測に関係するかやどのように関係するかを自動的に認識し（多くのモデルには特徴選択機能が組み込まれています）、関係性がどのように表現されるかを自動的に検出できます。 そして、もしモデルが正しく学習されたのであれば、最終モデルは現実の非常によい近似となります。 多くの分析ツールは既にデータモデルに基づいています（それらは分布の仮定に基づいているためです）。 スチューデントのt検定のような単純な仮説検定 交絡因子を調整した仮説検定（通常はGLM） 分散分析（ANOVA） 相関係数（標準化された線形回帰の係数はピアソンの相関係数と関係があります） ここで私が言っていることは何も新しいことではありません。 それではなぜ、仮定に基づいた透明性の高いモデルの分析から、仮定のないブラックボックスモデルの分析に切り替えるのでしょうか？ なぜなら、これらの仮定をすることには問題があるからです。 これらは通常間違っていて（世界のほとんどがガウス分布に従っていると信じない限り）、チェックが難しく、非常に柔軟性に乏しく、そして自動化が難しいものです。 多くの領域では仮定に基づくモデルの場合、通常、ブラックボックスの機械学習モデルよりも、未知のテストデータに対する予測性能が劣ります。 これは、大きなデータセットに対してのみ当てはまります。 なぜなら、良い仮定をもつ解釈可能モデルは、ブラックボックスモデルよりも小さなデータセットでよりうまくいくことが多いからです。 ブラックボックスモデルをうまく機能させるためには、多くのデータを必要とします。 デジタル化によってデータセットがさらに大きくなるため、ブラックボックスモデルはより魅力的になります。 私たちは、仮定を立てることなく（学習データの過適合を回避しながら）現実を可能な限り近似します。 私は、統計学で使われている全てのツールを開発し（仮説検定、相関、相互作用、可視化ツール、信頼区間、p-値、予測区間、確立分布）、それらをブラックボックスモデル用に書き直す必要があると主張します。 ある意味では、これはすでに起きていることです。 古典的な線形モデルを考えてみましょう。標準化された回帰係数はすでに特徴量重要度の尺度です。permutation feature importance measureを使えば、任意のモデルで機能するツールが得られます。 線形モデルでは、係数は予測された結果に対して1つの特徴量の影響を測定します。この一般化されたバージョンは、partial dependence plotです。 AとBのどちらがよいかをテストしたい場合にも、partial dependence functionを利用できます。（私の知る限り）まだ持っていないのは、任意のブラックボックスモデルの統計的検定です。 データサイエンティストは自身のタスクを自動化するでしょう。 私は、データサイエンティストは、最終的には多くの分析や予測の仕事から開放され、自分自身の仕事を自動化すると信じています。 これを実現するには、タスクを明確に定義し、その周囲にいくつかのプロセスとルーチンが存在する必要があります。 今日では、これらのルーチンやプロセスは揃っていませんが、データサイエンティストはそれらのことに取り組んでいます。 機械学習が多くの業界や機関で必要不可欠な役割を負うようになるにつれ、多くのタスクは自動化されるでしょう。 ロボットやプログラムが自身を説明するでしょう。 私たちは、機械学習を多用する機械やプログラムに対する、より直観的なインターフェースを必要としています。 例えば、突然停止した理由を報告する自動運転車（「子供が道路を横断する確率70%」）や、 クレジットの申請が却下された理由を銀行の従業員に説明するクレジットデフォルトプログラム（「申請者は非常に多くのクレジットカードを所有しており、不安定な仕事に就いている」）、 アイテムをベルトコンベアからゴミ箱に移動した理由を説明するロボットアーム（「アイテムの底にひび割れがある」）などです。 解釈可能性は機械知能研究を後押しする可能性があります。 プログラムや機械がどのように自身を説明できるかについて更なる研究を行うことで、私たちの知能の理解が深まり、より知能の高い機械をつくれるようになるでしょう。 最後に、これらの予測は全て推測にすぎません。 未来が実際に何をもたらすのかについて見極めていく必要があります。 自身の意見をもって学習し続けましょう！ "],["contribute.html", "Chapter 9 著者貢献", " Chapter 9 著者貢献 Interpretable Machine Learningをお読みいただきありがとうございます。 この本は継続的に執筆中です。 何度も改善され、さらなる章が追加されていく予定です。 ソフトウェアが開発されていくのとまるで同じように。 すべてのテキストとコードはオープンソースであり、github.comで公開されています。 間違いを発見した場合や、何かが欠けていた場合は、Githubページで修正を提案し、issueを作成することができます。 "],["cite.html", "Chapter 10 この本の引用", " Chapter 10 この本の引用 もしこの本がブログの投稿、研究論文や製品に有用ならば、次のように引用していただけると幸いです。 Molnar, Christoph. &quot;Interpretable machine learning. A Guide for Making Black Box Models Explainable&quot;, 2019. https://christophm.github.io/interpretable-ml-book/. もしくは、次の bibtex entry を用いてください。 @book{molnar2019, title = {Interpretable Machine Learning}, author = {Christoph Molnar}, note = {\\url{https://christophm.github.io/interpretable-ml-book/}}, year = {2019}, subtitle = {A Guide for Making Black Box Models Explainable} } 私は、産業や研究においてどこでどのように解釈法が用いられているのか、常に興味を持っています。 この本を参考にするとき、私に一言何のために用いるのかを伝えてくれれば嬉しいです。 もちろんこれは任意であり、私自身の好奇心を満たし、興味深い交流を刺激するためだけのものです。 私のメールアドレスは christoph.molnar.ai@gmail.com です。 "],["translations.html", "Chapter 11 翻訳", " Chapter 11 翻訳 この本の翻訳に興味はありませんか？ この本のライセンスはCreative Commons Attribution-NonCommercial-ShareAlike 4.0 International Licenseです。 つまり、この本を翻訳してオンライン上にあげることができます。 原著者である私に知らせる必要があり、本として売ることはできません。 もしこの本の翻訳に興味があるのならば、メッセージを送ってくれればここにリンクを載せます。 私のアドレスは christoph.molnar.ai@gmail.com です。 翻訳のリスト 中国語: https://github.com/MingchaoZhu/InterpretableMLBook Mingchao Zhuによって翻訳されました。 https://blog.csdn.net/wizardforcel/article/details/98992150 CSDN というプログラマのオンラインコミュニティによってほとんどの章が翻訳されています。 https://zhuanlan.zhihu.com/p/63408696 知乎によっていくつかの章が翻訳されています。このウェブサイトには様々なユーザからの質問とその答えが含まれています。 韓国語: https://tootouch.github.io/IML/taxonomy_of_interpretability_methods/ TooTouchによって翻訳されました。 https://subinium.github.io/IML/ An Subinによって一部が翻訳されました。 スペイン語: https://fedefliguer.github.io/AAI/ Federico Fliguerによって第1章が翻訳されました。 もしほかにもこの本もしくは各章の翻訳を知っているのなら、email (christoph.molnar.ai@gmail.com) で連絡ください。 このリストに載せようと思います。 "],["謝辞.html", "Chapter 12 謝辞", " Chapter 12 謝辞 この本を書くのはとても楽しかったです（そして今でもそうです）。 しかし大変な作業でもあり、サポートしていただいたことにはとても満足しています。 時間的にも労力的にも一番大変な仕事をしてくれた Katrin には一番感謝しています。 彼女は最初から最後まで本を構成してくれて私が見つけられなかったであろうスペルミスや矛盾をたくさん見つけてくれました。 彼女には大変お世話になりました。 画像データに対するLIMEの章について書いてくださった Verena Haunschmid に感謝します。 彼女はデータサイエンスの仕事をしていて、Twitter で彼女をフォロー(@ExpectAPatronum)することをお勧めします。 また、Github 上に修正を投稿してくれた初期の読者に感謝します！ さらに、イラストを作成してくださった皆様に感謝したいと思います。 表紙は友人 @YvonneDoinel によるもので、シャープレイ値の章と敵対的サンプルの章の亀の例は Heidi Seibold 、RuleFitの章は Verena Haunschmid が作成してくれました。 少なくとも3つの面で、私がこの本を出版した方法は型破りです。 第一に、Webサイトや電子書籍、PDFでも入手可能です。 この本を作るのに使用したソフトウェアは、Rとテキストを簡単に多く組み合わせることができるRパッケージを多く作成している Yihui Xie が書いた bookdown と呼ばれるソフトウェアです。 ありがとうございました！ 第二に、従来の出版社ではなく、Leanpub というプラットフォームで本を自費出版しました。 第三に、私はこの本を執筆中の本として出版しましたが、そのおかげでフィードバックを得て、途中で収益化できました。 また、親愛なる読者の皆様、大きな出版社名なしのこの本を読んでくださったことに感謝します。 バイエレン州科学芸術省の Digitisation.Bavaria (ZD.B) の枠組みの中で、私の解釈可能な機械学習の研究に資金を提供してくれたことに感謝しています。 "],["references.html", "References", " References &quot;Definition of Algorithm.&quot; https://www.merriam-webster.com/dictionary/algorithm. (2017). Aamodt, Agnar, and Enric Plaza. &quot;Case-based reasoning: Foundational issues, methodological variations, and system approaches.&quot; AI communications 7.1 (1994): 39-59. Alberto, Túlio C, Johannes V Lochter, and Tiago A Almeida. &quot;Tubespam: comment spam filtering on YouTube.&quot; In Machine Learning and Applications (Icmla), Ieee 14th International Conference on, 138–43. IEEE. (2015). Alvarez-Melis, David, and Tommi S. Jaakkola. &quot;On the robustness of interpretability methods.&quot; arXiv preprint arXiv:1806.08049 (2018). Apley, Daniel W. &quot;Visualizing the effects of predictor variables in black box supervised learning models.&quot; arXiv preprint arXiv:1612.08468 (2016). Athalye, Anish, and Ilya Sutskever. &quot;Synthesizing robust adversarial examples.&quot; arXiv preprint arXiv:1707.07397 (2017). Bau, David, et al. &quot;Network dissection: Quantifying interpretability of deep visual representations.&quot; Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2017. Biggio, Battista, and Fabio Roli. &quot;Wild Patterns: Ten years after the rise of adversarial machine learning.&quot; Pattern Recognition 84 (2018): 317-331. Breiman, Leo.“Random Forests.” Machine Learning 45 (1). Springer: 5-32 (2001). Brown, Tom B., et al. &quot;Adversarial patch.&quot; arXiv preprint arXiv:1712.09665 (2017). Cohen, William W. &quot;Fast effective rule induction.&quot; Machine Learning Proceedings (1995). 115-123. Cook, R. Dennis. &quot;Detection of influential observation in linear regression.&quot; Technometrics 19.1 (1977): 15-18. Doshi-Velez, Finale, and Been Kim. &quot;Towards a rigorous science of interpretable machine learning,&quot; no. Ml: 1–13. http://arxiv.org/abs/1702.08608 ( 2017). Emilie Kaufmann and Shivaram Kalyanakrishnan. “Information Complexity in Bandit Subset Selection”. Proceedings of Machine Learning Research (2013). Fanaee-T, Hadi, and Joao Gama. &quot;Event labeling combining ensemble detectors and background knowledge.&quot; Progress in Artificial Intelligence. Springer Berlin Heidelberg, 1–15. doi:10.1007/s13748-013-0040-3. (2013). Fernandes, Kelwin, Jaime S Cardoso, and Jessica Fernandes. &quot;Transfer learning with partial observability applied to cervical cancer screening.&quot; In Iberian Conference on Pattern Recognition and Image Analysis, 243–50. Springer. (2017). Fisher, Aaron, Cynthia Rudin, and Francesca Dominici. “Model Class Reliance: Variable importance measures for any machine learning model class, from the ‘Rashomon’ perspective.” http://arxiv.org/abs/1801.01489 (2018). Fokkema, Marjolein, and Benjamin Christoffersen. &quot;Pre: Prediction rule ensembles&quot;. https://CRAN.R-project.org/package=pre (2017). Friedman, Jerome H, and Bogdan E Popescu. &quot;Predictive learning via rule ensembles.&quot; The Annals of Applied Statistics. JSTOR, 916–54. (2008). Friedman, Jerome H. &quot;Greedy function approximation: A gradient boosting machine.&quot; Annals of statistics (2001): 1189-1232. Friedman, Jerome, Trevor Hastie, and Robert Tibshirani. &quot;The elements of statistical learning&quot;. www.web.stanford.edu/~hastie/ElemStatLearn/ (2009). Fürnkranz, Johannes, Dragan Gamberger, and Nada Lavrač. &quot;Foundations of rule learning.&quot; Springer Science &amp; Business Media, (2012). Goldstein, Alex, et al. &quot;Package ‘ICEbox’.&quot; (2017). Goldstein, Alex, et al. &quot;Peeking inside the black box: Visualizing statistical learning with plots of individual conditional expectation.&quot; Journal of Computational and Graphical Statistics 24.1 (2015): 44-65. Goodfellow, Ian J., Jonathon Shlens, and Christian Szegedy. &quot;Explaining and harnessing adversarial examples.&quot; arXiv preprint arXiv:1412.6572 (2014). Greenwell, Brandon M., Bradley C. Boehmke, and Andrew J. McCarthy. &quot;A simple and effective model-based variable importance measure.&quot; arXiv preprint arXiv:1805.04755 (2018). Heider, Fritz, and Marianne Simmel. &quot;An experimental study of apparent behavior.&quot; The American Journal of Psychology 57 (2). JSTOR: 243–59. (1944). Holte, Robert C. &quot;Very simple classification rules perform well on most commonly used datasets.&quot; Machine learning 11.1 (1993): 63-90. Hooker, Giles. &quot;Discovering additive structure in black box functions.&quot; Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining. (2004). Janzing, Dominik, Lenon Minorics, and Patrick Blöbaum. &quot;Feature relevance quantification in explainable AI: A causal problem.&quot; International Conference on Artificial Intelligence and Statistics. PMLR, 2020. Janzing, Dominik, Lenon Minorics, and Patrick Blöbaum. &quot;Feature relevance quantification in explainable AI: A causality problem.&quot; arXiv preprint arXiv:1910.13413 (2019). Kahneman, Daniel, and Amos Tversky. &quot;The Simulation Heuristic.&quot; Stanford Univ CA Dept of Psychology. (1981). Kaufman, Leonard, and Peter Rousseeuw. &quot;Clustering by means of medoids&quot;. North-Holland (1987). Kim, Been, Rajiv Khanna, and Oluwasanmi O. Koyejo. &quot;Examples are not enough, learn to criticize! Criticism for interpretability.&quot; Advances in Neural Information Processing Systems (2016). Kim, Been, et al. &quot;Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (tcav).&quot; arXiv preprint arXiv:1711.11279 (2017). Koh, Pang Wei, and Percy Liang. &quot;Understanding black-box predictions via influence functions.&quot; arXiv preprint arXiv:1703.04730 (2017). Laugel, Thibault, et al. &quot;Inverse classification for comparison-based interpretability in machine learning.&quot; arXiv preprint arXiv:1712.08443 (2017). Letham, Benjamin, et al. &quot;Interpretable classifiers using rules and Bayesian analysis: Building a better stroke prediction model.&quot; The Annals of Applied Statistics 9.3 (2015): 1350-1371. Lipton, Peter. &quot;Contrastive explanation.&quot; Royal Institute of Philosophy Supplements 27 (1990): 247-266. Lipton, Zachary C. &quot;The mythos of model interpretability.&quot; arXiv preprint arXiv:1606.03490, (2016). Lundberg, Scott M., and Su-In Lee. &quot;A unified approach to interpreting model predictions.&quot; Advances in Neural Information Processing Systems. 2017. Marco Tulio Ribeiro, Sameer Singh, Carlos Guestrin. &quot;Anchors: High-Precision Model-Agnostic Explanations.&quot; AAAI Conference on Artificial Intelligence (AAAI), 2018 Martens, David, and Foster Provost. &quot;Explaining data-driven document classifications.&quot; (2014). Miller, Tim. &quot;Explanation in artificial intelligence: Insights from the social sciences.&quot; arXiv Preprint arXiv:1706.07269. (2017). Nguyen, Anh, et al. &quot;Plug &amp; play generative networks: Conditional iterative generation of images in latent space.&quot; Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2017. Nguyen, Anh, et al. &quot;Synthesizing the preferred inputs for neurons in neural networks via deep generator networks.&quot; Advances in Neural Information Processing Systems. 2016. Nickerson, Raymond S. &quot;Confirmation Bias: A ubiquitous phenomenon in many guises.&quot; Review of General Psychology 2 (2). Educational Publishing Foundation: 175. (1998). Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg and Li Fei-Fei. (* = equal contribution) ImageNet Large Scale Visual Recognition Challenge. IJCV, 2015 Papernot, Nicolas, et al. &quot;Practical black-box attacks against machine learning.&quot; Proceedings of the 2017 ACM on Asia Conference on Computer and Communications Security. ACM (2017). Ribeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. &quot;Anchors: High-precision model-agnostic explanations.&quot; AAAI Conference on Artificial Intelligence (2018). Ribeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. &quot;Model-agnostic interpretability of machine learning.&quot; ICML Workshop on Human Interpretability in Machine Learning. (2016). Ribeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. &quot;Why should I trust you?: Explaining the predictions of any classifier.&quot; Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining. ACM (2016). Shapley, Lloyd S. &quot;A value for n-person games.&quot; Contributions to the Theory of Games 2.28 (1953): 307-317. Staniak, Mateusz, and Przemyslaw Biecek. &quot;Explanations of model predictions with live and breakDown packages.&quot; arXiv preprint arXiv:1804.01955 (2018). Su, Jiawei, Danilo Vasconcellos Vargas, and Kouichi Sakurai. &quot;One pixel attack for fooling deep neural networks.&quot; IEEE Transactions on Evolutionary Computation (2019). Sundararajan, Mukund, and Amir Najmi. &quot;The many Shapley values for model explanation.&quot; arXiv preprint arXiv:1908.08474 (2019). Sundararajan, Mukund, and Amir Najmi. &quot;The many Shapley values for model explanation.&quot; arXiv preprint arXiv:1908.08474 (2019). Szegedy, Christian, et al. &quot;Intriguing properties of neural networks.&quot; arXiv preprint arXiv:1312.6199 (2013). Van Looveren, Arnaud, and Janis Klaise. &quot;Interpretable Counterfactual Explanations Guided by Prototypes.&quot; arXiv preprint arXiv:1907.02584 (2019). Wachter, Sandra, Brent Mittelstadt, and Chris Russell. &quot;Counterfactual explanations without opening the black box: Automated decisions and the GDPR.&quot; (2017). Yang, Hongyu, Cynthia Rudin, and Margo Seltzer. &quot;Scalable Bayesian rule lists.&quot; Proceedings of the 34th International Conference on Machine Learning-Volume 70. JMLR. org, 2017. Zhao, Qingyuan, and Trevor Hastie. &quot;Causal interpretations of black-box models.&quot; Journal of Business &amp; Economic Statistics, to appear. (2017). Štrumbelj, Erik, and Igor Kononenko. &quot;A general method for visualizing and explaining black-box regression models.&quot; In International Conference on Adaptive and Natural Computing Algorithms, 21–30. Springer. (2011). Štrumbelj, Erik, and Igor Kononenko. &quot;Explaining prediction models and individual predictions with feature contributions.&quot; Knowledge and information systems 41.3 (2014): 647-665. "],["r-packages-used-for-examples.html", "R Packages Used for Examples", " R Packages Used for Examples base. R Core Team (2020). R: A language and environment for statistical computing. R Foundation for Statistical Computing, Vienna, Austria. URL https://www.R-project.org/. data.table. Matt Dowle and Arun Srinivasan (2021). data.table: Extension of data.frame. R package version 1.14.0. https://CRAN.R-project.org/package=data.table dplyr. Hadley Wickham, Romain François, Lionel Henry and Kirill Müller (2021). dplyr: A Grammar of Data Manipulation. R package version 1.0.6. https://CRAN.R-project.org/package=dplyr ggplot2. Hadley Wickham, Winston Chang, Lionel Henry, Thomas Lin Pedersen, Kohske Takahashi, Claus Wilke, Kara Woo, Hiroaki Yutani and Dewey Dunnington (2020). ggplot2: Create Elegant Data Visualisations Using the Grammar of Graphics. R package version 3.3.3. https://CRAN.R-project.org/package=ggplot2 iml. Christoph Molnar and Patrick Schratz (2020). iml: Interpretable Machine Learning. R package version 0.10.1. https://CRAN.R-project.org/package=iml knitr. Yihui Xie (2021). knitr: A General-Purpose Package for Dynamic Report Generation in R. R package version 1.33. https://CRAN.R-project.org/package=knitr libcoin. Torsten Hothorn (2021). libcoin: Linear Test Statistics for Permutation Inference. R package version 1.0-8. https://CRAN.R-project.org/package=libcoin memoise. Hadley Wickham, Jim Hester, Winston Chang, Kirill Müller and Daniel Cook (2021). memoise: Memoisation of Functions. R package version 2.0.0. https://CRAN.R-project.org/package=memoise mlr. Bernd Bischl, Michel Lang, Lars Kotthoff, Patrick Schratz, Julia Schiffner, Jakob Richter, Zachary Jones, Giuseppe Casalicchio and Mason Gallo (2021). mlr: Machine Learning in R. R package version 2.19.0. https://CRAN.R-project.org/package=mlr mvtnorm. Alan Genz, Frank Bretz, Tetsuhisa Miwa, Xuefei Mi and Torsten Hothorn (2020). mvtnorm: Multivariate Normal and t Distributions. R package version 1.1-1. https://CRAN.R-project.org/package=mvtnorm NLP. Kurt Hornik (2020). NLP: Natural Language Processing Infrastructure. R package version 0.2-1. https://CRAN.R-project.org/package=NLP ParamHelpers. Bernd Bischl, Michel Lang, Jakob Richter, Jakob Bossek, Daniel Horn and Pascal Kerschke (2020). ParamHelpers: Helpers for Parameters in Black-Box Optimization, Tuning and Machine Learning. R package version 1.14. https://CRAN.R-project.org/package=ParamHelpers partykit. Torsten Hothorn and Achim Zeileis (2021). partykit: A Toolkit for Recursive Partytioning. R package version 1.2-13. https://CRAN.R-project.org/package=partykit pre. Marjolein Fokkema and Benjamin Christoffersen (2020). pre: Prediction Rule Ensembles. R package version 1.0.0. https://CRAN.R-project.org/package=pre readr. Hadley Wickham and Jim Hester (2020). readr: Read Rectangular Text Data. R package version 1.4.0. https://CRAN.R-project.org/package=readr rpart. Terry Therneau and Beth Atkinson (2019). rpart: Recursive Partitioning and Regression Trees. R package version 4.1-15. https://CRAN.R-project.org/package=rpart tidyr. Hadley Wickham (2021). tidyr: Tidy Messy Data. R package version 1.1.3. https://CRAN.R-project.org/package=tidyr tm. Ingo Feinerer and Kurt Hornik (2020). tm: Text Mining Package. R package version 0.7-8. https://CRAN.R-project.org/package=tm viridis. Simon Garnier (2021). viridis: Colorblind-Friendly Color Maps for R. R package version 0.6.1. https://CRAN.R-project.org/package=viridis viridisLite. Simon Garnier (2021). viridisLite: Colorblind-Friendly Color Maps (Lite Version). R package version 0.4.0. https://CRAN.R-project.org/package=viridisLite "]]
